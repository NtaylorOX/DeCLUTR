{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8jt6ML03DS5"
      },
      "source": [
        "# Training your own model\n",
        "\n",
        "This notebook will walk you through training your own model using [DeCLUTR](https://github.com/JohnGiorgi/DeCLUTR)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU3Iod2-g0-o"
      },
      "source": [
        "## üîß Install the prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sr4r5pN40Kli"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/JohnGiorgi/DeCLUTR.git\n",
        "\n",
        "# go to main dir i.e. DeCLUTR on local and run \"pip install --editable .\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<multiprocessing.pool.Pool at 0x1a914b8ed08>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# testing multiprocessing fork - \n",
        "\n",
        "from multiprocessing import get_context\n",
        "num_processes = 8\n",
        "pool = get_context(\"spawn\").Pool(num_processes)\n",
        "pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zog7ApwuUD7_"
      },
      "source": [
        "## üìñ Preparing a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwnLpUmN4Art"
      },
      "source": [
        "\n",
        "A dataset is simply a file containing one item of text (a document, a scientific paper, etc.) per line. For demonstration purposes, we have provided a script that will download the [WikiText-103](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/) dataset and format it for training with our method.\n",
        "\n",
        "The only \"gotcha\" is that each piece of text needs to be long enough so that we can sample spans from it. In general, you should collect documents of a minimum length according to the following:\n",
        "\n",
        "```python\n",
        "min_length = num_anchors * max_span_len * 2\n",
        "```\n",
        "\n",
        "In our paper, we set `num_anchors=2` and `max_span_len=512`, so we require documents of `min_length=2048`. We simply need to provide this value as an argument when running the script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q0fwnwq23aAZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "train_data_path = \"E:/wiki_text/wikitext-103/train.txt\"\n",
        "\n",
        "# run this to download and preprocess data\n",
        "\n",
        "min_length = 2048\n",
        "\n",
        "# !python ../scripts/preprocess_wikitext_103.py $train_data_path --min-length $min_length --max-instances 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUEFeupP6qy-"
      },
      "source": [
        "Lets confirm that our dataset looks as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K7ffGXCn7Cpq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     500 E:/wiki_text/wikitext-103/train.txt\n",
            "     500 total\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wc: '#': No such file or directory\n",
            "wc: This: No such file or directory\n",
            "wc: should: No such file or directory\n",
            "wc: be: No such file or directory\n",
            "wc: approximately: No such file or directory\n",
            "wc: 17.8K: No such file or directory\n",
            "wc: lines: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!wc -l $train_data_path  # This should be approximately 17.8K lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "10DprWZc9iV6"
      },
      "outputs": [],
      "source": [
        "# !head -n 1 $train_data_path  # This should be a single Wikipedia entry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Look at sampling technique\n",
        "\n",
        "This will help get an idea of what "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\conda_envs\\37_declutr\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from declutr.common.contrastive_utils import sample_anchor_positive_pairs\n",
        "from declutr.losses import NTXentLoss\n",
        "import torch\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"this is just an example sentence to test out some sampling and loss calculation from DeCLUTR. We want to see exactly how it works in order to implement it for our own use case\"\n",
        "len_text = len(text.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# just go with one anchor for now\n",
        "\n",
        "num_anchors = 1\n",
        "max_span_len = int((len_text/2)/num_anchors) \n",
        "min_span_len = 5\n",
        "num_positives = 5\n",
        "sampling_strat = \"adjacent\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "anchor_spans, positive_spans = sample_anchor_positive_pairs(\n",
        "    text = text,\n",
        "    num_anchors = num_anchors,\n",
        "    num_positives = num_positives,\n",
        "    max_span_len = max_span_len,\n",
        "    min_span_len = min_span_len,\n",
        "    sampling_strategy = sampling_strat\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['to see exactly how it works in order to implement it for our own use']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "anchor_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['loss calculation from DeCLUTR. We want',\n",
              " 'and loss calculation from DeCLUTR. We want',\n",
              " 'sampling and loss calculation from DeCLUTR. We want',\n",
              " 'sampling and loss calculation from DeCLUTR. We want',\n",
              " 'some sampling and loss calculation from DeCLUTR. We want']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "positive_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test loss function\n",
        "anchor_emb = torch.rand(64).unsqueeze(0)\n",
        "pos_emb = torch.rand(64).unsqueeze(0)\n",
        "neg_emb = torch.rand(64).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "anchor_pos_embs = torch.cat((anchor_emb, pos_emb))\n",
        "loss_func = NTXentLoss\n",
        "embs, labels = NTXentLoss.get_embeddings_and_label(anchor_emb, pos_emb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKYdambZ59nM"
      },
      "source": [
        "## üèÉ Training the model\n",
        "\n",
        "Once you have collected the dataset, you can easily initiate a training session with the `allennlp train` command. An experiment is configured using a [Jsonnet](https://jsonnet.org/) config file. Lets take a look at the config for the DeCLUTR-small model presented in [our paper](https://arxiv.org/abs/2006.03659):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open(\"../training_config/declutr_small.jsonnet\", \"r\") as f:\n",
        "#     print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f1HqWSscWOx"
      },
      "source": [
        "\n",
        "The only thing to configure is the path to the training set (`train_data_path`), which can be passed to `allennlp train` via the `--overrides` argument (but you can also provide it in your config file directly, if you prefer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YS9VuxESBcr3"
      },
      "outputs": [],
      "source": [
        "# overrides = (\n",
        "#     f\"{{'train_data_path': '{train_data_path}', \"\n",
        "#     # lower the batch size to be able to train on Colab GPUs\n",
        "#     \"'data_loader.batch_size': 2, \"\n",
        "#     # training examples / batch size. Not required, but gives us a more informative progress bar during training\n",
        "#     \"'data_loader.batches_per_epoch': None}\"\n",
        "# )\n",
        "\n",
        "\n",
        "overrides = (\n",
        "    f\"{{'train_data_path': '{train_data_path}', \"\n",
        "    # lower the batch size to be able to train on Colab GPUs\n",
        "    \"'data_loader.batch_size': 2,}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2v4tiiXgBC2M"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"{'train_data_path': 'E:/wiki_text/wikitext-103/train.txt', 'data_loader.batch_size': 2,}\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "overrides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Db_cNfZ76KRf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-09-21 13:12:47,625 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2022-09-21 13:12:47,625 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2022-09-21 13:12:47,626 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2022-09-21 13:12:47,773 - INFO - allennlp.common.checks - Pytorch version: 1.6.0\n",
            "2022-09-21 13:12:47,773 - INFO - allennlp.common.params - type = default\n",
            "2022-09-21 13:12:47,774 - INFO - allennlp.common.params - dataset_reader.type = declutr\n",
            "2022-09-21 13:12:47,774 - INFO - allennlp.common.params - dataset_reader.lazy = False\n",
            "2022-09-21 13:12:47,774 - INFO - allennlp.common.params - dataset_reader.cache_directory = None\n",
            "2022-09-21 13:12:47,774 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2022-09-21 13:12:47,775 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2022-09-21 13:12:47,775 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False\n",
            "2022-09-21 13:12:47,775 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n",
            "2022-09-21 13:12:47,775 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = distilroberta-base\n",
            "2022-09-21 13:12:47,775 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = True\n",
            "2022-09-21 13:12:47,775 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = 510\n",
            "2022-09-21 13:12:47,775 - INFO - allennlp.common.params - dataset_reader.tokenizer.stride = 0\n",
            "2022-09-21 13:12:47,775 - INFO - allennlp.common.params - dataset_reader.tokenizer.truncation_strategy = longest_first\n",
            "2022-09-21 13:12:47,775 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n",
            "2022-09-21 13:12:48,163 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-config.json from cache at C:\\Users\\niall/.cache\\torch\\transformers\\d52ced8fd31ba6aa311b6eeeae65178cca00ddd6333c087be4601dc46c20bd96.1221e000e415a518ec3c28f32c14c1dd3baa36dc0537db4848b7236b38f50313\n",
            "2022-09-21 13:12:48,164 - INFO - transformers.configuration_utils - Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "2022-09-21 13:12:48,871 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-vocab.json from cache at C:\\Users\\niall/.cache\\torch\\transformers\\5f11352d3c3e932888f3ba75bc24579eacb5d1596d39ce56166aeae8fd363df8.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "2022-09-21 13:12:48,871 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-merges.txt from cache at C:\\Users\\niall/.cache\\torch\\transformers\\01f63a14ad93494c050af2090c59930fb787bdfb347c4cad7ce9063e1a5fe140.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "2022-09-21 13:12:49,371 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-config.json from cache at C:\\Users\\niall/.cache\\torch\\transformers\\d52ced8fd31ba6aa311b6eeeae65178cca00ddd6333c087be4601dc46c20bd96.1221e000e415a518ec3c28f32c14c1dd3baa36dc0537db4848b7236b38f50313\n",
            "2022-09-21 13:12:49,371 - INFO - transformers.configuration_utils - Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "2022-09-21 13:12:50,067 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-vocab.json from cache at C:\\Users\\niall/.cache\\torch\\transformers\\5f11352d3c3e932888f3ba75bc24579eacb5d1596d39ce56166aeae8fd363df8.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "2022-09-21 13:12:50,067 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-merges.txt from cache at C:\\Users\\niall/.cache\\torch\\transformers\\01f63a14ad93494c050af2090c59930fb787bdfb347c4cad7ce9063e1a5fe140.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "2022-09-21 13:12:50,156 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer\n",
            "2022-09-21 13:12:50,156 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2022-09-21 13:12:50,156 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = distilroberta-base\n",
            "2022-09-21 13:12:50,156 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2022-09-21 13:12:50,156 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = None\n",
            "2022-09-21 13:12:50,157 - INFO - allennlp.common.params - dataset_reader.num_anchors = 2\n",
            "2022-09-21 13:12:50,157 - INFO - allennlp.common.params - dataset_reader.num_positives = 2\n",
            "2022-09-21 13:12:50,157 - INFO - allennlp.common.params - dataset_reader.max_span_len = 512\n",
            "2022-09-21 13:12:50,157 - INFO - allennlp.common.params - dataset_reader.min_span_len = 32\n",
            "2022-09-21 13:12:50,157 - INFO - allennlp.common.params - dataset_reader.sampling_strategy = None\n",
            "2022-09-21 13:12:50,157 - INFO - allennlp.common.params - train_data_path = E:/wiki_text/wikitext-103/train.txt\n",
            "2022-09-21 13:12:50,158 - INFO - allennlp.common.params - datasets_for_vocab_creation = None\n",
            "2022-09-21 13:12:50,158 - INFO - allennlp.common.params - validation_dataset_reader = None\n",
            "2022-09-21 13:12:50,158 - INFO - allennlp.common.params - validation_data_path = None\n",
            "2022-09-21 13:12:50,158 - INFO - allennlp.common.params - validation_data_loader = None\n",
            "2022-09-21 13:12:50,158 - INFO - allennlp.common.params - test_data_path = None\n",
            "2022-09-21 13:12:50,158 - INFO - allennlp.common.params - evaluate_on_test = False\n",
            "2022-09-21 13:12:50,158 - INFO - allennlp.common.params - batch_weight_key = \n",
            "2022-09-21 13:12:50,158 - INFO - allennlp.training.util - Reading training data from E:/wiki_text/wikitext-103/train.txt\n",
            "2022-09-21 13:12:50,166 - INFO - declutr.dataset_reader - Reading instances from lines in file at: E:/wiki_text/wikitext-103/train.txt\n",
            "2022-09-21 13:13:07,286 - INFO - allennlp.common.params - vocabulary.type = empty\n",
            "2022-09-21 13:13:07,286 - INFO - allennlp.common.params - model.type = declutr\n",
            "2022-09-21 13:13:07,287 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2022-09-21 13:13:07,287 - INFO - allennlp.common.params - model.text_field_embedder.type = mlm\n",
            "2022-09-21 13:13:07,287 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mlm\n",
            "2022-09-21 13:13:07,287 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = distilroberta-base\n",
            "2022-09-21 13:13:07,287 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = None\n",
            "2022-09-21 13:13:07,288 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_module = None\n",
            "2022-09-21 13:13:07,288 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
            "2022-09-21 13:13:07,288 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True\n",
            "2022-09-21 13:13:07,288 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None\n",
            "2022-09-21 13:13:07,288 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n",
            "2022-09-21 13:13:07,288 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
            "2022-09-21 13:13:07,288 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None\n",
            "2022-09-21 13:13:07,288 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None\n",
            "2022-09-21 13:13:07,288 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.masked_language_modeling = True\n",
            "2022-09-21 13:13:07,658 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-config.json from cache at C:\\Users\\niall/.cache\\torch\\transformers\\d52ced8fd31ba6aa311b6eeeae65178cca00ddd6333c087be4601dc46c20bd96.1221e000e415a518ec3c28f32c14c1dd3baa36dc0537db4848b7236b38f50313\n",
            "2022-09-21 13:13:07,659 - INFO - transformers.configuration_utils - Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "2022-09-21 13:13:07,762 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/distilroberta-base-pytorch_model.bin from cache at C:\\Users\\niall/.cache\\torch\\transformers\\5aab0d7dfa1db7d97ead13a37479db888b133a51a05ae4ab62ff5c8d1fcabb65.52b6ec356fb91985b3940e086d1b2ebf8cd40f8df0ba1cabf4cac27769dee241\n",
            "2022-09-21 13:13:10,951 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "2022-09-21 13:13:10,951 - WARNING - transformers.modeling_utils - Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2022-09-21 13:13:10,952 - INFO - allennlp.common.params - model.seq2vec_encoder = None\n",
            "2022-09-21 13:13:10,952 - INFO - allennlp.common.params - model.feedforward = None\n",
            "2022-09-21 13:13:10,952 - INFO - allennlp.common.params - model.miner = None\n",
            "2022-09-21 13:13:10,952 - INFO - allennlp.common.params - model.loss.type = nt_xent\n",
            "2022-09-21 13:13:10,952 - INFO - allennlp.common.params - model.loss.temperature = 0.05\n",
            "2022-09-21 13:13:10,953 - INFO - allennlp.common.params - model.scale_fix = False\n",
            "2022-09-21 13:13:10,953 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x0000025DA8C4B708>\n",
            "2022-09-21 13:13:10,953 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2022-09-21 13:13:10,953 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2022-09-21 13:13:10,953 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.bias\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.bias\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.weight\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.bias\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.weight\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.bias\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.weight\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.position_embeddings.weight\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.token_type_embeddings.weight\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.word_embeddings.weight\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.bias\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.weight\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.bias\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.weight\n",
            "2022-09-21 13:13:10,954 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.bias\n",
            "2022-09-21 13:13:10,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.weight\n",
            "2022-09-21 13:13:10,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.bias\n",
            "2022-09-21 13:13:10,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.weight\n",
            "2022-09-21 13:13:10,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.bias\n",
            "2022-09-21 13:13:10,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.weight\n",
            "2022-09-21 13:13:10,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.bias\n",
            "2022-09-21 13:13:10,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.weight\n",
            "2022-09-21 13:13:10,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.bias\n",
            "2022-09-21 13:13:10,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.weight\n",
            "2022-09-21 13:13:10,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:13:10,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:13:10,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.bias\n",
            "2022-09-21 13:13:10,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.weight\n",
            "2022-09-21 13:13:10,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.bias\n",
            "2022-09-21 13:13:10,955 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.weight\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.bias\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.weight\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.bias\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.weight\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.bias\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.weight\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.bias\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.weight\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.bias\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.weight\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.bias\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.weight\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.bias\n",
            "2022-09-21 13:13:10,956 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.weight\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.bias\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.weight\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.bias\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.weight\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.bias\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.weight\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.bias\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.weight\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.bias\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.weight\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.bias\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.weight\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.bias\n",
            "2022-09-21 13:13:10,957 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.weight\n",
            "2022-09-21 13:13:10,958 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.bias\n",
            "2022-09-21 13:13:10,958 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.weight\n",
            "2022-09-21 13:13:10,958 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.bias\n",
            "2022-09-21 13:13:10,958 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.weight\n",
            "2022-09-21 13:13:10,958 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.bias\n",
            "2022-09-21 13:13:10,958 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.weight\n",
            "2022-09-21 13:13:10,958 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.bias\n",
            "2022-09-21 13:13:10,958 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.weight\n",
            "2022-09-21 13:13:10,958 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.bias\n",
            "2022-09-21 13:13:10,958 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.weight\n",
            "2022-09-21 13:13:10,958 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:13:10,958 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:13:10,958 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.bias\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.weight\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.bias\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.weight\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.bias\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.weight\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.bias\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.weight\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.bias\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.weight\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.bias\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.weight\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.bias\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.weight\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:13:10,959 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.bias\n",
            "2022-09-21 13:13:10,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.weight\n",
            "2022-09-21 13:13:10,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.bias\n",
            "2022-09-21 13:13:10,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.weight\n",
            "2022-09-21 13:13:10,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.bias\n",
            "2022-09-21 13:13:10,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.weight\n",
            "2022-09-21 13:13:10,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.bias\n",
            "2022-09-21 13:13:10,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.weight\n",
            "2022-09-21 13:13:10,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.bias\n",
            "2022-09-21 13:13:10,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.weight\n",
            "2022-09-21 13:13:10,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.bias\n",
            "2022-09-21 13:13:10,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.weight\n",
            "2022-09-21 13:13:10,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.bias\n",
            "2022-09-21 13:13:10,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.weight\n",
            "2022-09-21 13:13:10,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.pooler.dense.bias\n",
            "2022-09-21 13:13:10,960 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.pooler.dense.weight\n",
            "2022-09-21 13:13:10,962 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader\n",
            "2022-09-21 13:13:10,962 - INFO - allennlp.common.params - data_loader.batch_size = 2\n",
            "2022-09-21 13:13:10,962 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2022-09-21 13:13:10,962 - INFO - allennlp.common.params - data_loader.sampler = None\n",
            "2022-09-21 13:13:10,962 - INFO - allennlp.common.params - data_loader.batch_sampler = None\n",
            "2022-09-21 13:13:10,962 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
            "2022-09-21 13:13:10,962 - INFO - allennlp.common.params - data_loader.pin_memory = False\n",
            "2022-09-21 13:13:10,962 - INFO - allennlp.common.params - data_loader.drop_last = True\n",
            "2022-09-21 13:13:10,962 - INFO - allennlp.common.params - data_loader.timeout = 0\n",
            "2022-09-21 13:13:10,962 - INFO - allennlp.common.params - data_loader.worker_init_fn = None\n",
            "2022-09-21 13:13:10,963 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None\n",
            "2022-09-21 13:13:10,963 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2022-09-21 13:13:10,963 - INFO - allennlp.common.params - trainer.type = gradient_descent\n",
            "2022-09-21 13:13:10,963 - INFO - allennlp.common.params - trainer.patience = None\n",
            "2022-09-21 13:13:10,963 - INFO - allennlp.common.params - trainer.validation_metric = -loss\n",
            "2022-09-21 13:13:10,963 - INFO - allennlp.common.params - trainer.num_epochs = 1\n",
            "2022-09-21 13:13:10,964 - INFO - allennlp.common.params - trainer.cuda_device = None\n",
            "2022-09-21 13:13:10,964 - INFO - allennlp.common.params - trainer.grad_norm = 1\n",
            "2022-09-21 13:13:10,964 - INFO - allennlp.common.params - trainer.grad_clipping = None\n",
            "2022-09-21 13:13:10,964 - INFO - allennlp.common.params - trainer.distributed = None\n",
            "2022-09-21 13:13:10,964 - INFO - allennlp.common.params - trainer.world_size = 1\n",
            "2022-09-21 13:13:10,964 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1\n",
            "2022-09-21 13:13:10,964 - INFO - allennlp.common.params - trainer.use_amp = True\n",
            "2022-09-21 13:13:10,964 - INFO - allennlp.common.params - trainer.no_grad = None\n",
            "2022-09-21 13:13:10,964 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2022-09-21 13:13:10,964 - INFO - allennlp.common.params - trainer.tensorboard_writer = None\n",
            "2022-09-21 13:13:10,965 - INFO - allennlp.common.params - trainer.moving_average = None\n",
            "2022-09-21 13:13:10,965 - INFO - allennlp.common.params - trainer.batch_callbacks = None\n",
            "2022-09-21 13:13:10,965 - INFO - allennlp.common.params - trainer.epoch_callbacks = None\n",
            "2022-09-21 13:13:12,404 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw\n",
            "2022-09-21 13:13:12,405 - INFO - allennlp.common.params - trainer.optimizer.lr = 5e-05\n",
            "2022-09-21 13:13:12,405 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)\n",
            "2022-09-21 13:13:12,405 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06\n",
            "2022-09-21 13:13:12,405 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1\n",
            "2022-09-21 13:13:12,405 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = False\n",
            "2022-09-21 13:13:12,406 - INFO - allennlp.training.optimizers - Done constructing parameter groups.\n",
            "2022-09-21 13:13:12,406 - INFO - allennlp.training.optimizers - Group 0: ['_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.pooler.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.lm_head.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.bias'], {'weight_decay': 0}\n",
            "2022-09-21 13:13:12,406 - INFO - allennlp.training.optimizers - Group 1: ['_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.token_type_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.word_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.pooler.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.position_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.weight'], {}\n",
            "2022-09-21 13:13:12,406 - INFO - allennlp.training.optimizers - Number of trainable parameters: 82760793\n",
            "2022-09-21 13:13:12,407 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):\n",
            "2022-09-21 13:13:12,407 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):\n",
            "2022-09-21 13:13:12,407 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.word_embeddings.weight\n",
            "2022-09-21 13:13:12,407 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.position_embeddings.weight\n",
            "2022-09-21 13:13:12,407 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.token_type_embeddings.weight\n",
            "2022-09-21 13:13:12,407 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.weight\n",
            "2022-09-21 13:13:12,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.bias\n",
            "2022-09-21 13:13:12,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.weight\n",
            "2022-09-21 13:13:12,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.bias\n",
            "2022-09-21 13:13:12,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.weight\n",
            "2022-09-21 13:13:12,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.bias\n",
            "2022-09-21 13:13:12,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.weight\n",
            "2022-09-21 13:13:12,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.bias\n",
            "2022-09-21 13:13:12,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.weight\n",
            "2022-09-21 13:13:12,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.bias\n",
            "2022-09-21 13:13:12,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:13:12,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:13:12,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.weight\n",
            "2022-09-21 13:13:12,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.bias\n",
            "2022-09-21 13:13:12,408 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.weight\n",
            "2022-09-21 13:13:12,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.bias\n",
            "2022-09-21 13:13:12,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.weight\n",
            "2022-09-21 13:13:12,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.bias\n",
            "2022-09-21 13:13:12,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.weight\n",
            "2022-09-21 13:13:12,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.bias\n",
            "2022-09-21 13:13:12,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.weight\n",
            "2022-09-21 13:13:12,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.bias\n",
            "2022-09-21 13:13:12,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.weight\n",
            "2022-09-21 13:13:12,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.bias\n",
            "2022-09-21 13:13:12,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.weight\n",
            "2022-09-21 13:13:12,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.bias\n",
            "2022-09-21 13:13:12,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:13:12,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:13:12,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.weight\n",
            "2022-09-21 13:13:12,409 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.bias\n",
            "2022-09-21 13:13:12,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.weight\n",
            "2022-09-21 13:13:12,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.bias\n",
            "2022-09-21 13:13:12,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.weight\n",
            "2022-09-21 13:13:12,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.bias\n",
            "2022-09-21 13:13:12,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.weight\n",
            "2022-09-21 13:13:12,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.bias\n",
            "2022-09-21 13:13:12,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.weight\n",
            "2022-09-21 13:13:12,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.bias\n",
            "2022-09-21 13:13:12,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.weight\n",
            "2022-09-21 13:13:12,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.bias\n",
            "2022-09-21 13:13:12,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.weight\n",
            "2022-09-21 13:13:12,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.bias\n",
            "2022-09-21 13:13:12,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:13:12,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:13:12,410 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.weight\n",
            "2022-09-21 13:13:12,411 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.bias\n",
            "2022-09-21 13:13:12,411 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.weight\n",
            "2022-09-21 13:13:12,411 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.bias\n",
            "2022-09-21 13:13:12,411 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.weight\n",
            "2022-09-21 13:13:12,411 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.bias\n",
            "2022-09-21 13:13:12,411 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.weight\n",
            "2022-09-21 13:13:12,411 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.bias\n",
            "2022-09-21 13:13:12,411 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.weight\n",
            "2022-09-21 13:13:12,411 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.bias\n",
            "2022-09-21 13:13:12,411 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.weight\n",
            "2022-09-21 13:13:12,411 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.bias\n",
            "2022-09-21 13:13:12,411 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.weight\n",
            "2022-09-21 13:13:12,411 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.bias\n",
            "2022-09-21 13:13:12,411 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:13:12,411 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:13:12,412 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.weight\n",
            "2022-09-21 13:13:12,412 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.bias\n",
            "2022-09-21 13:13:12,412 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.weight\n",
            "2022-09-21 13:13:12,412 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.bias\n",
            "2022-09-21 13:13:12,412 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.weight\n",
            "2022-09-21 13:13:12,412 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.bias\n",
            "2022-09-21 13:13:12,412 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.weight\n",
            "2022-09-21 13:13:12,412 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.bias\n",
            "2022-09-21 13:13:12,412 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.weight\n",
            "2022-09-21 13:13:12,412 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.bias\n",
            "2022-09-21 13:13:12,412 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.weight\n",
            "2022-09-21 13:13:12,412 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.bias\n",
            "2022-09-21 13:13:12,412 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.weight\n",
            "2022-09-21 13:13:12,412 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.bias\n",
            "2022-09-21 13:13:12,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:13:12,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:13:12,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.weight\n",
            "2022-09-21 13:13:12,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.bias\n",
            "2022-09-21 13:13:12,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.weight\n",
            "2022-09-21 13:13:12,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.bias\n",
            "2022-09-21 13:13:12,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.weight\n",
            "2022-09-21 13:13:12,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.bias\n",
            "2022-09-21 13:13:12,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.weight\n",
            "2022-09-21 13:13:12,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.bias\n",
            "2022-09-21 13:13:12,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.weight\n",
            "2022-09-21 13:13:12,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.bias\n",
            "2022-09-21 13:13:12,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.weight\n",
            "2022-09-21 13:13:12,413 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.bias\n",
            "2022-09-21 13:13:12,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.weight\n",
            "2022-09-21 13:13:12,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.bias\n",
            "2022-09-21 13:13:12,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:13:12,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:13:12,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.weight\n",
            "2022-09-21 13:13:12,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.bias\n",
            "2022-09-21 13:13:12,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.weight\n",
            "2022-09-21 13:13:12,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.bias\n",
            "2022-09-21 13:13:12,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.weight\n",
            "2022-09-21 13:13:12,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.bias\n",
            "2022-09-21 13:13:12,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.pooler.dense.weight\n",
            "2022-09-21 13:13:12,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.pooler.dense.bias\n",
            "2022-09-21 13:13:12,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.bias\n",
            "2022-09-21 13:13:12,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.weight\n",
            "2022-09-21 13:13:12,414 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.bias\n",
            "2022-09-21 13:13:12,415 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.weight\n",
            "2022-09-21 13:13:12,415 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.bias\n",
            "2022-09-21 13:13:12,415 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular\n",
            "2022-09-21 13:13:12,415 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.1\n",
            "2022-09-21 13:13:12,415 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32\n",
            "2022-09-21 13:13:12,415 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1\n",
            "2022-09-21 13:13:12,415 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False\n",
            "2022-09-21 13:13:12,415 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False\n",
            "2022-09-21 13:13:12,415 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38\n",
            "2022-09-21 13:13:12,416 - INFO - allennlp.common.params - trainer.checkpointer.type = default\n",
            "2022-09-21 13:13:12,416 - INFO - allennlp.common.params - trainer.checkpointer.keep_serialized_model_every_num_seconds = None\n",
            "2022-09-21 13:13:12,416 - INFO - allennlp.common.params - trainer.checkpointer.num_serialized_models_to_keep = -1\n",
            "2022-09-21 13:13:12,416 - INFO - allennlp.common.params - trainer.checkpointer.model_save_interval = None\n",
            "2022-09-21 13:13:12,420 - INFO - allennlp.training.trainer - Beginning training.\n",
            "2022-09-21 13:13:12,420 - INFO - allennlp.training.trainer - Epoch 0/0\n",
            "2022-09-21 13:13:12,420 - INFO - allennlp.training.trainer - Worker 0 memory usage MB: 0.0\n",
            "2022-09-21 13:13:12,458 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 796\n",
            "2022-09-21 13:13:12,459 - INFO - allennlp.training.trainer - Training\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([9176944608507082456, 4574198950947548237, 8215715368053322000,\n",
            "        8702971098320801752, 9176944608507082456, 4574198950947548237,\n",
            "        8215715368053322000, 8702971098320801752], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6761994353746384020, 4055016370415818190, 1797321631751336165,\n",
            "        4549124191601178330, 6761994353746384020, 4055016370415818190,\n",
            "        1797321631751336165, 4549124191601178330], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6080065504369763083, 7765718612929860484, 2652223007757596604,\n",
            "        7984429739375298978, 6080065504369763083, 7765718612929860484,\n",
            "        2652223007757596604, 7984429739375298978], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4579095173446365276, 8939082714000405055,  198105302294426294,\n",
            "        3856597545864084927, 4579095173446365276, 8939082714000405055,\n",
            "         198105302294426294, 3856597545864084927], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2683943914604359006, 4436519592712230771, 9182373098047847512,\n",
            "        2537908558419268170, 2683943914604359006, 4436519592712230771,\n",
            "        9182373098047847512, 2537908558419268170], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1513464452453114123, 1103079009712923186, 6165252286467555562,\n",
            "        5923184901200951142, 1513464452453114123, 1103079009712923186,\n",
            "        6165252286467555562, 5923184901200951142], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([9170613480599834567, 3067283315346796009, 2095341513846108474,\n",
            "        7446583362332552965, 9170613480599834567, 3067283315346796009,\n",
            "        2095341513846108474, 7446583362332552965], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8013016499136110228, 5720075678294015547, 6850794830663990808,\n",
            "        3665511373977142407, 8013016499136110228, 5720075678294015547,\n",
            "        6850794830663990808, 3665511373977142407], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 678386945549055217, 5867243656692977476, 7775425774860874952,\n",
            "        8146519729753933257,  678386945549055217, 5867243656692977476,\n",
            "        7775425774860874952, 8146519729753933257], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1128040384501685841, 3002959080973630793, 1638248897611222555,\n",
            "        4012675958727033182, 1128040384501685841, 3002959080973630793,\n",
            "        1638248897611222555, 4012675958727033182], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6739291824322957151, 2194502756953174020, 3184217020564207803,\n",
            "        3913251646856712899, 6739291824322957151, 2194502756953174020,\n",
            "        3184217020564207803, 3913251646856712899], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4145948164654813781, 8972520870918714640, 4829110122016312732,\n",
            "        9208219709296517540, 4145948164654813781, 8972520870918714640,\n",
            "        4829110122016312732, 9208219709296517540], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5357362646660870605, 8034983911530454591, 4664556477214003967,\n",
            "        7175983295580108947, 5357362646660870605, 8034983911530454591,\n",
            "        4664556477214003967, 7175983295580108947], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8800546111629782023, 3591163737525700710, 4999739130124920290,\n",
            "        3867333090797557197, 8800546111629782023, 3591163737525700710,\n",
            "        4999739130124920290, 3867333090797557197], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8449704421933736211, 6985469131481735034, 1455050570070912684,\n",
            "        5363113526953650586, 8449704421933736211, 6985469131481735034,\n",
            "        1455050570070912684, 5363113526953650586], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8464341278729831687, 4199901500191115691, 4999517135304805532,\n",
            "        3146694354507543425, 8464341278729831687, 4199901500191115691,\n",
            "        4999517135304805532, 3146694354507543425], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3663066193739620510, 1620365318635304283, 6566100604977607601,\n",
            "        5397142381816191123, 3663066193739620510, 1620365318635304283,\n",
            "        6566100604977607601, 5397142381816191123], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1069040333200043428, 3810703864076583791, 8466188230826018366,\n",
            "        3421091973956715283, 1069040333200043428, 3810703864076583791,\n",
            "        8466188230826018366, 3421091973956715283], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6299998631482497201, 4537384483476602585, 2065202909703605320,\n",
            "        2639478277234846202, 6299998631482497201, 4537384483476602585,\n",
            "        2065202909703605320, 2639478277234846202], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4830680015503808254, 3697910590574353402, 7160767376217933065,\n",
            "        2325145771663898653, 4830680015503808254, 3697910590574353402,\n",
            "        7160767376217933065, 2325145771663898653], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5323584455668666396, 1416409762106517168,  233604777383947299,\n",
            "        1446286127976974189, 5323584455668666396, 1416409762106517168,\n",
            "         233604777383947299, 1446286127976974189], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7311386263291282745, 2312022816841694811, 4135404194460210477,\n",
            "        8794873453230262241, 7311386263291282745, 2312022816841694811,\n",
            "        4135404194460210477, 8794873453230262241], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6174111824005760581, 6532128722022706144, 6806436881901355390,\n",
            "        7610660853659757720, 6174111824005760581, 6532128722022706144,\n",
            "        6806436881901355390, 7610660853659757720], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2436515895952211248, 4316837412817266314,    7872123039231129,\n",
            "        4493009932868440476, 2436515895952211248, 4316837412817266314,\n",
            "           7872123039231129, 4493009932868440476], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6556963080469655704, 7113567501592670328, 9156922805918477084,\n",
            "        3990376013497822487, 6556963080469655704, 7113567501592670328,\n",
            "        9156922805918477084, 3990376013497822487], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8907530354454789612, 6568962925402838372, 4110098144866480081,\n",
            "        7902631868961754234, 8907530354454789612, 6568962925402838372,\n",
            "        4110098144866480081, 7902631868961754234], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4787556786784883290, 8897941977066515227, 7009558379443901882,\n",
            "        4191197553386930161, 4787556786784883290, 8897941977066515227,\n",
            "        7009558379443901882, 4191197553386930161], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6227730442344078795, 5043843670011883114, 8381611653033640149,\n",
            "        1801808980841124270, 6227730442344078795, 5043843670011883114,\n",
            "        8381611653033640149, 1801808980841124270], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6895554069773369135, 5771318778859739289, 3536288587060562089,\n",
            "         272406679926261075, 6895554069773369135, 5771318778859739289,\n",
            "        3536288587060562089,  272406679926261075], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8548795014034454984,  144027540765161522, 1964513904072382012,\n",
            "        2686955382385827311, 8548795014034454984,  144027540765161522,\n",
            "        1964513904072382012, 2686955382385827311], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6289196091610703034, 3358738557180505649, 3945039890083061112,\n",
            "        7930356197289321216, 6289196091610703034, 3358738557180505649,\n",
            "        3945039890083061112, 7930356197289321216], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2267491008147459806, 7481680710857757337, 7320155744758519407,\n",
            "        8596691908122375880, 2267491008147459806, 7481680710857757337,\n",
            "        7320155744758519407, 8596691908122375880], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4172311617858847882, 3306084485069584620,  143390361395595791,\n",
            "        2160412421705428370, 4172311617858847882, 3306084485069584620,\n",
            "         143390361395595791, 2160412421705428370], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2329882650779238306, 8909377457914439132, 7069088398129625427,\n",
            "        1198952745837719019, 2329882650779238306, 8909377457914439132,\n",
            "        7069088398129625427, 1198952745837719019], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8656797760482526469, 7805541017710452124, 8365021603805512806,\n",
            "        3062577319382434677, 8656797760482526469, 7805541017710452124,\n",
            "        8365021603805512806, 3062577319382434677], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3531036347547334825, 1790413769326342485, 6133163507715883042,\n",
            "        7317268924006778797, 3531036347547334825, 1790413769326342485,\n",
            "        6133163507715883042, 7317268924006778797], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2912185667997398012, 3737321862682390830, 3469908456472239796,\n",
            "        8771425716134065104, 2912185667997398012, 3737321862682390830,\n",
            "        3469908456472239796, 8771425716134065104], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1257148522774914885, 1550929634094365825, 4236854658386525515,\n",
            "        8570848937986033869, 1257148522774914885, 1550929634094365825,\n",
            "        4236854658386525515, 8570848937986033869], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3586302344071649291, 8265941642014892926, 4192833133791353325,\n",
            "        6240096543967068727, 3586302344071649291, 8265941642014892926,\n",
            "        4192833133791353325, 6240096543967068727], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4739919285127544136, 3097956852258260832, 7598917258640004769,\n",
            "        3528546583892909305, 4739919285127544136, 3097956852258260832,\n",
            "        7598917258640004769, 3528546583892909305], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5306753304541872818, 3372219817344401115, 4111773335666379575,\n",
            "        4245836875270748525, 5306753304541872818, 3372219817344401115,\n",
            "        4111773335666379575, 4245836875270748525], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2947115302502192756, 3912413869914542214,  349171974868880809,\n",
            "        2497947872253196549, 2947115302502192756, 3912413869914542214,\n",
            "         349171974868880809, 2497947872253196549], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5212360746015736519, 4291983127346596675,  350109975828272291,\n",
            "        3611869852324697612, 5212360746015736519, 4291983127346596675,\n",
            "         350109975828272291, 3611869852324697612], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([9186959446641316176, 6456196558450427666, 1965766242494361539,\n",
            "        5765596429312281342, 9186959446641316176, 6456196558450427666,\n",
            "        1965766242494361539, 5765596429312281342], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6733028970206720674, 1904489211441600375, 6062352605275704148,\n",
            "        7651710998659084887, 6733028970206720674, 1904489211441600375,\n",
            "        6062352605275704148, 7651710998659084887], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3317821709050368250,  372806203348863623, 1668329518047835356,\n",
            "        4494517032229882363, 3317821709050368250,  372806203348863623,\n",
            "        1668329518047835356, 4494517032229882363], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8555994183898988114, 1531216528064853487,  391979670321895784,\n",
            "        8229941860348881280, 8555994183898988114, 1531216528064853487,\n",
            "         391979670321895784, 8229941860348881280], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8054424615306675710, 2804224383189744235, 2615230879983633725,\n",
            "        2976023427047696257, 8054424615306675710, 2804224383189744235,\n",
            "        2615230879983633725, 2976023427047696257], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2399523141377679132, 1433131497850804756, 6865032371060833998,\n",
            "        5499790436536942453, 2399523141377679132, 1433131497850804756,\n",
            "        6865032371060833998, 5499790436536942453], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3791681788208908488, 1572884270814763048, 4631404780609482751,\n",
            "         979320511472310791, 3791681788208908488, 1572884270814763048,\n",
            "        4631404780609482751,  979320511472310791], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1119938750875970489, 7695664122333917859, 2482317648828816157,\n",
            "        3854204762101162913, 1119938750875970489, 7695664122333917859,\n",
            "        2482317648828816157, 3854204762101162913], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8825114250590728243, 7772186830290650359, 5237805581080308306,\n",
            "          99098215486833408, 8825114250590728243, 7772186830290650359,\n",
            "        5237805581080308306,   99098215486833408], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7276219467070941611, 1203718126912908688, 7515332999131308655,\n",
            "        1491496699959712487, 7276219467070941611, 1203718126912908688,\n",
            "        7515332999131308655, 1491496699959712487], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 778583923634385445, 7508407658171316130, 6508275181221077295,\n",
            "        3689554740274206725,  778583923634385445, 7508407658171316130,\n",
            "        6508275181221077295, 3689554740274206725], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8589458780331591818, 6507975760909374526, 7350021963347511554,\n",
            "        2624590327980818619, 8589458780331591818, 6507975760909374526,\n",
            "        7350021963347511554, 2624590327980818619], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5887701112388723250, 6580664940895927037, 8528053960898351474,\n",
            "        8070821090509687580, 5887701112388723250, 6580664940895927037,\n",
            "        8528053960898351474, 8070821090509687580], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2673831486792869698,  217808915026546005, 1633042607503477435,\n",
            "        4477835246383761059, 2673831486792869698,  217808915026546005,\n",
            "        1633042607503477435, 4477835246383761059], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8695609388803836721, 2350980700133114887, 4920346588424627804,\n",
            "         993450198893268939, 8695609388803836721, 2350980700133114887,\n",
            "        4920346588424627804,  993450198893268939], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8277958313928557384, 8886615371553661916, 7183474265001383880,\n",
            "        1069377903689667820, 8277958313928557384, 8886615371553661916,\n",
            "        7183474265001383880, 1069377903689667820], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1411970170009116724,  400448784756751612, 6992192652972706284,\n",
            "        2762922743908347343, 1411970170009116724,  400448784756751612,\n",
            "        6992192652972706284, 2762922743908347343], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1934116731626143515, 3108991284016810357, 9030803232837658868,\n",
            "        7780833847430358941, 1934116731626143515, 3108991284016810357,\n",
            "        9030803232837658868, 7780833847430358941], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4411976272517853137, 1930740905604497299, 7139311867151135908,\n",
            "        8360639233469821619, 4411976272517853137, 1930740905604497299,\n",
            "        7139311867151135908, 8360639233469821619], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1949192827283402906,  367776514726772856, 7868764476234070781,\n",
            "         340685792586664519, 1949192827283402906,  367776514726772856,\n",
            "        7868764476234070781,  340685792586664519], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6468460864109368682, 1273986807797156042,  362228219844898291,\n",
            "        8781380370380087967, 6468460864109368682, 1273986807797156042,\n",
            "         362228219844898291, 8781380370380087967], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8227118191011970138, 3581067188534978654,  703606036779028142,\n",
            "        2102473447259137619, 8227118191011970138, 3581067188534978654,\n",
            "         703606036779028142, 2102473447259137619], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3453879993707289774, 3572375765735059175, 7747559577115726332,\n",
            "        6847553504629512601, 3453879993707289774, 3572375765735059175,\n",
            "        7747559577115726332, 6847553504629512601], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 271289708763450266, 2567291921665961532, 2467820052433081743,\n",
            "        8551921119381692219,  271289708763450266, 2567291921665961532,\n",
            "        2467820052433081743, 8551921119381692219], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([  85923387209473994, 1486057646653255794, 2712086504684319059,\n",
            "         700193128535142363,   85923387209473994, 1486057646653255794,\n",
            "        2712086504684319059,  700193128535142363], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2895174893013003200, 6238758280549153462,  973263384535816604,\n",
            "        3011031395581452543, 2895174893013003200, 6238758280549153462,\n",
            "         973263384535816604, 3011031395581452543], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1519704375953947095, 6619670298849071017, 4882630598196483845,\n",
            "        5298615746274080149, 1519704375953947095, 6619670298849071017,\n",
            "        4882630598196483845, 5298615746274080149], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5212144354725330659, 6734537560264156207, 3476977346908828313,\n",
            "         267812025908167512, 5212144354725330659, 6734537560264156207,\n",
            "        3476977346908828313,  267812025908167512], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8826075959846288525, 8331048825988433067, 6366249035193959315,\n",
            "        7867131311592586794, 8826075959846288525, 8331048825988433067,\n",
            "        6366249035193959315, 7867131311592586794], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2182961751178881004, 8899949258657093153, 7890932167783352808,\n",
            "         639411322737624279, 2182961751178881004, 8899949258657093153,\n",
            "        7890932167783352808,  639411322737624279], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5578859973313824083,  309033846445837383, 7428954605561991158,\n",
            "        9180895189561965376, 5578859973313824083,  309033846445837383,\n",
            "        7428954605561991158, 9180895189561965376], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8281281275554098504, 5040137748241912003, 5898692641736283301,\n",
            "         243506635314476198, 8281281275554098504, 5040137748241912003,\n",
            "        5898692641736283301,  243506635314476198], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6604082909313102809, 6944309971441585841, 1846898861664627881,\n",
            "        1119395722724402175, 6604082909313102809, 6944309971441585841,\n",
            "        1846898861664627881, 1119395722724402175], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1631138680106970929, 8536245767610015504,  427023207127409157,\n",
            "        1352591538107876644, 1631138680106970929, 8536245767610015504,\n",
            "         427023207127409157, 1352591538107876644], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1382642997326555547, 3728576293961789370, 2632099960233696756,\n",
            "        2606342089708327905, 1382642997326555547, 3728576293961789370,\n",
            "        2632099960233696756, 2606342089708327905], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5158893703819892803, 9213877535455437589, 6996278330586799117,\n",
            "        3059703647775911576, 5158893703819892803, 9213877535455437589,\n",
            "        6996278330586799117, 3059703647775911576], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6692953268734217747, 4484760575354638107, 2355502594667754978,\n",
            "        1061230545340735793, 6692953268734217747, 4484760575354638107,\n",
            "        2355502594667754978, 1061230545340735793], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8435028512235481063, 9172145578957963716, 4435604334748289987,\n",
            "         176584471918382747, 8435028512235481063, 9172145578957963716,\n",
            "        4435604334748289987,  176584471918382747], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8215067989600370516, 3865127095436389466, 9102038755662674052,\n",
            "         365922792897163557, 8215067989600370516, 3865127095436389466,\n",
            "        9102038755662674052,  365922792897163557], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3327895547082814915, 8737145598086727488, 3215741236858605449,\n",
            "        6129543857735262682, 3327895547082814915, 8737145598086727488,\n",
            "        3215741236858605449, 6129543857735262682], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1469682800319492594, 8039938659418402604, 6976091584034499697,\n",
            "        3937109847599847758, 1469682800319492594, 8039938659418402604,\n",
            "        6976091584034499697, 3937109847599847758], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3988413032835500634, 5361602642508669974, 4092183032589396786,\n",
            "        1373701390556577159, 3988413032835500634, 5361602642508669974,\n",
            "        4092183032589396786, 1373701390556577159], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8939134258157593751, 8526040386659588624,   79877000927641701,\n",
            "        6866305611824610675, 8939134258157593751, 8526040386659588624,\n",
            "          79877000927641701, 6866305611824610675], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4234256539447291550, 7600107228008227842, 7956558153997246647,\n",
            "        6962991711471611645, 4234256539447291550, 7600107228008227842,\n",
            "        7956558153997246647, 6962991711471611645], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1002413408665974097, 3548588928869478987, 1458692326885999596,\n",
            "        7550816851747033172, 1002413408665974097, 3548588928869478987,\n",
            "        1458692326885999596, 7550816851747033172], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5347988506670325514,   34315732140025370, 4111490789891623251,\n",
            "        4152810341659423412, 5347988506670325514,   34315732140025370,\n",
            "        4111490789891623251, 4152810341659423412], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 968165099132372076, 4996950319687074747, 8021146751063511359,\n",
            "        9136079383034512841,  968165099132372076, 4996950319687074747,\n",
            "        8021146751063511359, 9136079383034512841], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7442825450003611781, 5563766919961668129, 4979183508751237987,\n",
            "        3349232724143455845, 7442825450003611781, 5563766919961668129,\n",
            "        4979183508751237987, 3349232724143455845], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8911117099120876208, 4755421335389571186, 9222859609412710102,\n",
            "        3955816898376730292, 8911117099120876208, 4755421335389571186,\n",
            "        9222859609412710102, 3955816898376730292], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8813760750494812542, 5677691350334726697, 3801480146538713457,\n",
            "        4936725192985743262, 8813760750494812542, 5677691350334726697,\n",
            "        3801480146538713457, 4936725192985743262], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6653611287401641164,    1391091015450527, 7642115007984888878,\n",
            "        6892205053128369261, 6653611287401641164,    1391091015450527,\n",
            "        7642115007984888878, 6892205053128369261], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 451211790519651118, 6387447603675106148, 5371579114630565051,\n",
            "        4387470183161293527,  451211790519651118, 6387447603675106148,\n",
            "        5371579114630565051, 4387470183161293527], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1549497628103396294,  330641732699822758,   73930973695623112,\n",
            "        2937783723522265715, 1549497628103396294,  330641732699822758,\n",
            "          73930973695623112, 2937783723522265715], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8892387688610209315, 6626720821289722176, 1221352273662908448,\n",
            "        7612070075729034270, 8892387688610209315, 6626720821289722176,\n",
            "        1221352273662908448, 7612070075729034270], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1769564454596344257,  643752906144644592, 4658634284825323744,\n",
            "        5470934193957444942, 1769564454596344257,  643752906144644592,\n",
            "        4658634284825323744, 5470934193957444942], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7523701113837704945, 5653219169605003035, 3095093639089822488,\n",
            "        1739256047408054706, 7523701113837704945, 5653219169605003035,\n",
            "        3095093639089822488, 1739256047408054706], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([  49089127656566830, 1259446897656021910, 8479837919300533484,\n",
            "        5675422039565715324,   49089127656566830, 1259446897656021910,\n",
            "        8479837919300533484, 5675422039565715324], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2961229698498725733, 7457975820461350532, 1288408522951810476,\n",
            "        5517170517936838215, 2961229698498725733, 7457975820461350532,\n",
            "        1288408522951810476, 5517170517936838215], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6591703537517372279, 8984747551246089343, 4753389377266554966,\n",
            "        8329278952105344153, 6591703537517372279, 8984747551246089343,\n",
            "        4753389377266554966, 8329278952105344153], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3098369112945069516, 9010722675244062816, 4437161492699735251,\n",
            "        3967316651252437027, 3098369112945069516, 9010722675244062816,\n",
            "        4437161492699735251, 3967316651252437027], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3718731796170472268, 2361087645155076235, 7784833375756792893,\n",
            "        1214759803809499172, 3718731796170472268, 2361087645155076235,\n",
            "        7784833375756792893, 1214759803809499172], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2668151557408097463, 8293622751206947712,  403035889257120252,\n",
            "        6545335817920571726, 2668151557408097463, 8293622751206947712,\n",
            "         403035889257120252, 6545335817920571726], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2439721358236188122, 5456035497688614811, 1061111562476425395,\n",
            "        4234332356100253037, 2439721358236188122, 5456035497688614811,\n",
            "        1061111562476425395, 4234332356100253037], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6524506491896729314, 2273257944999499984, 4837766908445256488,\n",
            "        7177042944294562900, 6524506491896729314, 2273257944999499984,\n",
            "        4837766908445256488, 7177042944294562900], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2625877683973254041, 2649573137944313905, 3305411202305964382,\n",
            "        1909099359265466818, 2625877683973254041, 2649573137944313905,\n",
            "        3305411202305964382, 1909099359265466818], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2124152336186715639, 5881033591714095535, 1483978286158162951,\n",
            "        7358364155818753656, 2124152336186715639, 5881033591714095535,\n",
            "        1483978286158162951, 7358364155818753656], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([  66641897719607196, 2377750996917551325, 6142701366581089751,\n",
            "        9041292348969933077,   66641897719607196, 2377750996917551325,\n",
            "        6142701366581089751, 9041292348969933077], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6480160235634306876, 2014336275337210955, 1518441057483274625,\n",
            "        7978873539787925510, 6480160235634306876, 2014336275337210955,\n",
            "        1518441057483274625, 7978873539787925510], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2559418709696406950, 8099410831739022005, 4693589621092725016,\n",
            "        6857075697056625811, 2559418709696406950, 8099410831739022005,\n",
            "        4693589621092725016, 6857075697056625811], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2568048146314325695, 8495616166980746087, 4385846650410990411,\n",
            "        7773561474165262619, 2568048146314325695, 8495616166980746087,\n",
            "        4385846650410990411, 7773561474165262619], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2811102707132515649, 2688434459234264465,  175586462031537901,\n",
            "         811017940699586921, 2811102707132515649, 2688434459234264465,\n",
            "         175586462031537901,  811017940699586921], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2122793944728221456, 7356034735120337884, 4183471720319244842,\n",
            "        7882267911779219145, 2122793944728221456, 7356034735120337884,\n",
            "        4183471720319244842, 7882267911779219145], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4984348324187172728, 2608935534460491505, 3618558343482865891,\n",
            "        1580499958238672050, 4984348324187172728, 2608935534460491505,\n",
            "        3618558343482865891, 1580499958238672050], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3751688740198715805, 6743463503824838445,  971060087701974024,\n",
            "        1252259317926196022, 3751688740198715805, 6743463503824838445,\n",
            "         971060087701974024, 1252259317926196022], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3320129282729274989, 3791302622781557021, 5468274824903514047,\n",
            "        4746725559417782683, 3320129282729274989, 3791302622781557021,\n",
            "        5468274824903514047, 4746725559417782683], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8444332118260632330,  126388481511434647, 8318124976614436694,\n",
            "        2702046446553714103, 8444332118260632330,  126388481511434647,\n",
            "        8318124976614436694, 2702046446553714103], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3955153778622039186, 8586072136812084282,  452405189402622588,\n",
            "        6442628877869006701, 3955153778622039186, 8586072136812084282,\n",
            "         452405189402622588, 6442628877869006701], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3336514143566268704, 5170385474664432905, 4077105585049354444,\n",
            "        3367068716845305463, 3336514143566268704, 5170385474664432905,\n",
            "        4077105585049354444, 3367068716845305463], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1517986802867469621, 1359078010177251209, 6074477273766689798,\n",
            "        2323523442168651625, 1517986802867469621, 1359078010177251209,\n",
            "        6074477273766689798, 2323523442168651625], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4346144251478257496, 6585937447094072998,  290282831618348369,\n",
            "        8986419115797721984, 4346144251478257496, 6585937447094072998,\n",
            "         290282831618348369, 8986419115797721984], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6877385498064745040, 3716066247088258789, 4777441041211580686,\n",
            "        1983853451587731688, 6877385498064745040, 3716066247088258789,\n",
            "        4777441041211580686, 1983853451587731688], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2666133180488878525,   53853891112810613, 7574408167811323982,\n",
            "        7329374162038493426, 2666133180488878525,   53853891112810613,\n",
            "        7574408167811323982, 7329374162038493426], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6840408313444901218, 7189251237955131962, 5995517631257308106,\n",
            "        1124028561131153720, 6840408313444901218, 7189251237955131962,\n",
            "        5995517631257308106, 1124028561131153720], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 148613634122646295, 3795196016486616309, 8891772936476539077,\n",
            "        3933450979159226153,  148613634122646295, 3795196016486616309,\n",
            "        8891772936476539077, 3933450979159226153], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1208190263481926376, 6172965110503947091, 3069430202649601474,\n",
            "        1715547115639096209, 1208190263481926376, 6172965110503947091,\n",
            "        3069430202649601474, 1715547115639096209], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5930231957914362324, 2763151896113203227, 8516189769055701142,\n",
            "        5431952865047582375, 5930231957914362324, 2763151896113203227,\n",
            "        8516189769055701142, 5431952865047582375], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8832702312323942657,  680077218630906383, 8174335851961489200,\n",
            "        6732700887356646370, 8832702312323942657,  680077218630906383,\n",
            "        8174335851961489200, 6732700887356646370], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7141699983056493462, 6630321108222750407, 4702004671294810115,\n",
            "        1630081762645152174, 7141699983056493462, 6630321108222750407,\n",
            "        4702004671294810115, 1630081762645152174], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 357590022186106549, 4751785844751405539, 7546056669402781246,\n",
            "        2443294221477746832,  357590022186106549, 4751785844751405539,\n",
            "        7546056669402781246, 2443294221477746832], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7855765095658887453, 4929619499177775218, 5642917152966375340,\n",
            "        2883572329820538090, 7855765095658887453, 4929619499177775218,\n",
            "        5642917152966375340, 2883572329820538090], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5057021188855586371, 8325058263190303026, 1867589477208153540,\n",
            "        8160509780809184397, 5057021188855586371, 8325058263190303026,\n",
            "        1867589477208153540, 8160509780809184397], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1526823511600816106, 5112174209624979062, 7769616233994733939,\n",
            "        5599339312300106984, 1526823511600816106, 5112174209624979062,\n",
            "        7769616233994733939, 5599339312300106984], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3343277430802996182, 6797998278591912202, 8994025250243204161,\n",
            "        3546058417607763814, 3343277430802996182, 6797998278591912202,\n",
            "        8994025250243204161, 3546058417607763814], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7740870842122211055, 4768327471994016818, 1545316633050072096,\n",
            "        3281212582024690800, 7740870842122211055, 4768327471994016818,\n",
            "        1545316633050072096, 3281212582024690800], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 350089070692137296, 1996317775182887369, 1853081649958227454,\n",
            "         491413139890419548,  350089070692137296, 1996317775182887369,\n",
            "        1853081649958227454,  491413139890419548], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8020189940760767452, 4770950883226262900, 2143086068915094132,\n",
            "        7072477041647460488, 8020189940760767452, 4770950883226262900,\n",
            "        2143086068915094132, 7072477041647460488], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1307253692802843920,  492111239293378600, 1673689397264324100,\n",
            "        7739653677895073856, 1307253692802843920,  492111239293378600,\n",
            "        1673689397264324100, 7739653677895073856], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5181441410892817998, 3547998517146780058, 3977400587152208247,\n",
            "        3062323414473630620, 5181441410892817998, 3547998517146780058,\n",
            "        3977400587152208247, 3062323414473630620], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7720618396671075317, 1211641548280066276, 2336473158952791358,\n",
            "        1565791510028222872, 7720618396671075317, 1211641548280066276,\n",
            "        2336473158952791358, 1565791510028222872], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4379864619221518042, 2041354297974278614, 7946494421215652418,\n",
            "        8295803791734616743, 4379864619221518042, 2041354297974278614,\n",
            "        7946494421215652418, 8295803791734616743], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7031225989866407117, 3313964858889484977, 4636094496960049093,\n",
            "        5291035546929381140, 7031225989866407117, 3313964858889484977,\n",
            "        4636094496960049093, 5291035546929381140], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 971602216421418977, 3907055677739194686, 7608033586642503123,\n",
            "        6261532851857734598,  971602216421418977, 3907055677739194686,\n",
            "        7608033586642503123, 6261532851857734598], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 820039512667538750, 2349999775342686369, 7446354488155006790,\n",
            "        7225323770863742045,  820039512667538750, 2349999775342686369,\n",
            "        7446354488155006790, 7225323770863742045], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5414840233456755089, 7812526803750164868, 8719286025459541352,\n",
            "         924365819552160534, 5414840233456755089, 7812526803750164868,\n",
            "        8719286025459541352,  924365819552160534], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6871834921204480128, 5280014013036840560, 2298197836193342900,\n",
            "        1604062533666414313, 6871834921204480128, 5280014013036840560,\n",
            "        2298197836193342900, 1604062533666414313], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6870275230232216791,  386982954591534141, 2138086979074522872,\n",
            "        1744204317854005302, 6870275230232216791,  386982954591534141,\n",
            "        2138086979074522872, 1744204317854005302], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5443959730371094957, 2977858443107895953, 2033465058408352105,\n",
            "        7333248996885632404, 5443959730371094957, 2977858443107895953,\n",
            "        2033465058408352105, 7333248996885632404], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 971372139012582256, 6407732728886632616, 4064898869790158785,\n",
            "         460021065433190658,  971372139012582256, 6407732728886632616,\n",
            "        4064898869790158785,  460021065433190658], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1407266453601853219, 4300487382355831071, 6745557511942407346,\n",
            "        5308058008473700250, 1407266453601853219, 4300487382355831071,\n",
            "        6745557511942407346, 5308058008473700250], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5726264220218144226,  152563158996753634, 1677235070996506063,\n",
            "        5429383239435011063, 5726264220218144226,  152563158996753634,\n",
            "        1677235070996506063, 5429383239435011063], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4457062044903596732, 5985403564025637815, 4392201809053131220,\n",
            "        3080722247873745476, 4457062044903596732, 5985403564025637815,\n",
            "        4392201809053131220, 3080722247873745476], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 743258853103382021, 8891131220255944977, 8467642316932179556,\n",
            "        6110228165497288417,  743258853103382021, 8891131220255944977,\n",
            "        8467642316932179556, 6110228165497288417], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 496488651917071237, 1521041855541115897, 2465753049129009603,\n",
            "         155064143036349742,  496488651917071237, 1521041855541115897,\n",
            "        2465753049129009603,  155064143036349742], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 964255631200654491, 7686401181042650423, 6198775171147677272,\n",
            "        5546847879632485377,  964255631200654491, 7686401181042650423,\n",
            "        6198775171147677272, 5546847879632485377], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([9069144973510071633, 7264199325313961670, 7766529325247847059,\n",
            "        3899567636063229130, 9069144973510071633, 7264199325313961670,\n",
            "        7766529325247847059, 3899567636063229130], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1678352973191158021, 2224452398641372402, 4136444724652700257,\n",
            "        8711655802212609801, 1678352973191158021, 2224452398641372402,\n",
            "        4136444724652700257, 8711655802212609801], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2394391025576455670,  844104225297037291, 3902328963914523457,\n",
            "        3780745322317907609, 2394391025576455670,  844104225297037291,\n",
            "        3902328963914523457, 3780745322317907609], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7706043779524725977, 4023493066589174498, 6533858349024278314,\n",
            "        4494017561750118638, 7706043779524725977, 4023493066589174498,\n",
            "        6533858349024278314, 4494017561750118638], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1729915329576126326, 3004670919324134117, 7635621557309265462,\n",
            "        2413078827822355056, 1729915329576126326, 3004670919324134117,\n",
            "        7635621557309265462, 2413078827822355056], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8007355617119434535, 5163086795398280788, 5703503269487994615,\n",
            "        5804745512584930711, 8007355617119434535, 5163086795398280788,\n",
            "        5703503269487994615, 5804745512584930711], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2395535378715403764, 3434681967142971287, 3894891718846519304,\n",
            "        1559135972344207840, 2395535378715403764, 3434681967142971287,\n",
            "        3894891718846519304, 1559135972344207840], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7885262489877364244, 5667502782837123557, 7802973453718455394,\n",
            "        2986918393136220175, 7885262489877364244, 5667502782837123557,\n",
            "        7802973453718455394, 2986918393136220175], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 641081825744299097, 7916895771248166842, 6084995821442021111,\n",
            "        3038167087701181312,  641081825744299097, 7916895771248166842,\n",
            "        6084995821442021111, 3038167087701181312], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2460497180936028360, 1774504682001020540, 7245311568175335103,\n",
            "        7537074777606513714, 2460497180936028360, 1774504682001020540,\n",
            "        7245311568175335103, 7537074777606513714], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6318364302236673351, 7626584479999749520, 5375019912074091677,\n",
            "         762899144009802991, 6318364302236673351, 7626584479999749520,\n",
            "        5375019912074091677,  762899144009802991], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6022897302898532656,  995180148540959806, 8851914416014906388,\n",
            "        8754746944067747789, 6022897302898532656,  995180148540959806,\n",
            "        8851914416014906388, 8754746944067747789], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5037599011981560521, 2144606845973292334, 4140552036465290750,\n",
            "        1272234680070769321, 5037599011981560521, 2144606845973292334,\n",
            "        4140552036465290750, 1272234680070769321], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5497256428741846480, 8639817167884741101, 3179277699960826095,\n",
            "        4150145923470772846, 5497256428741846480, 8639817167884741101,\n",
            "        3179277699960826095, 4150145923470772846], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3772273629741010175, 6111080045636544473, 2327415711352177358,\n",
            "        2952958632578625637, 3772273629741010175, 6111080045636544473,\n",
            "        2327415711352177358, 2952958632578625637], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([9089036557677414529, 9064868501423079447, 6741366517906256657,\n",
            "        8538438530818106504, 9089036557677414529, 9064868501423079447,\n",
            "        6741366517906256657, 8538438530818106504], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8176796860179797054, 3865217912632145683, 9108103497837906192,\n",
            "        5736546565544306537, 8176796860179797054, 3865217912632145683,\n",
            "        9108103497837906192, 5736546565544306537], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6818308913146468023,  660091425031579268, 1081823227676638485,\n",
            "        8957918452833178930, 6818308913146468023,  660091425031579268,\n",
            "        1081823227676638485, 8957918452833178930], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2811204366430984436, 7095822328557837048, 6561631011036334256,\n",
            "         848535780356337947, 2811204366430984436, 7095822328557837048,\n",
            "        6561631011036334256,  848535780356337947], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5837873599984451123, 7368037331041111085, 3454987080412424397,\n",
            "        3361474986026084690, 5837873599984451123, 7368037331041111085,\n",
            "        3454987080412424397, 3361474986026084690], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6137119567346784996, 5963175935363282073, 1357287570115318254,\n",
            "        3482828992461132097, 6137119567346784996, 5963175935363282073,\n",
            "        1357287570115318254, 3482828992461132097], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7047028665453742530, 8336705580926757227, 6166499679775666890,\n",
            "        1103379791776817505, 7047028665453742530, 8336705580926757227,\n",
            "        6166499679775666890, 1103379791776817505], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8807779660312685563, 7715983860039438478, 1395407186660168031,\n",
            "        6421904312496962337, 8807779660312685563, 7715983860039438478,\n",
            "        1395407186660168031, 6421904312496962337], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2326518176446302178, 4008451512620970039, 5143098591202738565,\n",
            "        4706686180332462171, 2326518176446302178, 4008451512620970039,\n",
            "        5143098591202738565, 4706686180332462171], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5416950451454962345, 7113513000532060806, 6295712282553862259,\n",
            "        1575493214264070987, 5416950451454962345, 7113513000532060806,\n",
            "        6295712282553862259, 1575493214264070987], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8970674016436270129,  747651170919566228, 2198913773733349211,\n",
            "        5080708338868355712, 8970674016436270129,  747651170919566228,\n",
            "        2198913773733349211, 5080708338868355712], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3545624035541627918, 6497436905793490142,  342853009160107568,\n",
            "        1067273012334523086, 3545624035541627918, 6497436905793490142,\n",
            "         342853009160107568, 1067273012334523086], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6043111914831825118, 2536450268765574321, 6160635044001516878,\n",
            "        8825513064456678930, 6043111914831825118, 2536450268765574321,\n",
            "        6160635044001516878, 8825513064456678930], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2543319284825754247,   63594510263535743, 5136560628531197087,\n",
            "        2354800890000790849, 2543319284825754247,   63594510263535743,\n",
            "        5136560628531197087, 2354800890000790849], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6368982463757568125, 2072823810796595872, 2876168276155151858,\n",
            "        5727043264730075626, 6368982463757568125, 2072823810796595872,\n",
            "        2876168276155151858, 5727043264730075626], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3900767474745332570,  852762544547376059, 5863909142035689604,\n",
            "           8316685395600431, 3900767474745332570,  852762544547376059,\n",
            "        5863909142035689604,    8316685395600431], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 406951703521678139, 7251846316011850293, 6816238379677512914,\n",
            "        1074816446942128975,  406951703521678139, 7251846316011850293,\n",
            "        6816238379677512914, 1074816446942128975], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1338754155186711550, 2536718788118801173, 4807703554794338565,\n",
            "        8474738744613148001, 1338754155186711550, 2536718788118801173,\n",
            "        4807703554794338565, 8474738744613148001], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2303561547579288718, 8425048316555879639, 8655458323280480749,\n",
            "        6066368144358191399, 2303561547579288718, 8425048316555879639,\n",
            "        8655458323280480749, 6066368144358191399], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8444321371684417065, 5439968629249037107, 7354488984776408516,\n",
            "        4964001229310384795, 8444321371684417065, 5439968629249037107,\n",
            "        7354488984776408516, 4964001229310384795], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7912398426286517141, 8986846400731945960, 7742610858185695656,\n",
            "         915009920179619027, 7912398426286517141, 8986846400731945960,\n",
            "        7742610858185695656,  915009920179619027], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6105070356835449023,  922170975568610389, 1539561280362459466,\n",
            "        3045975865315265063, 6105070356835449023,  922170975568610389,\n",
            "        1539561280362459466, 3045975865315265063], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8691823486268630341, 3350151090079398534, 5351607209615853962,\n",
            "        8015155032507951255, 8691823486268630341, 3350151090079398534,\n",
            "        5351607209615853962, 8015155032507951255], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8064788678432100856, 1265166227945484203, 7181971987621322013,\n",
            "         447560781985001058, 8064788678432100856, 1265166227945484203,\n",
            "        7181971987621322013,  447560781985001058], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3387666420006458965, 7844315677961533949, 8173727209095925308,\n",
            "        6620738304490933584, 3387666420006458965, 7844315677961533949,\n",
            "        8173727209095925308, 6620738304490933584], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6018147526109439449, 6726369096403756399, 2334574092653893463,\n",
            "        4051456929713490897, 6018147526109439449, 6726369096403756399,\n",
            "        2334574092653893463, 4051456929713490897], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3823636475997057984,   31120625774181134, 3846544680069237013,\n",
            "        1702348729101661486, 3823636475997057984,   31120625774181134,\n",
            "        3846544680069237013, 1702348729101661486], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 461988043481042059, 8320922370122382147, 4757182785925061016,\n",
            "        8797549931247137306,  461988043481042059, 8320922370122382147,\n",
            "        4757182785925061016, 8797549931247137306], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1529678304913929331, 1896256898753234976, 4749309450724660857,\n",
            "        3297588962661516530, 1529678304913929331, 1896256898753234976,\n",
            "        4749309450724660857, 3297588962661516530], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4207141835116148302, 1365825751437510135, 6078346547048442921,\n",
            "        1456406243394387575, 4207141835116148302, 1365825751437510135,\n",
            "        6078346547048442921, 1456406243394387575], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 728623402312441148, 4520990269422431434, 8881041077697035793,\n",
            "        3186663735571360495,  728623402312441148, 4520990269422431434,\n",
            "        8881041077697035793, 3186663735571360495], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([1537869138023868068, 9201521335894109057, 2254028775702644793,\n",
            "        4468815300357921629, 1537869138023868068, 9201521335894109057,\n",
            "        2254028775702644793, 4468815300357921629], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3959927164460624128, 9043166188803150364, 7480968788519519815,\n",
            "        2134853192654416602, 3959927164460624128, 9043166188803150364,\n",
            "        7480968788519519815, 2134853192654416602], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7452111599163884563, 8193951329771958340,  163960929888637103,\n",
            "         311130422371298470, 7452111599163884563, 8193951329771958340,\n",
            "         163960929888637103,  311130422371298470], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4289417269037892898, 4129596229108358669, 3215521509069053115,\n",
            "        2256316834937132466, 4289417269037892898, 4129596229108358669,\n",
            "        3215521509069053115, 2256316834937132466], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6688883658700685441,  966317670757064301, 3710259306947010219,\n",
            "        8974130757794030799, 6688883658700685441,  966317670757064301,\n",
            "        3710259306947010219, 8974130757794030799], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4751053223863447083, 7736967993323469281, 3684795168507751383,\n",
            "        4096490110643752926, 4751053223863447083, 7736967993323469281,\n",
            "        3684795168507751383, 4096490110643752926], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4578953993101811173, 5847518429453517175, 8252485493216801116,\n",
            "        1684495052928800292, 4578953993101811173, 5847518429453517175,\n",
            "        8252485493216801116, 1684495052928800292], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8385365801297895127, 3544030878840136358, 6405434646027879199,\n",
            "        3612247201868718669, 8385365801297895127, 3544030878840136358,\n",
            "        6405434646027879199, 3612247201868718669], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8925098405939811570, 7061574199299593934, 2240626663877328749,\n",
            "        3985056623481788274, 8925098405939811570, 7061574199299593934,\n",
            "        2240626663877328749, 3985056623481788274], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8423480368791602178, 3836195270809441393, 9007191845678092063,\n",
            "        3994489917649604457, 8423480368791602178, 3836195270809441393,\n",
            "        9007191845678092063, 3994489917649604457], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([9006988284589396156, 1955890997092249583, 4716245218288973047,\n",
            "        4098345644029195306, 9006988284589396156, 1955890997092249583,\n",
            "        4716245218288973047, 4098345644029195306], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6751640337956182481, 6638817097218251063, 7487134577183557892,\n",
            "         552531666865432168, 6751640337956182481, 6638817097218251063,\n",
            "        7487134577183557892,  552531666865432168], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6963520823339329921, 6210680227804442280, 8146710749678008622,\n",
            "        7139275279627537154, 6963520823339329921, 6210680227804442280,\n",
            "        8146710749678008622, 7139275279627537154], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3252330486611145757, 6302430149253112808, 3364028246781201075,\n",
            "        5098130474460402888, 3252330486611145757, 6302430149253112808,\n",
            "        3364028246781201075, 5098130474460402888], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6752312799156613087,  887237859564212189, 3701778463858522880,\n",
            "        5683001585800130570, 6752312799156613087,  887237859564212189,\n",
            "        3701778463858522880, 5683001585800130570], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3637662260992945741, 3628678080598360691, 6603573857377935647,\n",
            "        6460958135557801430, 3637662260992945741, 3628678080598360691,\n",
            "        6603573857377935647, 6460958135557801430], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6191831943565995514, 2966197098613478744,  528292354783854603,\n",
            "        1125412536484753424, 6191831943565995514, 2966197098613478744,\n",
            "         528292354783854603, 1125412536484753424], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8091009664586702343, 3262884614041553884, 8468909820468970005,\n",
            "        8979454527996961563, 8091009664586702343, 3262884614041553884,\n",
            "        8468909820468970005, 8979454527996961563], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([5746408478152104229, 4297795160050965120, 7566156704832837060,\n",
            "        5769847174785331685, 5746408478152104229, 4297795160050965120,\n",
            "        7566156704832837060, 5769847174785331685], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3359615227038059151, 8800674375851597950, 7066636393440727814,\n",
            "        7601539190338690120, 3359615227038059151, 8800674375851597950,\n",
            "        7066636393440727814, 7601539190338690120], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7749846021144052922, 6931974055906087673, 5893872928733181285,\n",
            "        1252218938278888702, 7749846021144052922, 6931974055906087673,\n",
            "        5893872928733181285, 1252218938278888702], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2489136540604434058, 8618134188812341685, 5082740454891270658,\n",
            "        4366195636216978323, 2489136540604434058, 8618134188812341685,\n",
            "        5082740454891270658, 4366195636216978323], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7801016321200271434, 6062505810755057776, 3223259152679516038,\n",
            "         832164583452689216, 7801016321200271434, 6062505810755057776,\n",
            "        3223259152679516038,  832164583452689216], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4294392330717969053, 8966068710486873636, 6742857504056514155,\n",
            "         118826125605248313, 4294392330717969053, 8966068710486873636,\n",
            "        6742857504056514155,  118826125605248313], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2936504905236341610,   68506896714394873, 2843791554327906910,\n",
            "        8303482079987156080, 2936504905236341610,   68506896714394873,\n",
            "        2843791554327906910, 8303482079987156080], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7113493058742951887,  533153536014580572, 8406605853384471153,\n",
            "        3612403969233021416, 7113493058742951887,  533153536014580572,\n",
            "        8406605853384471153, 3612403969233021416], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6286474736435514533, 8391322015127676966, 6965444801430455723,\n",
            "        3269906901868215027, 6286474736435514533, 8391322015127676966,\n",
            "        6965444801430455723, 3269906901868215027], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7842290632497292308, 9133309131016429501, 2310795117846507554,\n",
            "        3984247080943304284, 7842290632497292308, 9133309131016429501,\n",
            "        2310795117846507554, 3984247080943304284], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([3250782435067462757, 2453573774520343585, 4890512397533113016,\n",
            "         858424328690205912, 3250782435067462757, 2453573774520343585,\n",
            "        4890512397533113016,  858424328690205912], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6779571895244079900, 8360419457276463835, 7377834979994019080,\n",
            "        5621589833380544163, 6779571895244079900, 8360419457276463835,\n",
            "        7377834979994019080, 5621589833380544163], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7929670604204541471, 5153083030797126279, 8853273276383479995,\n",
            "        4552803252934565130, 7929670604204541471, 5153083030797126279,\n",
            "        8853273276383479995, 4552803252934565130], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7690663617965437787, 6620135050944167589, 3437938985452237596,\n",
            "        1061982879234913255, 7690663617965437787, 6620135050944167589,\n",
            "        3437938985452237596, 1061982879234913255], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4086701736700231363, 5388268488543763837, 6694070436801592345,\n",
            "         425454572600896595, 4086701736700231363, 5388268488543763837,\n",
            "        6694070436801592345,  425454572600896595], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([9163782599161522277, 1656398840318246832, 9217092047232299970,\n",
            "        3466027709250441782, 9163782599161522277, 1656398840318246832,\n",
            "        9217092047232299970, 3466027709250441782], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7174792214536565229, 8706206589652974208, 8200621656672811848,\n",
            "        2776958873378918125, 7174792214536565229, 8706206589652974208,\n",
            "        8200621656672811848, 2776958873378918125], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 459061879781072400, 7588376212011749186, 6643540043839201822,\n",
            "        2074933292472089384,  459061879781072400, 7588376212011749186,\n",
            "        6643540043839201822, 2074933292472089384], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([2468416927382488369, 1902311188641202628, 9036849251161011901,\n",
            "        1200656993012645514, 2468416927382488369, 1902311188641202628,\n",
            "        9036849251161011901, 1200656993012645514], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 837495494682786117,  435339787360342292, 2379844992694861998,\n",
            "        6977744434278962923,  837495494682786117,  435339787360342292,\n",
            "        2379844992694861998, 6977744434278962923], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8781797578040708182, 3982457161056608927, 1787464929796483001,\n",
            "        4109472538475729735, 8781797578040708182, 3982457161056608927,\n",
            "        1787464929796483001, 4109472538475729735], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4098701582083557777, 7525876953129229582, 2721647924224179629,\n",
            "        2374913002419279573, 4098701582083557777, 7525876953129229582,\n",
            "        2721647924224179629, 2374913002419279573], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([8323992971206206289,  125181593418295654, 6000663639009042050,\n",
            "        7319750918454289383, 8323992971206206289,  125181593418295654,\n",
            "        6000663639009042050, 7319750918454289383], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4294280007158341764, 3783218015681381466, 5366334761035441500,\n",
            "         539699857046353453, 4294280007158341764, 3783218015681381466,\n",
            "        5366334761035441500,  539699857046353453], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4058572465682168239, 2746476713141791787, 1098481638444278748,\n",
            "        3050536278257544878, 4058572465682168239, 2746476713141791787,\n",
            "        1098481638444278748, 3050536278257544878], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([ 964938234708199109, 4503654809199239096, 6899037277546649251,\n",
            "        8106079840566229489,  964938234708199109, 4503654809199239096,\n",
            "        6899037277546649251, 8106079840566229489], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([7377906773207272689, 7130901732378689344,  225803094838292849,\n",
            "        8408581344595727496, 7377906773207272689, 7130901732378689344,\n",
            "         225803094838292849, 8408581344595727496], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([6732710242015340225,  477399746868268464, 3156573652565110770,\n",
            "        5911969170140978451, 6732710242015340225,  477399746868268464,\n",
            "        3156573652565110770, 5911969170140978451], device='cuda:0') with shape: torch.Size([8])\n",
            "embeddings inside loss: torch.Size([8, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([8, 768]) and labels:tensor([4362616892204824429, 4759839745983391022, 8061542636796683545,\n",
            "        1854482549245037420, 4362616892204824429, 4759839745983391022,\n",
            "        8061542636796683545, 1854482549245037420], device='cuda:0') with shape: torch.Size([8])\n",
            "2022-09-21 13:14:47,933 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2022-09-21 13:14:47,934 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   796.000  |       N/A\n",
            "2022-09-21 13:14:47,934 - INFO - allennlp.training.tensorboard_writer - loss               |     2.731  |       N/A\n",
            "2022-09-21 13:14:47,934 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |     0.000  |       N/A\n",
            "2022-09-21 13:15:01,569 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'E:/saved_models/declutr/wiki/output/best.th'.\n",
            "2022-09-21 13:15:01,941 - INFO - allennlp.training.trainer - Epoch duration: 0:01:49.520769\n",
            "2022-09-21 13:15:01,942 - INFO - allennlp.training.checkpointer - loading best weights\n",
            "2022-09-21 13:15:02,156 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"best_epoch\": 0,\n",
            "  \"peak_worker_0_memory_MB\": 0,\n",
            "  \"peak_gpu_0_memory_MB\": 796,\n",
            "  \"training_duration\": \"0:01:35.513162\",\n",
            "  \"training_start_epoch\": 0,\n",
            "  \"training_epochs\": 0,\n",
            "  \"epoch\": 0,\n",
            "  \"training_loss\": 2.7308712019920347,\n",
            "  \"training_worker_0_memory_MB\": 0.0,\n",
            "  \"training_gpu_0_memory_MB\": 796\n",
            "}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "reading instances: 0it [00:00, ?it/s]\n",
            "reading instances: 1it [00:00,  9.61it/s]\n",
            "reading instances: 5it [00:00, 21.78it/s]\n",
            "reading instances: 9it [00:00, 24.29it/s]\n",
            "reading instances: 12it [00:00, 24.40it/s]\n",
            "reading instances: 16it [00:00, 27.60it/s]\n",
            "reading instances: 20it [00:00, 29.40it/s]\n",
            "reading instances: 23it [00:00, 28.30it/s]\n",
            "reading instances: 27it [00:01, 28.68it/s]\n",
            "reading instances: 30it [00:01, 27.02it/s]\n",
            "reading instances: 33it [00:01, 26.82it/s]\n",
            "reading instances: 36it [00:01, 27.02it/s]\n",
            "reading instances: 39it [00:01, 27.60it/s]\n",
            "reading instances: 42it [00:01, 28.26it/s]\n",
            "reading instances: 46it [00:01, 30.80it/s]\n",
            "reading instances: 50it [00:01, 30.39it/s]\n",
            "reading instances: 55it [00:01, 33.75it/s]\n",
            "reading instances: 59it [00:02, 34.41it/s]\n",
            "reading instances: 63it [00:02, 28.04it/s]\n",
            "reading instances: 67it [00:02, 28.38it/s]\n",
            "reading instances: 71it [00:02, 30.11it/s]\n",
            "reading instances: 75it [00:02, 30.79it/s]\n",
            "reading instances: 79it [00:02, 30.30it/s]\n",
            "reading instances: 83it [00:02, 30.65it/s]\n",
            "reading instances: 87it [00:02, 32.80it/s]\n",
            "reading instances: 91it [00:03, 33.79it/s]\n",
            "reading instances: 95it [00:03, 32.42it/s]\n",
            "reading instances: 99it [00:03, 32.06it/s]\n",
            "reading instances: 103it [00:03, 31.97it/s]\n",
            "reading instances: 107it [00:03, 28.31it/s]\n",
            "reading instances: 110it [00:03, 27.36it/s]\n",
            "reading instances: 113it [00:03, 23.29it/s]\n",
            "reading instances: 116it [00:04, 24.46it/s]\n",
            "reading instances: 120it [00:04, 26.76it/s]\n",
            "reading instances: 124it [00:04, 27.88it/s]\n",
            "reading instances: 128it [00:04, 29.69it/s]\n",
            "reading instances: 132it [00:04, 30.23it/s]\n",
            "reading instances: 136it [00:04, 30.32it/s]\n",
            "reading instances: 140it [00:04, 31.48it/s]\n",
            "reading instances: 144it [00:04, 30.97it/s]\n",
            "reading instances: 148it [00:05, 29.79it/s]\n",
            "reading instances: 152it [00:05, 29.28it/s]\n",
            "reading instances: 156it [00:05, 30.19it/s]\n",
            "reading instances: 160it [00:05, 31.51it/s]\n",
            "reading instances: 164it [00:05, 26.86it/s]\n",
            "reading instances: 168it [00:05, 28.10it/s]\n",
            "reading instances: 172it [00:05, 29.43it/s]\n",
            "reading instances: 176it [00:06, 29.88it/s]\n",
            "reading instances: 180it [00:06, 31.65it/s]\n",
            "reading instances: 184it [00:06, 33.01it/s]\n",
            "reading instances: 188it [00:06, 34.21it/s]\n",
            "reading instances: 192it [00:06, 32.06it/s]\n",
            "reading instances: 196it [00:06, 33.41it/s]\n",
            "reading instances: 200it [00:06, 33.98it/s]\n",
            "reading instances: 204it [00:06, 32.47it/s]\n",
            "reading instances: 208it [00:06, 33.13it/s]\n",
            "reading instances: 212it [00:07, 31.32it/s]\n",
            "reading instances: 216it [00:07, 30.10it/s]\n",
            "reading instances: 220it [00:07, 31.44it/s]\n",
            "reading instances: 224it [00:07, 31.31it/s]\n",
            "reading instances: 228it [00:07, 31.96it/s]\n",
            "reading instances: 232it [00:07, 30.72it/s]\n",
            "reading instances: 236it [00:08, 25.31it/s]\n",
            "reading instances: 240it [00:08, 26.57it/s]\n",
            "reading instances: 243it [00:08, 26.06it/s]\n",
            "reading instances: 246it [00:08, 25.44it/s]\n",
            "reading instances: 250it [00:08, 26.74it/s]\n",
            "reading instances: 254it [00:08, 28.74it/s]\n",
            "reading instances: 258it [00:08, 28.69it/s]\n",
            "reading instances: 262it [00:08, 29.77it/s]\n",
            "reading instances: 266it [00:09, 29.20it/s]\n",
            "reading instances: 269it [00:09, 27.84it/s]\n",
            "reading instances: 273it [00:09, 30.03it/s]\n",
            "reading instances: 277it [00:09, 29.69it/s]\n",
            "reading instances: 281it [00:09, 30.95it/s]\n",
            "reading instances: 285it [00:09, 33.00it/s]\n",
            "reading instances: 289it [00:09, 31.90it/s]\n",
            "reading instances: 293it [00:09, 31.55it/s]\n",
            "reading instances: 297it [00:10, 31.99it/s]\n",
            "reading instances: 301it [00:10, 28.61it/s]\n",
            "reading instances: 305it [00:10, 29.62it/s]\n",
            "reading instances: 309it [00:10, 30.23it/s]\n",
            "reading instances: 313it [00:10, 29.71it/s]\n",
            "reading instances: 317it [00:10, 29.29it/s]\n",
            "reading instances: 320it [00:10, 22.47it/s]\n",
            "reading instances: 323it [00:11, 23.54it/s]\n",
            "reading instances: 327it [00:11, 26.18it/s]\n",
            "reading instances: 330it [00:11, 26.89it/s]\n",
            "reading instances: 334it [00:11, 27.24it/s]\n",
            "reading instances: 338it [00:11, 28.48it/s]\n",
            "reading instances: 342it [00:11, 30.20it/s]\n",
            "reading instances: 346it [00:11, 29.54it/s]\n",
            "reading instances: 350it [00:11, 27.33it/s]\n",
            "reading instances: 354it [00:12, 27.01it/s]\n",
            "reading instances: 357it [00:12, 27.20it/s]\n",
            "reading instances: 361it [00:12, 29.79it/s]\n",
            "reading instances: 365it [00:12, 31.73it/s]\n",
            "reading instances: 369it [00:12, 31.58it/s]\n",
            "reading instances: 373it [00:12, 32.26it/s]\n",
            "reading instances: 377it [00:12, 32.66it/s]\n",
            "reading instances: 381it [00:13, 28.77it/s]\n",
            "reading instances: 384it [00:13, 28.66it/s]\n",
            "reading instances: 387it [00:13, 28.42it/s]\n",
            "reading instances: 391it [00:13, 29.23it/s]\n",
            "reading instances: 394it [00:13, 29.20it/s]\n",
            "reading instances: 397it [00:13, 28.95it/s]\n",
            "reading instances: 400it [00:13, 29.00it/s]\n",
            "reading instances: 404it [00:13, 29.59it/s]\n",
            "reading instances: 408it [00:13, 30.81it/s]\n",
            "reading instances: 412it [00:14, 32.74it/s]\n",
            "reading instances: 416it [00:14, 33.19it/s]\n",
            "reading instances: 420it [00:14, 34.20it/s]\n",
            "reading instances: 424it [00:14, 22.54it/s]\n",
            "reading instances: 428it [00:14, 24.87it/s]\n",
            "reading instances: 432it [00:14, 27.56it/s]\n",
            "reading instances: 436it [00:14, 27.92it/s]\n",
            "reading instances: 441it [00:15, 30.95it/s]\n",
            "reading instances: 445it [00:15, 31.89it/s]\n",
            "reading instances: 449it [00:15, 32.29it/s]\n",
            "reading instances: 453it [00:15, 32.59it/s]\n",
            "reading instances: 457it [00:15, 30.18it/s]\n",
            "reading instances: 461it [00:15, 29.94it/s]\n",
            "reading instances: 465it [00:15, 28.10it/s]\n",
            "reading instances: 469it [00:15, 29.03it/s]\n",
            "reading instances: 472it [00:16, 29.19it/s]\n",
            "reading instances: 475it [00:16, 27.92it/s]\n",
            "reading instances: 478it [00:16, 27.67it/s]\n",
            "reading instances: 481it [00:16, 25.36it/s]\n",
            "reading instances: 484it [00:16, 25.32it/s]\n",
            "reading instances: 487it [00:16, 25.73it/s]\n",
            "reading instances: 491it [00:16, 28.56it/s]\n",
            "reading instances: 494it [00:16, 28.41it/s]\n",
            "reading instances: 498it [00:17, 29.85it/s]\n",
            "reading instances: 500it [00:17, 29.19it/s]\n",
            "\n",
            "  0%|          | 0/250 [00:00<?, ?it/s]\n",
            "batch_loss: 4.2543, loss: 4.2543 ||:   0%|          | 1/250 [00:00<03:26,  1.21it/s]\n",
            "batch_loss: 4.2006, loss: 4.2275 ||:   1%|          | 2/250 [00:01<02:14,  1.84it/s]\n",
            "batch_loss: 4.5083, loss: 4.3211 ||:   1%|1         | 3/250 [00:01<01:51,  2.21it/s]\n",
            "batch_loss: 4.8261, loss: 4.4473 ||:   2%|1         | 4/250 [00:01<01:37,  2.52it/s]\n",
            "batch_loss: 4.0560, loss: 4.3691 ||:   2%|2         | 5/250 [00:02<01:39,  2.47it/s]\n",
            "batch_loss: 4.2769, loss: 4.3537 ||:   2%|2         | 6/250 [00:02<01:38,  2.48it/s]\n",
            "batch_loss: 4.2096, loss: 4.3331 ||:   3%|2         | 7/250 [00:03<01:37,  2.48it/s]\n",
            "batch_loss: 3.7943, loss: 4.2658 ||:   3%|3         | 8/250 [00:03<01:34,  2.55it/s]\n",
            "batch_loss: 4.4232, loss: 4.2833 ||:   4%|3         | 9/250 [00:03<01:35,  2.51it/s]\n",
            "batch_loss: 4.3903, loss: 4.2940 ||:   4%|4         | 10/250 [00:04<01:35,  2.52it/s]\n",
            "batch_loss: 3.8854, loss: 4.2568 ||:   4%|4         | 11/250 [00:04<01:30,  2.64it/s]\n",
            "batch_loss: 4.1277, loss: 4.2461 ||:   5%|4         | 12/250 [00:04<01:30,  2.64it/s]\n",
            "batch_loss: 3.5829, loss: 4.1951 ||:   5%|5         | 13/250 [00:05<01:29,  2.66it/s]\n",
            "batch_loss: 2.9028, loss: 4.1028 ||:   6%|5         | 14/250 [00:05<01:29,  2.65it/s]\n",
            "batch_loss: 3.5201, loss: 4.0639 ||:   6%|6         | 15/250 [00:06<01:34,  2.49it/s]\n",
            "batch_loss: 3.7009, loss: 4.0412 ||:   6%|6         | 16/250 [00:06<01:32,  2.53it/s]\n",
            "batch_loss: 3.4092, loss: 4.0040 ||:   7%|6         | 17/250 [00:06<01:29,  2.61it/s]\n",
            "batch_loss: 2.7946, loss: 3.9368 ||:   7%|7         | 18/250 [00:07<01:30,  2.56it/s]\n",
            "batch_loss: 3.5029, loss: 3.9140 ||:   8%|7         | 19/250 [00:07<01:26,  2.66it/s]\n",
            "batch_loss: 3.7973, loss: 3.9082 ||:   8%|8         | 20/250 [00:07<01:22,  2.80it/s]\n",
            "batch_loss: 3.4171, loss: 3.8848 ||:   8%|8         | 21/250 [00:08<01:22,  2.79it/s]\n",
            "batch_loss: 3.6479, loss: 3.8740 ||:   9%|8         | 22/250 [00:08<01:23,  2.74it/s]\n",
            "batch_loss: 3.2193, loss: 3.8455 ||:   9%|9         | 23/250 [00:09<01:23,  2.72it/s]\n",
            "batch_loss: 2.7743, loss: 3.8009 ||:  10%|9         | 24/250 [00:09<01:29,  2.53it/s]\n",
            "batch_loss: 3.1126, loss: 3.7734 ||:  10%|#         | 25/250 [00:09<01:26,  2.61it/s]\n",
            "batch_loss: 3.0003, loss: 3.7436 ||:  10%|#         | 26/250 [00:10<01:21,  2.76it/s]\n",
            "batch_loss: 3.4675, loss: 3.7334 ||:  11%|#         | 27/250 [00:10<01:19,  2.79it/s]\n",
            "batch_loss: 4.5390, loss: 3.7622 ||:  11%|#1        | 28/250 [00:10<01:16,  2.89it/s]\n",
            "batch_loss: 3.0054, loss: 3.7361 ||:  12%|#1        | 29/250 [00:11<01:16,  2.90it/s]\n",
            "batch_loss: 3.1817, loss: 3.7176 ||:  12%|#2        | 30/250 [00:11<01:17,  2.83it/s]\n",
            "batch_loss: 3.8633, loss: 3.7223 ||:  12%|#2        | 31/250 [00:11<01:18,  2.80it/s]\n",
            "batch_loss: 2.9687, loss: 3.6988 ||:  13%|#2        | 32/250 [00:12<01:19,  2.76it/s]\n",
            "batch_loss: 3.2971, loss: 3.6866 ||:  13%|#3        | 33/250 [00:12<01:18,  2.76it/s]\n",
            "batch_loss: 2.9246, loss: 3.6642 ||:  14%|#3        | 34/250 [00:13<01:19,  2.71it/s]\n",
            "batch_loss: 3.0113, loss: 3.6455 ||:  14%|#4        | 35/250 [00:13<01:17,  2.77it/s]\n",
            "batch_loss: 3.2500, loss: 3.6345 ||:  14%|#4        | 36/250 [00:13<01:20,  2.65it/s]\n",
            "batch_loss: 2.8629, loss: 3.6137 ||:  15%|#4        | 37/250 [00:14<01:19,  2.67it/s]\n",
            "batch_loss: 3.1711, loss: 3.6020 ||:  15%|#5        | 38/250 [00:14<01:21,  2.62it/s]\n",
            "batch_loss: 2.7646, loss: 3.5806 ||:  16%|#5        | 39/250 [00:14<01:18,  2.68it/s]\n",
            "batch_loss: 3.2023, loss: 3.5711 ||:  16%|#6        | 40/250 [00:15<01:14,  2.83it/s]\n",
            "batch_loss: 3.6169, loss: 3.5722 ||:  16%|#6        | 41/250 [00:15<01:08,  3.07it/s]\n",
            "batch_loss: 2.7385, loss: 3.5524 ||:  17%|#6        | 42/250 [00:15<01:11,  2.92it/s]\n",
            "batch_loss: 3.2943, loss: 3.5464 ||:  17%|#7        | 43/250 [00:16<01:08,  3.00it/s]\n",
            "batch_loss: 2.7883, loss: 3.5291 ||:  18%|#7        | 44/250 [00:16<01:10,  2.91it/s]\n",
            "batch_loss: 3.7349, loss: 3.5337 ||:  18%|#8        | 45/250 [00:16<01:13,  2.79it/s]\n",
            "batch_loss: 3.1728, loss: 3.5259 ||:  18%|#8        | 46/250 [00:17<01:09,  2.93it/s]\n",
            "batch_loss: 2.9416, loss: 3.5134 ||:  19%|#8        | 47/250 [00:17<01:09,  2.94it/s]\n",
            "batch_loss: 3.7067, loss: 3.5175 ||:  19%|#9        | 48/250 [00:17<01:08,  2.93it/s]\n",
            "batch_loss: 3.1801, loss: 3.5106 ||:  20%|#9        | 49/250 [00:18<01:09,  2.89it/s]\n",
            "batch_loss: 2.6875, loss: 3.4941 ||:  20%|##        | 50/250 [00:18<01:06,  3.00it/s]\n",
            "batch_loss: 2.9590, loss: 3.4836 ||:  20%|##        | 51/250 [00:19<01:12,  2.75it/s]\n",
            "batch_loss: 2.7772, loss: 3.4700 ||:  21%|##        | 52/250 [00:19<01:16,  2.57it/s]\n",
            "batch_loss: 3.0878, loss: 3.4628 ||:  21%|##1       | 53/250 [00:19<01:18,  2.50it/s]\n",
            "batch_loss: 2.7187, loss: 3.4490 ||:  22%|##1       | 54/250 [00:20<01:20,  2.45it/s]\n",
            "batch_loss: 2.7946, loss: 3.4371 ||:  22%|##2       | 55/250 [00:20<01:20,  2.41it/s]\n",
            "batch_loss: 2.5423, loss: 3.4212 ||:  22%|##2       | 56/250 [00:21<01:18,  2.48it/s]\n",
            "batch_loss: 2.8881, loss: 3.4118 ||:  23%|##2       | 57/250 [00:21<01:16,  2.54it/s]\n",
            "batch_loss: 2.2103, loss: 3.3911 ||:  23%|##3       | 58/250 [00:22<01:20,  2.38it/s]\n",
            "batch_loss: 2.6339, loss: 3.3783 ||:  24%|##3       | 59/250 [00:22<01:19,  2.40it/s]\n",
            "batch_loss: 2.5931, loss: 3.3652 ||:  24%|##4       | 60/250 [00:22<01:19,  2.39it/s]\n",
            "batch_loss: 3.0530, loss: 3.3601 ||:  24%|##4       | 61/250 [00:23<01:22,  2.28it/s]\n",
            "batch_loss: 4.5591, loss: 3.3794 ||:  25%|##4       | 62/250 [00:23<01:20,  2.34it/s]\n",
            "batch_loss: 3.1981, loss: 3.3765 ||:  25%|##5       | 63/250 [00:24<01:15,  2.48it/s]\n",
            "batch_loss: 2.4161, loss: 3.3615 ||:  26%|##5       | 64/250 [00:24<01:13,  2.55it/s]\n",
            "batch_loss: 2.2161, loss: 3.3439 ||:  26%|##6       | 65/250 [00:24<01:12,  2.55it/s]\n",
            "batch_loss: 3.5557, loss: 3.3471 ||:  26%|##6       | 66/250 [00:25<01:12,  2.53it/s]\n",
            "batch_loss: 2.8065, loss: 3.3390 ||:  27%|##6       | 67/250 [00:25<01:09,  2.65it/s]\n",
            "batch_loss: 2.8394, loss: 3.3317 ||:  27%|##7       | 68/250 [00:25<01:10,  2.59it/s]\n",
            "batch_loss: 2.3445, loss: 3.3174 ||:  28%|##7       | 69/250 [00:26<01:10,  2.58it/s]\n",
            "batch_loss: 2.4049, loss: 3.3043 ||:  28%|##8       | 70/250 [00:26<01:05,  2.76it/s]\n",
            "batch_loss: 2.8357, loss: 3.2977 ||:  28%|##8       | 71/250 [00:27<01:05,  2.71it/s]\n",
            "batch_loss: 3.0229, loss: 3.2939 ||:  29%|##8       | 72/250 [00:27<01:08,  2.58it/s]\n",
            "batch_loss: 2.6799, loss: 3.2855 ||:  29%|##9       | 73/250 [00:27<01:06,  2.67it/s]\n",
            "batch_loss: 2.5019, loss: 3.2749 ||:  30%|##9       | 74/250 [00:28<01:05,  2.67it/s]\n",
            "batch_loss: 2.4065, loss: 3.2634 ||:  30%|###       | 75/250 [00:28<01:06,  2.64it/s]\n",
            "batch_loss: 2.7426, loss: 3.2565 ||:  30%|###       | 76/250 [00:28<01:05,  2.66it/s]\n",
            "batch_loss: 2.5557, loss: 3.2474 ||:  31%|###       | 77/250 [00:29<01:05,  2.63it/s]\n",
            "batch_loss: 2.4956, loss: 3.2378 ||:  31%|###1      | 78/250 [00:29<01:04,  2.67it/s]\n",
            "batch_loss: 3.3551, loss: 3.2392 ||:  32%|###1      | 79/250 [00:30<01:04,  2.67it/s]\n",
            "batch_loss: 2.8224, loss: 3.2340 ||:  32%|###2      | 80/250 [00:30<01:04,  2.62it/s]\n",
            "batch_loss: 3.0913, loss: 3.2323 ||:  32%|###2      | 81/250 [00:30<01:03,  2.67it/s]\n",
            "batch_loss: 3.2855, loss: 3.2329 ||:  33%|###2      | 82/250 [00:31<01:02,  2.68it/s]\n",
            "batch_loss: 2.9140, loss: 3.2291 ||:  33%|###3      | 83/250 [00:31<01:02,  2.68it/s]\n",
            "batch_loss: 2.8306, loss: 3.2243 ||:  34%|###3      | 84/250 [00:31<00:59,  2.80it/s]\n",
            "batch_loss: 2.7351, loss: 3.2186 ||:  34%|###4      | 85/250 [00:32<00:56,  2.91it/s]\n",
            "batch_loss: 2.9362, loss: 3.2153 ||:  34%|###4      | 86/250 [00:32<00:59,  2.77it/s]\n",
            "batch_loss: 2.8892, loss: 3.2115 ||:  35%|###4      | 87/250 [00:32<00:59,  2.75it/s]\n",
            "batch_loss: 1.9273, loss: 3.1970 ||:  35%|###5      | 88/250 [00:33<00:58,  2.77it/s]\n",
            "batch_loss: 2.8372, loss: 3.1929 ||:  36%|###5      | 89/250 [00:33<00:56,  2.83it/s]\n",
            "batch_loss: 3.0182, loss: 3.1910 ||:  36%|###6      | 90/250 [00:34<00:56,  2.85it/s]\n",
            "batch_loss: 2.6867, loss: 3.1854 ||:  36%|###6      | 91/250 [00:34<00:59,  2.68it/s]\n",
            "batch_loss: 2.2798, loss: 3.1756 ||:  37%|###6      | 92/250 [00:34<00:56,  2.82it/s]\n",
            "batch_loss: 2.5218, loss: 3.1686 ||:  37%|###7      | 93/250 [00:35<00:57,  2.72it/s]\n",
            "batch_loss: 2.4443, loss: 3.1609 ||:  38%|###7      | 94/250 [00:35<00:58,  2.66it/s]\n",
            "batch_loss: 2.2741, loss: 3.1515 ||:  38%|###8      | 95/250 [00:35<01:00,  2.55it/s]\n",
            "batch_loss: 2.1665, loss: 3.1413 ||:  38%|###8      | 96/250 [00:36<00:54,  2.82it/s]\n",
            "batch_loss: 2.6526, loss: 3.1362 ||:  39%|###8      | 97/250 [00:36<00:53,  2.86it/s]\n",
            "batch_loss: 2.6280, loss: 3.1310 ||:  39%|###9      | 98/250 [00:37<00:57,  2.66it/s]\n",
            "batch_loss: 2.6518, loss: 3.1262 ||:  40%|###9      | 99/250 [00:37<00:57,  2.62it/s]\n",
            "batch_loss: 2.8023, loss: 3.1230 ||:  40%|####      | 100/250 [00:37<01:04,  2.33it/s]\n",
            "batch_loss: 2.0278, loss: 3.1121 ||:  40%|####      | 101/250 [00:38<01:01,  2.42it/s]\n",
            "batch_loss: 2.6237, loss: 3.1073 ||:  41%|####      | 102/250 [00:38<01:01,  2.39it/s]\n",
            "batch_loss: 3.0123, loss: 3.1064 ||:  41%|####1     | 103/250 [00:39<01:01,  2.37it/s]\n",
            "batch_loss: 3.6446, loss: 3.1116 ||:  42%|####1     | 104/250 [00:39<00:58,  2.48it/s]\n",
            "batch_loss: 2.6403, loss: 3.1071 ||:  42%|####2     | 105/250 [00:40<01:00,  2.39it/s]\n",
            "batch_loss: 2.3504, loss: 3.0999 ||:  42%|####2     | 106/250 [00:40<01:01,  2.35it/s]\n",
            "batch_loss: 2.1020, loss: 3.0906 ||:  43%|####2     | 107/250 [00:40<00:59,  2.40it/s]\n",
            "batch_loss: 2.9193, loss: 3.0890 ||:  43%|####3     | 108/250 [00:41<00:56,  2.52it/s]\n",
            "batch_loss: 2.4844, loss: 3.0835 ||:  44%|####3     | 109/250 [00:41<00:56,  2.50it/s]\n",
            "batch_loss: 2.5399, loss: 3.0785 ||:  44%|####4     | 110/250 [00:41<00:50,  2.75it/s]\n",
            "batch_loss: 2.4569, loss: 3.0729 ||:  44%|####4     | 111/250 [00:42<00:47,  2.93it/s]\n",
            "batch_loss: 2.8814, loss: 3.0712 ||:  45%|####4     | 112/250 [00:42<00:49,  2.79it/s]\n",
            "batch_loss: 2.2265, loss: 3.0638 ||:  45%|####5     | 113/250 [00:43<00:52,  2.60it/s]\n",
            "batch_loss: 1.9618, loss: 3.0541 ||:  46%|####5     | 114/250 [00:43<00:51,  2.65it/s]\n",
            "batch_loss: 2.7183, loss: 3.0512 ||:  46%|####6     | 115/250 [00:43<00:50,  2.66it/s]\n",
            "batch_loss: 3.0364, loss: 3.0510 ||:  46%|####6     | 116/250 [00:44<00:46,  2.89it/s]\n",
            "batch_loss: 2.8470, loss: 3.0493 ||:  47%|####6     | 117/250 [00:44<00:48,  2.74it/s]\n",
            "batch_loss: 3.0697, loss: 3.0495 ||:  47%|####7     | 118/250 [00:44<00:48,  2.75it/s]\n",
            "batch_loss: 2.8118, loss: 3.0475 ||:  48%|####7     | 119/250 [00:45<00:49,  2.65it/s]\n",
            "batch_loss: 2.5123, loss: 3.0430 ||:  48%|####8     | 120/250 [00:45<00:50,  2.57it/s]\n",
            "batch_loss: 3.2584, loss: 3.0448 ||:  48%|####8     | 121/250 [00:45<00:47,  2.72it/s]\n",
            "batch_loss: 2.3278, loss: 3.0389 ||:  49%|####8     | 122/250 [00:46<00:45,  2.80it/s]\n",
            "batch_loss: 2.2463, loss: 3.0325 ||:  49%|####9     | 123/250 [00:46<00:45,  2.78it/s]\n",
            "batch_loss: 2.1844, loss: 3.0256 ||:  50%|####9     | 124/250 [00:47<00:47,  2.66it/s]\n",
            "batch_loss: 2.1129, loss: 3.0183 ||:  50%|#####     | 125/250 [00:47<00:47,  2.63it/s]\n",
            "batch_loss: 2.1178, loss: 3.0112 ||:  50%|#####     | 126/250 [00:47<00:46,  2.65it/s]\n",
            "batch_loss: 2.4757, loss: 3.0070 ||:  51%|#####     | 127/250 [00:48<00:47,  2.60it/s]\n",
            "batch_loss: 1.9637, loss: 2.9988 ||:  51%|#####1    | 128/250 [00:48<00:47,  2.58it/s]\n",
            "batch_loss: 2.7206, loss: 2.9967 ||:  52%|#####1    | 129/250 [00:49<00:49,  2.43it/s]\n",
            "batch_loss: 2.6613, loss: 2.9941 ||:  52%|#####2    | 130/250 [00:49<00:45,  2.61it/s]\n",
            "batch_loss: 2.6138, loss: 2.9912 ||:  52%|#####2    | 131/250 [00:49<00:44,  2.65it/s]\n",
            "batch_loss: 2.5148, loss: 2.9876 ||:  53%|#####2    | 132/250 [00:50<00:44,  2.62it/s]\n",
            "batch_loss: 2.1451, loss: 2.9812 ||:  53%|#####3    | 133/250 [00:50<00:45,  2.59it/s]\n",
            "batch_loss: 2.3176, loss: 2.9763 ||:  54%|#####3    | 134/250 [00:50<00:46,  2.50it/s]\n",
            "batch_loss: 2.2556, loss: 2.9709 ||:  54%|#####4    | 135/250 [00:51<00:43,  2.63it/s]\n",
            "batch_loss: 2.3117, loss: 2.9661 ||:  54%|#####4    | 136/250 [00:51<00:43,  2.64it/s]\n",
            "batch_loss: 2.7996, loss: 2.9649 ||:  55%|#####4    | 137/250 [00:52<00:42,  2.67it/s]\n",
            "batch_loss: 2.3122, loss: 2.9602 ||:  55%|#####5    | 138/250 [00:52<00:41,  2.67it/s]\n",
            "batch_loss: 2.2891, loss: 2.9553 ||:  56%|#####5    | 139/250 [00:52<00:40,  2.74it/s]\n",
            "batch_loss: 2.1109, loss: 2.9493 ||:  56%|#####6    | 140/250 [00:53<00:40,  2.70it/s]\n",
            "batch_loss: 2.3595, loss: 2.9451 ||:  56%|#####6    | 141/250 [00:53<00:40,  2.67it/s]\n",
            "batch_loss: 2.1317, loss: 2.9394 ||:  57%|#####6    | 142/250 [00:53<00:40,  2.69it/s]\n",
            "batch_loss: 2.2349, loss: 2.9345 ||:  57%|#####7    | 143/250 [00:54<00:40,  2.64it/s]\n",
            "batch_loss: 2.3093, loss: 2.9301 ||:  58%|#####7    | 144/250 [00:54<00:40,  2.62it/s]\n",
            "batch_loss: 2.1368, loss: 2.9246 ||:  58%|#####8    | 145/250 [00:55<00:42,  2.48it/s]\n",
            "batch_loss: 2.8592, loss: 2.9242 ||:  58%|#####8    | 146/250 [00:55<00:41,  2.53it/s]\n",
            "batch_loss: 2.7745, loss: 2.9232 ||:  59%|#####8    | 147/250 [00:55<00:38,  2.65it/s]\n",
            "batch_loss: 2.7162, loss: 2.9218 ||:  59%|#####9    | 148/250 [00:56<00:39,  2.59it/s]\n",
            "batch_loss: 2.4277, loss: 2.9185 ||:  60%|#####9    | 149/250 [00:56<00:40,  2.50it/s]\n",
            "batch_loss: 2.8554, loss: 2.9180 ||:  60%|######    | 150/250 [00:57<00:40,  2.47it/s]\n",
            "batch_loss: 2.3479, loss: 2.9143 ||:  60%|######    | 151/250 [00:57<00:38,  2.55it/s]\n",
            "batch_loss: 1.8928, loss: 2.9076 ||:  61%|######    | 152/250 [00:57<00:37,  2.60it/s]\n",
            "batch_loss: 2.4950, loss: 2.9049 ||:  61%|######1   | 153/250 [00:58<00:35,  2.76it/s]\n",
            "batch_loss: 3.2357, loss: 2.9070 ||:  62%|######1   | 154/250 [00:58<00:37,  2.59it/s]\n",
            "batch_loss: 2.3405, loss: 2.9033 ||:  62%|######2   | 155/250 [00:58<00:34,  2.74it/s]\n",
            "batch_loss: 2.0165, loss: 2.8977 ||:  62%|######2   | 156/250 [00:59<00:33,  2.81it/s]\n",
            "batch_loss: 3.1943, loss: 2.8996 ||:  63%|######2   | 157/250 [00:59<00:33,  2.76it/s]\n",
            "batch_loss: 2.5525, loss: 2.8974 ||:  63%|######3   | 158/250 [00:59<00:33,  2.71it/s]\n",
            "batch_loss: 2.9932, loss: 2.8980 ||:  64%|######3   | 159/250 [01:00<00:33,  2.70it/s]\n",
            "batch_loss: 3.1319, loss: 2.8994 ||:  64%|######4   | 160/250 [01:00<00:34,  2.64it/s]\n",
            "batch_loss: 2.2615, loss: 2.8955 ||:  64%|######4   | 161/250 [01:01<00:34,  2.55it/s]\n",
            "batch_loss: 2.0777, loss: 2.8904 ||:  65%|######4   | 162/250 [01:01<00:33,  2.66it/s]\n",
            "batch_loss: 2.9690, loss: 2.8909 ||:  65%|######5   | 163/250 [01:01<00:33,  2.60it/s]\n",
            "batch_loss: 2.3575, loss: 2.8876 ||:  66%|######5   | 164/250 [01:02<00:33,  2.54it/s]\n",
            "batch_loss: 2.3373, loss: 2.8843 ||:  66%|######6   | 165/250 [01:02<00:31,  2.66it/s]\n",
            "batch_loss: 3.2989, loss: 2.8868 ||:  66%|######6   | 166/250 [01:03<00:32,  2.59it/s]\n",
            "batch_loss: 3.7274, loss: 2.8918 ||:  67%|######6   | 167/250 [01:03<00:30,  2.71it/s]\n",
            "batch_loss: 2.6902, loss: 2.8906 ||:  67%|######7   | 168/250 [01:03<00:31,  2.58it/s]\n",
            "batch_loss: 1.9369, loss: 2.8850 ||:  68%|######7   | 169/250 [01:04<00:29,  2.72it/s]\n",
            "batch_loss: 1.6196, loss: 2.8775 ||:  68%|######8   | 170/250 [01:04<00:28,  2.84it/s]\n",
            "batch_loss: 2.4778, loss: 2.8752 ||:  68%|######8   | 171/250 [01:04<00:28,  2.73it/s]\n",
            "batch_loss: 2.4815, loss: 2.8729 ||:  69%|######8   | 172/250 [01:05<00:28,  2.78it/s]\n",
            "batch_loss: 2.5926, loss: 2.8713 ||:  69%|######9   | 173/250 [01:05<00:28,  2.70it/s]\n",
            "batch_loss: 2.2886, loss: 2.8680 ||:  70%|######9   | 174/250 [01:05<00:27,  2.74it/s]\n",
            "batch_loss: 2.7557, loss: 2.8673 ||:  70%|#######   | 175/250 [01:06<00:26,  2.79it/s]\n",
            "batch_loss: 2.5580, loss: 2.8656 ||:  70%|#######   | 176/250 [01:06<00:26,  2.76it/s]\n",
            "batch_loss: 2.5943, loss: 2.8640 ||:  71%|#######   | 177/250 [01:07<00:26,  2.75it/s]\n",
            "batch_loss: 2.4388, loss: 2.8616 ||:  71%|#######1  | 178/250 [01:07<00:26,  2.74it/s]\n",
            "batch_loss: 2.3538, loss: 2.8588 ||:  72%|#######1  | 179/250 [01:07<00:26,  2.66it/s]\n",
            "batch_loss: 2.1263, loss: 2.8547 ||:  72%|#######2  | 180/250 [01:08<00:26,  2.63it/s]\n",
            "batch_loss: 2.4690, loss: 2.8526 ||:  72%|#######2  | 181/250 [01:08<00:24,  2.77it/s]\n",
            "batch_loss: 2.6514, loss: 2.8515 ||:  73%|#######2  | 182/250 [01:08<00:24,  2.82it/s]\n",
            "batch_loss: 2.1713, loss: 2.8478 ||:  73%|#######3  | 183/250 [01:09<00:24,  2.76it/s]\n",
            "batch_loss: 2.5709, loss: 2.8463 ||:  74%|#######3  | 184/250 [01:09<00:25,  2.59it/s]\n",
            "batch_loss: 2.7499, loss: 2.8457 ||:  74%|#######4  | 185/250 [01:10<00:25,  2.54it/s]\n",
            "batch_loss: 2.2478, loss: 2.8425 ||:  74%|#######4  | 186/250 [01:10<00:25,  2.50it/s]\n",
            "batch_loss: 2.1808, loss: 2.8390 ||:  75%|#######4  | 187/250 [01:10<00:24,  2.62it/s]\n",
            "batch_loss: 2.4306, loss: 2.8368 ||:  75%|#######5  | 188/250 [01:11<00:23,  2.66it/s]\n",
            "batch_loss: 2.4109, loss: 2.8346 ||:  76%|#######5  | 189/250 [01:11<00:22,  2.72it/s]\n",
            "batch_loss: 2.6368, loss: 2.8335 ||:  76%|#######6  | 190/250 [01:11<00:22,  2.64it/s]\n",
            "batch_loss: 2.0209, loss: 2.8293 ||:  76%|#######6  | 191/250 [01:12<00:22,  2.64it/s]\n",
            "batch_loss: 1.9077, loss: 2.8245 ||:  77%|#######6  | 192/250 [01:12<00:22,  2.59it/s]\n",
            "batch_loss: 2.4700, loss: 2.8226 ||:  77%|#######7  | 193/250 [01:13<00:21,  2.62it/s]\n",
            "batch_loss: 2.5594, loss: 2.8213 ||:  78%|#######7  | 194/250 [01:13<00:20,  2.78it/s]\n",
            "batch_loss: 2.3833, loss: 2.8190 ||:  78%|#######8  | 195/250 [01:13<00:20,  2.69it/s]\n",
            "batch_loss: 2.6606, loss: 2.8182 ||:  78%|#######8  | 196/250 [01:14<00:19,  2.71it/s]\n",
            "batch_loss: 2.6448, loss: 2.8173 ||:  79%|#######8  | 197/250 [01:14<00:19,  2.73it/s]\n",
            "batch_loss: 2.5431, loss: 2.8160 ||:  79%|#######9  | 198/250 [01:14<00:19,  2.68it/s]\n",
            "batch_loss: 2.8662, loss: 2.8162 ||:  80%|#######9  | 199/250 [01:15<00:20,  2.46it/s]\n",
            "batch_loss: 1.8996, loss: 2.8116 ||:  80%|########  | 200/250 [01:15<00:22,  2.22it/s]\n",
            "batch_loss: 2.2677, loss: 2.8089 ||:  80%|########  | 201/250 [01:16<00:20,  2.39it/s]\n",
            "batch_loss: 2.2704, loss: 2.8063 ||:  81%|########  | 202/250 [01:16<00:19,  2.46it/s]\n",
            "batch_loss: 2.6597, loss: 2.8055 ||:  81%|########1 | 203/250 [01:17<00:19,  2.44it/s]\n",
            "batch_loss: 2.6951, loss: 2.8050 ||:  82%|########1 | 204/250 [01:17<00:18,  2.46it/s]\n",
            "batch_loss: 2.2507, loss: 2.8023 ||:  82%|########2 | 205/250 [01:17<00:17,  2.50it/s]\n",
            "batch_loss: 2.3099, loss: 2.7999 ||:  82%|########2 | 206/250 [01:18<00:17,  2.53it/s]\n",
            "batch_loss: 2.3987, loss: 2.7980 ||:  83%|########2 | 207/250 [01:18<00:16,  2.63it/s]\n",
            "batch_loss: 2.3782, loss: 2.7959 ||:  83%|########3 | 208/250 [01:19<00:16,  2.51it/s]\n",
            "batch_loss: 2.8657, loss: 2.7963 ||:  84%|########3 | 209/250 [01:19<00:16,  2.54it/s]\n",
            "batch_loss: 2.4217, loss: 2.7945 ||:  84%|########4 | 210/250 [01:19<00:14,  2.68it/s]\n",
            "batch_loss: 2.5440, loss: 2.7933 ||:  84%|########4 | 211/250 [01:20<00:14,  2.64it/s]\n",
            "batch_loss: 2.6319, loss: 2.7925 ||:  85%|########4 | 212/250 [01:20<00:15,  2.51it/s]\n",
            "batch_loss: 2.2875, loss: 2.7902 ||:  85%|########5 | 213/250 [01:21<00:14,  2.47it/s]\n",
            "batch_loss: 2.1585, loss: 2.7872 ||:  86%|########5 | 214/250 [01:21<00:13,  2.63it/s]\n",
            "batch_loss: 3.1269, loss: 2.7888 ||:  86%|########6 | 215/250 [01:21<00:13,  2.67it/s]\n",
            "batch_loss: 1.8790, loss: 2.7846 ||:  86%|########6 | 216/250 [01:22<00:12,  2.71it/s]\n",
            "batch_loss: 1.9929, loss: 2.7809 ||:  87%|########6 | 217/250 [01:22<00:12,  2.73it/s]\n",
            "batch_loss: 3.1499, loss: 2.7826 ||:  87%|########7 | 218/250 [01:22<00:11,  2.84it/s]\n",
            "batch_loss: 2.0499, loss: 2.7793 ||:  88%|########7 | 219/250 [01:23<00:11,  2.81it/s]\n",
            "batch_loss: 1.9539, loss: 2.7755 ||:  88%|########8 | 220/250 [01:23<00:10,  2.79it/s]\n",
            "batch_loss: 1.9607, loss: 2.7719 ||:  88%|########8 | 221/250 [01:23<00:11,  2.60it/s]\n",
            "batch_loss: 1.9254, loss: 2.7680 ||:  89%|########8 | 222/250 [01:24<00:11,  2.51it/s]\n",
            "batch_loss: 2.1208, loss: 2.7651 ||:  89%|########9 | 223/250 [01:24<00:10,  2.60it/s]\n",
            "batch_loss: 2.4821, loss: 2.7639 ||:  90%|########9 | 224/250 [01:25<00:09,  2.65it/s]\n",
            "batch_loss: 2.3522, loss: 2.7620 ||:  90%|######### | 225/250 [01:25<00:09,  2.53it/s]\n",
            "batch_loss: 2.3754, loss: 2.7603 ||:  90%|######### | 226/250 [01:25<00:09,  2.51it/s]\n",
            "batch_loss: 2.0856, loss: 2.7574 ||:  91%|######### | 227/250 [01:26<00:09,  2.50it/s]\n",
            "batch_loss: 2.7480, loss: 2.7573 ||:  91%|#########1| 228/250 [01:26<00:08,  2.52it/s]\n",
            "batch_loss: 2.6761, loss: 2.7570 ||:  92%|#########1| 229/250 [01:27<00:08,  2.53it/s]\n",
            "batch_loss: 2.1571, loss: 2.7544 ||:  92%|#########2| 230/250 [01:27<00:07,  2.54it/s]\n",
            "batch_loss: 2.6355, loss: 2.7538 ||:  92%|#########2| 231/250 [01:27<00:07,  2.52it/s]\n",
            "batch_loss: 2.6623, loss: 2.7534 ||:  93%|#########2| 232/250 [01:28<00:07,  2.57it/s]\n",
            "batch_loss: 2.2191, loss: 2.7512 ||:  93%|#########3| 233/250 [01:28<00:06,  2.59it/s]\n",
            "batch_loss: 1.7841, loss: 2.7470 ||:  94%|#########3| 234/250 [01:29<00:06,  2.48it/s]\n",
            "batch_loss: 2.4146, loss: 2.7456 ||:  94%|#########3| 235/250 [01:29<00:06,  2.48it/s]\n",
            "batch_loss: 3.2565, loss: 2.7478 ||:  94%|#########4| 236/250 [01:29<00:05,  2.51it/s]\n",
            "batch_loss: 2.6262, loss: 2.7473 ||:  95%|#########4| 237/250 [01:30<00:05,  2.49it/s]\n",
            "batch_loss: 2.1392, loss: 2.7447 ||:  95%|#########5| 238/250 [01:30<00:04,  2.67it/s]\n",
            "batch_loss: 2.3392, loss: 2.7430 ||:  96%|#########5| 239/250 [01:31<00:04,  2.56it/s]\n",
            "batch_loss: 2.6888, loss: 2.7428 ||:  96%|#########6| 240/250 [01:31<00:03,  2.50it/s]\n",
            "batch_loss: 2.9756, loss: 2.7437 ||:  96%|#########6| 241/250 [01:31<00:03,  2.51it/s]\n",
            "batch_loss: 2.3297, loss: 2.7420 ||:  97%|#########6| 242/250 [01:32<00:03,  2.44it/s]\n",
            "batch_loss: 3.1248, loss: 2.7436 ||:  97%|#########7| 243/250 [01:32<00:02,  2.44it/s]\n",
            "batch_loss: 1.9559, loss: 2.7404 ||:  98%|#########7| 244/250 [01:33<00:02,  2.56it/s]\n",
            "batch_loss: 2.2822, loss: 2.7385 ||:  98%|#########8| 245/250 [01:33<00:01,  2.59it/s]\n",
            "batch_loss: 2.6034, loss: 2.7380 ||:  98%|#########8| 246/250 [01:33<00:01,  2.52it/s]\n",
            "batch_loss: 2.0549, loss: 2.7352 ||:  99%|#########8| 247/250 [01:34<00:01,  2.52it/s]\n",
            "batch_loss: 2.2590, loss: 2.7333 ||:  99%|#########9| 248/250 [01:34<00:00,  2.43it/s]\n",
            "batch_loss: 2.1186, loss: 2.7308 ||: 100%|#########9| 249/250 [01:35<00:00,  2.38it/s]\n",
            "batch_loss: 2.7465, loss: 2.7309 ||: 100%|##########| 250/250 [01:35<00:00,  2.52it/s]\n",
            "batch_loss: 2.7465, loss: 2.7309 ||: 100%|##########| 250/250 [01:35<00:00,  2.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-09-21 13:15:02,156 - INFO - allennlp.models.archival - archiving weights and vocabulary to E:/saved_models/declutr/wiki/output\\model.tar.gz\n"
          ]
        }
      ],
      "source": [
        "!allennlp train \"../training_config/declutr_small_v2.jsonnet\" \\\n",
        "    --serialization-dir \"E:/saved_models/declutr/wiki/output\" \\\n",
        "    --overrides \"$overrides\" \\\n",
        "    --include-package \"declutr\" \\\n",
        "    -f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsbr6OMv16GQ"
      },
      "source": [
        "### ü§ó Exporting a trained model to HuggingFace Transformers\n",
        "\n",
        "We have provided a simple script to export a trained model so that it can be loaded with [Hugging Face Transformers](https://github.com/huggingface/transformers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KqmWVD0y16GQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ ü§ó Transformers compatible model saved to: E:\\saved_models\\declutr\\wiki\\output\\transformers_format. See https://huggingface.co/transformers/model_sharing.html for instructions on hosting the model with ü§ó Transformers.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "archive_file = \"E:/saved_models/declutr/wiki/output/\"\n",
        "save_directory = \"E:/saved_models/declutr/wiki/output/transformers_format/\"\n",
        "\n",
        "!python ../scripts/save_pretrained_hf.py --archive_file $archive_file --save_directory $save_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python ../scripts/save_pretrained_hf.py --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0-NTFaH16GQ"
      },
      "source": [
        "The model, saved to `--save-directory`, can then be loaded using the Hugging Face Transformers library\n",
        "\n",
        "> See the [embedding notebook](https://colab.research.google.com/github/JohnGiorgi/DeCLUTR/blob/master/notebooks/embedding.ipynb) for more details on using trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pAl1zIya16GQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
        "  \n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"{save_directory}\")\n",
        "model = AutoModel.from_pretrained(f\"{save_directory}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzQ0G4rp16GQ"
      },
      "source": [
        "> If you would like to upload your model to the Hugging Face model repository, follow the instructions [here](https://huggingface.co/transformers/model_sharing.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD5dZo18EE-S"
      },
      "source": [
        "## ‚ôªÔ∏è Conclusion\n",
        "\n",
        "That's it! In this notebook, we covered how to collect data for training the model, and specifically how _long_ that text needs to be. We then briefly covered configuring and running a training session. Please see [our paper](https://arxiv.org/abs/2006.03659) and [repo](https://github.com/JohnGiorgi/DeCLUTR) for more details, and don't hesitate to open an issue if you have any trouble!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "training.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('37_declutr')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "80c13ac005a4e8467463143a62ebf86e7f8ec07ba508cce481d79436c5ff6a9b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
