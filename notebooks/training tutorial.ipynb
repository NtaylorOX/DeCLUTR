{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8jt6ML03DS5"
      },
      "source": [
        "# Training your own model\n",
        "\n",
        "This notebook will walk you through training your own model using [DeCLUTR](https://github.com/JohnGiorgi/DeCLUTR)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU3Iod2-g0-o"
      },
      "source": [
        "## üîß Install the prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sr4r5pN40Kli"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/JohnGiorgi/DeCLUTR.git\n",
        "\n",
        "# go to main dir i.e. DeCLUTR on local and run \"pip install --editable .\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zog7ApwuUD7_"
      },
      "source": [
        "## üìñ Preparing a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwnLpUmN4Art"
      },
      "source": [
        "\n",
        "A dataset is simply a file containing one item of text (a document, a scientific paper, etc.) per line. For demonstration purposes, we have provided a script that will download the [WikiText-103](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/) dataset and format it for training with our method.\n",
        "\n",
        "The only \"gotcha\" is that each piece of text needs to be long enough so that we can sample spans from it. In general, you should collect documents of a minimum length according to the following:\n",
        "\n",
        "```python\n",
        "min_length = num_anchors * max_span_len * 2\n",
        "```\n",
        "\n",
        "In our paper, we set `num_anchors=2` and `max_span_len=512`, so we require documents of `min_length=2048`. We simply need to provide this value as an argument when running the script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q0fwnwq23aAZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "train_data_path = \"/mnt/sdg/niallt/wiki_text/wikitext-103/train.txt\"\n",
        "\n",
        "min_length = 2048\n",
        "# run this first timeP\n",
        "\n",
        "\n",
        "# !python ../scripts/preprocess_wikitext_103.py $train_data_path --min-length $min_length --max-instances 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUEFeupP6qy-"
      },
      "source": [
        "Lets confirm that our dataset looks as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K7ffGXCn7Cpq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17824 /mnt/sdg/niallt/wiki_text/wikitext-103/train.txt\n"
          ]
        }
      ],
      "source": [
        "!wc -l $train_data_path  # This should be approximately 17.8K lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "10DprWZc9iV6"
      },
      "outputs": [],
      "source": [
        "# !head -n 1 $train_data_path  # This should be a single Wikipedia entry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Look at sampling technique\n",
        "\n",
        "This will help get an idea of what "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from declutr.common.contrastive_utils import sample_anchor_positive_pairs\n",
        "from declutr.losses import NTXentLoss\n",
        "import torch\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"this is just an example sentence to test out some sampling and loss calculation from DeCLUTR. We want to see exactly how it works in order to implement it for our own use case\"\n",
        "len_text = len(text.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# just go with one anchor for now\n",
        "\n",
        "num_anchors = 1\n",
        "max_span_len = int((len_text/2)/num_anchors) \n",
        "min_span_len = 5\n",
        "num_positives = 5\n",
        "sampling_strat = \"adjacent\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "anchor_spans, positive_spans = sample_anchor_positive_pairs(\n",
        "    text = text,\n",
        "    num_anchors = num_anchors,\n",
        "    num_positives = num_positives,\n",
        "    max_span_len = max_span_len,\n",
        "    min_span_len = min_span_len,\n",
        "    sampling_strategy = sampling_strat\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['is just an example sentence to test out some sampling and loss']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "anchor_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['loss calculation from DeCLUTR. We want',\n",
              " 'and loss calculation from DeCLUTR. We want',\n",
              " 'sampling and loss calculation from DeCLUTR. We want',\n",
              " 'sampling and loss calculation from DeCLUTR. We want',\n",
              " 'some sampling and loss calculation from DeCLUTR. We want']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "positive_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKYdambZ59nM"
      },
      "source": [
        "## üèÉ Training the model\n",
        "\n",
        "Once you have collected the dataset, you can easily initiate a training session with the `allennlp train` command. An experiment is configured using a [Jsonnet](https://jsonnet.org/) config file. Lets take a look at the config for the DeCLUTR-small model presented in [our paper](https://arxiv.org/abs/2006.03659):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open(\"../training_config/declutr_small.jsonnet\", \"r\") as f:\n",
        "#     print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f1HqWSscWOx"
      },
      "source": [
        "\n",
        "The only thing to configure is the path to the training set (`train_data_path`), which can be passed to `allennlp train` via the `--overrides` argument (but you can also provide it in your config file directly, if you prefer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YS9VuxESBcr3"
      },
      "outputs": [],
      "source": [
        "# overrides = (\n",
        "#     f\"{{'train_data_path': '{train_data_path}', \"\n",
        "#     # lower the batch size to be able to train on Colab GPUs\n",
        "#     \"'data_loader.batch_size': 2, \"\n",
        "#     # training examples / batch size. Not required, but gives us a more informative progress bar during training\n",
        "#     \"'data_loader.batches_per_epoch': None}\"\n",
        "# )\n",
        "\n",
        "\n",
        "overrides = (\n",
        "    f\"{{'train_data_path': '{train_data_path}', \"\n",
        "    # lower the batch size to be able to train on Colab GPUs\n",
        "    \"'data_loader.batch_size': 2,}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2v4tiiXgBC2M"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"{'train_data_path': '/mnt/sdg/niallt/wiki_text/wikitext-103/train.txt', 'data_loader.batch_size': 2,}\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "overrides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --serialization-dir \"/mnt/sdg/niallt/declutr/wiki/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Db_cNfZ76KRf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-21 10:52:02,188 - INFO - allennlp.common.params - evaluation = None\n",
            "2022-11-21 10:52:02,188 - INFO - allennlp.common.params - include_in_archive = None\n",
            "2022-11-21 10:52:02,188 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2022-11-21 10:52:02,188 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2022-11-21 10:52:02,188 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2022-11-21 10:52:02,190 - INFO - allennlp.common.checks - Pytorch version: 1.11.0+cu102\n",
            "2022-11-21 10:52:02,191 - INFO - allennlp.common.params - type = default\n",
            "2022-11-21 10:52:02,191 - INFO - allennlp.common.params - dataset_reader.type = declutr\n",
            "2022-11-21 10:52:02,191 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2022-11-21 10:52:02,191 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2022-11-21 10:52:02,191 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
            "2022-11-21 10:52:02,191 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n",
            "2022-11-21 10:52:02,192 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = distilroberta-base\n",
            "2022-11-21 10:52:02,192 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = True\n",
            "2022-11-21 10:52:02,192 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = 510\n",
            "2022-11-21 10:52:02,192 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n",
            "2022-11-21 10:52:02,192 - INFO - allennlp.common.params - dataset_reader.tokenizer.verification_tokens = None\n",
            "2022-11-21 10:52:09,443 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer\n",
            "2022-11-21 10:52:09,444 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2022-11-21 10:52:09,444 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = distilroberta-base\n",
            "2022-11-21 10:52:09,444 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2022-11-21 10:52:09,444 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = None\n",
            "2022-11-21 10:52:09,444 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2022-11-21 10:52:09,445 - INFO - allennlp.common.params - dataset_reader.num_anchors = 2\n",
            "2022-11-21 10:52:09,445 - INFO - allennlp.common.params - dataset_reader.num_positives = 2\n",
            "2022-11-21 10:52:09,445 - INFO - allennlp.common.params - dataset_reader.max_span_len = 512\n",
            "2022-11-21 10:52:09,445 - INFO - allennlp.common.params - dataset_reader.min_span_len = 32\n",
            "2022-11-21 10:52:09,445 - INFO - allennlp.common.params - dataset_reader.sampling_strategy = None\n",
            "2022-11-21 10:52:09,445 - INFO - allennlp.common.params - train_data_path = /mnt/sdg/niallt/wiki_text/wikitext-103/train.txt\n",
            "2022-11-21 10:52:09,446 - INFO - allennlp.common.params - datasets_for_vocab_creation = None\n",
            "2022-11-21 10:52:09,446 - INFO - allennlp.common.params - validation_dataset_reader = None\n",
            "2022-11-21 10:52:09,446 - INFO - allennlp.common.params - validation_data_path = None\n",
            "2022-11-21 10:52:09,446 - INFO - allennlp.common.params - validation_data_loader = None\n",
            "2022-11-21 10:52:09,446 - INFO - allennlp.common.params - test_data_path = None\n",
            "2022-11-21 10:52:09,446 - INFO - allennlp.common.params - evaluate_on_test = False\n",
            "2022-11-21 10:52:09,446 - INFO - allennlp.common.params - batch_weight_key = \n",
            "2022-11-21 10:52:09,447 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
            "2022-11-21 10:52:09,447 - INFO - allennlp.common.params - data_loader.batch_size = 2\n",
            "2022-11-21 10:52:09,447 - INFO - allennlp.common.params - data_loader.drop_last = True\n",
            "2022-11-21 10:52:09,447 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2022-11-21 10:52:09,447 - INFO - allennlp.common.params - data_loader.batch_sampler = None\n",
            "2022-11-21 10:52:09,447 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2022-11-21 10:52:09,447 - INFO - allennlp.common.params - data_loader.num_workers = 1\n",
            "2022-11-21 10:52:09,447 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
            "2022-11-21 10:52:09,447 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
            "2022-11-21 10:52:09,448 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
            "2022-11-21 10:52:09,448 - INFO - allennlp.common.params - data_loader.quiet = False\n",
            "2022-11-21 10:52:09,448 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x7fed2246cb80>\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "loading instances: 0it [00:00, ?it/s]2022-11-21 10:52:09,466 - INFO - declutr.dataset_reader - Reading instances from lines in file at: /mnt/sdg/niallt/wiki_text/wikitext-103/train.txt\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (4358 > 512). Running this sequence through the model will result in indexing errors\n",
            "loading instances: 500it [00:32, 15.51it/s]\n",
            "2022-11-21 10:52:41,730 - INFO - allennlp.common.params - vocabulary.type = empty\n",
            "2022-11-21 10:52:41,731 - INFO - allennlp.common.params - model.type = declutr\n",
            "2022-11-21 10:52:41,732 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2022-11-21 10:52:41,732 - INFO - allennlp.common.params - model.text_field_embedder.type = mlm\n",
            "2022-11-21 10:52:41,733 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mlm\n",
            "2022-11-21 10:52:41,733 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = distilroberta-base\n",
            "2022-11-21 10:52:41,733 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = None\n",
            "2022-11-21 10:52:41,734 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_module = None\n",
            "2022-11-21 10:52:41,734 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
            "2022-11-21 10:52:41,734 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True\n",
            "2022-11-21 10:52:41,734 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None\n",
            "2022-11-21 10:52:41,734 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n",
            "2022-11-21 10:52:41,734 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
            "2022-11-21 10:52:41,734 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None\n",
            "2022-11-21 10:52:41,735 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None\n",
            "2022-11-21 10:52:41,735 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.masked_language_modeling = True\n",
            "2022-11-21 10:52:44,063 - INFO - allennlp.common.params - model.seq2vec_encoder = None\n",
            "2022-11-21 10:52:44,063 - INFO - allennlp.common.params - model.feedforward = None\n",
            "2022-11-21 10:52:44,063 - INFO - allennlp.common.params - model.miner = None\n",
            "2022-11-21 10:52:44,064 - INFO - allennlp.common.params - model.loss.type = nt_xent\n",
            "2022-11-21 10:52:44,064 - INFO - allennlp.common.params - model.loss.temperature = 0.05\n",
            "2022-11-21 10:52:44,064 - INFO - allennlp.common.params - model.scale_fix = False\n",
            "2022-11-21 10:52:44,065 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x7fed1dd96850>\n",
            "2022-11-21 10:52:44,065 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2022-11-21 10:52:44,065 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2022-11-21 10:52:44,065 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.bias\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.bias\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.weight\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.bias\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.weight\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.bias\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.weight\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.position_embeddings.weight\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.token_type_embeddings.weight\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.word_embeddings.weight\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.bias\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.weight\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.bias\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.weight\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.bias\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.weight\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.bias\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.weight\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.bias\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.weight\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.bias\n",
            "2022-11-21 10:52:44,066 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.weight\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.bias\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.weight\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.bias\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.weight\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.bias\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.weight\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.bias\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.weight\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.bias\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.weight\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.bias\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.weight\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.bias\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.weight\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.bias\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.weight\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.bias\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.weight\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.bias\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.weight\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.bias\n",
            "2022-11-21 10:52:44,067 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.weight\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.bias\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.weight\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.bias\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.weight\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.bias\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.weight\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.bias\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.weight\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.bias\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.weight\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.bias\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.weight\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.bias\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.weight\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.bias\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.weight\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.bias\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.weight\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.bias\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.weight\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.bias\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.weight\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.bias\n",
            "2022-11-21 10:52:44,068 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.weight\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.bias\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.weight\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.bias\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.weight\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.bias\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.weight\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.bias\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.weight\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.bias\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.weight\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.bias\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.weight\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.bias\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.weight\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.bias\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.weight\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.bias\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.weight\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.bias\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.weight\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.bias\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.weight\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.bias\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.weight\n",
            "2022-11-21 10:52:44,069 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.bias\n",
            "2022-11-21 10:52:44,070 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.weight\n",
            "2022-11-21 10:53:09,417 - INFO - allennlp.common.params - trainer.type = gradient_descent\n",
            "2022-11-21 10:53:09,418 - INFO - allennlp.common.params - trainer.cuda_device = None\n",
            "2022-11-21 10:53:09,418 - INFO - allennlp.common.params - trainer.distributed = False\n",
            "2022-11-21 10:53:09,418 - INFO - allennlp.common.params - trainer.world_size = 1\n",
            "2022-11-21 10:53:09,418 - INFO - allennlp.common.params - trainer.patience = None\n",
            "2022-11-21 10:53:09,418 - INFO - allennlp.common.params - trainer.validation_metric = -loss\n",
            "2022-11-21 10:53:09,418 - INFO - allennlp.common.params - trainer.num_epochs = 1\n",
            "2022-11-21 10:53:09,418 - INFO - allennlp.common.params - trainer.grad_norm = 1\n",
            "2022-11-21 10:53:09,418 - INFO - allennlp.common.params - trainer.grad_clipping = None\n",
            "2022-11-21 10:53:09,418 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1\n",
            "2022-11-21 10:53:09,418 - INFO - allennlp.common.params - trainer.use_amp = True\n",
            "2022-11-21 10:53:09,418 - INFO - allennlp.common.params - trainer.no_grad = None\n",
            "2022-11-21 10:53:09,419 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2022-11-21 10:53:09,419 - INFO - allennlp.common.params - trainer.moving_average = None\n",
            "2022-11-21 10:53:09,419 - INFO - allennlp.common.params - trainer.callbacks = None\n",
            "2022-11-21 10:53:09,419 - INFO - allennlp.common.params - trainer.enable_default_callbacks = True\n",
            "2022-11-21 10:53:09,419 - INFO - allennlp.common.params - trainer.run_confidence_checks = True\n",
            "2022-11-21 10:53:09,419 - INFO - allennlp.common.params - trainer.grad_scaling = True\n",
            "2022-11-21 10:53:12,363 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw\n",
            "2022-11-21 10:53:12,365 - INFO - allennlp.common.params - trainer.optimizer.lr = 5e-05\n",
            "2022-11-21 10:53:12,365 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)\n",
            "2022-11-21 10:53:12,365 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06\n",
            "2022-11-21 10:53:12,365 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1\n",
            "2022-11-21 10:53:12,365 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = False\n",
            "2022-11-21 10:53:12,366 - INFO - allennlp.training.optimizers - Done constructing parameter groups.\n",
            "2022-11-21 10:53:12,366 - INFO - allennlp.training.optimizers - Group 0: ['_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.lm_head.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.bias'], {'weight_decay': 0}\n",
            "2022-11-21 10:53:12,366 - INFO - allennlp.training.optimizers - Group 1: ['_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.token_type_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.word_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.position_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.weight'], {}\n",
            "2022-11-21 10:53:12,366 - INFO - allennlp.training.optimizers - Number of trainable parameters: 82170201\n",
            "/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "2022-11-21 10:53:12,371 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):\n",
            "2022-11-21 10:53:12,371 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):\n",
            "2022-11-21 10:53:12,371 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.word_embeddings.weight\n",
            "2022-11-21 10:53:12,371 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.position_embeddings.weight\n",
            "2022-11-21 10:53:12,371 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.token_type_embeddings.weight\n",
            "2022-11-21 10:53:12,371 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.weight\n",
            "2022-11-21 10:53:12,371 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.bias\n",
            "2022-11-21 10:53:12,371 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.weight\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.bias\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.weight\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.bias\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.weight\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.bias\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.weight\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.bias\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.weight\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.bias\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.weight\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.bias\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.weight\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.bias\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.weight\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.bias\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.weight\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.bias\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.weight\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.bias\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.weight\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.bias\n",
            "2022-11-21 10:53:12,372 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.weight\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.bias\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.weight\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.bias\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.weight\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.bias\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.weight\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.bias\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.weight\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.bias\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.weight\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.bias\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.weight\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.bias\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.weight\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.bias\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.weight\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.bias\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.weight\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.bias\n",
            "2022-11-21 10:53:12,373 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.weight\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.bias\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.weight\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.bias\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.weight\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.bias\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.weight\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.bias\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.weight\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.bias\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.weight\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.bias\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.weight\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.bias\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.weight\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.bias\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.weight\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.bias\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.weight\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.bias\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.weight\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.bias\n",
            "2022-11-21 10:53:12,374 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.weight\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.bias\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.weight\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.bias\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.weight\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.bias\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.weight\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.bias\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.weight\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.bias\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.weight\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.bias\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.weight\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.bias\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.weight\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.bias\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.weight\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.bias\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.weight\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.bias\n",
            "2022-11-21 10:53:12,375 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.bias\n",
            "2022-11-21 10:53:12,376 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.weight\n",
            "2022-11-21 10:53:12,376 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.bias\n",
            "2022-11-21 10:53:12,376 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.weight\n",
            "2022-11-21 10:53:12,376 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.bias\n",
            "2022-11-21 10:53:12,376 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular\n",
            "2022-11-21 10:53:12,376 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.1\n",
            "2022-11-21 10:53:12,376 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32\n",
            "2022-11-21 10:53:12,376 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1\n",
            "2022-11-21 10:53:12,376 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False\n",
            "2022-11-21 10:53:12,376 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False\n",
            "2022-11-21 10:53:12,376 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38\n",
            "2022-11-21 10:53:12,376 - INFO - allennlp.common.params - trainer.checkpointer.type = default\n",
            "2022-11-21 10:53:12,377 - INFO - allennlp.common.params - trainer.checkpointer.save_completed_epochs = True\n",
            "2022-11-21 10:53:12,377 - INFO - allennlp.common.params - trainer.checkpointer.save_every_num_seconds = None\n",
            "2022-11-21 10:53:12,377 - INFO - allennlp.common.params - trainer.checkpointer.save_every_num_batches = None\n",
            "2022-11-21 10:53:12,377 - INFO - allennlp.common.params - trainer.checkpointer.keep_most_recent_by_count = 1\n",
            "2022-11-21 10:53:12,377 - INFO - allennlp.common.params - trainer.checkpointer.keep_most_recent_by_age = None\n",
            "2022-11-21 10:53:12,378 - INFO - allennlp.training.gradient_descent_trainer - Beginning training.\n",
            "2022-11-21 10:53:12,378 - INFO - allennlp.training.gradient_descent_trainer - Epoch 0/0\n",
            "2022-11-21 10:53:12,379 - INFO - allennlp.training.gradient_descent_trainer - Worker 0 memory usage: 7.0G\n",
            "/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/torch/cuda/memory.py:271: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
            "  warnings.warn(\n",
            "2022-11-21 10:53:12,379 - INFO - allennlp.training.gradient_descent_trainer - GPU 0 memory usage: 315M\n",
            "2022-11-21 10:53:12,380 - CRITICAL - root - Uncaught exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/niallt/venvs/39_declutr/bin/allennlp\", line 8, in <module>\n",
            "    sys.exit(run())\n",
            "  File \"/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/allennlp/__main__.py\", line 39, in run\n",
            "    main(prog=\"allennlp\")\n",
            "  File \"/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/allennlp/commands/__init__.py\", line 120, in main\n",
            "    args.func(args)\n",
            "  File \"/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/allennlp/commands/train.py\", line 111, in train_model_from_args\n",
            "    train_model_from_file(\n",
            "  File \"/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/allennlp/commands/train.py\", line 177, in train_model_from_file\n",
            "    return train_model(\n",
            "  File \"/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/allennlp/commands/train.py\", line 258, in train_model\n",
            "    model = _train_worker(\n",
            "  File \"/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/allennlp/commands/train.py\", line 508, in _train_worker\n",
            "    metrics = train_loop.run()\n",
            "  File \"/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/allennlp/commands/train.py\", line 581, in run\n",
            "    return self.trainer.train()\n",
            "  File \"/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/allennlp/training/gradient_descent_trainer.py\", line 771, in train\n",
            "    metrics, epoch = self._try_train()\n",
            "  File \"/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/allennlp/training/gradient_descent_trainer.py\", line 793, in _try_train\n",
            "    train_metrics = self._train_epoch(epoch)\n",
            "  File \"/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/allennlp/training/gradient_descent_trainer.py\", line 444, in _train_epoch\n",
            "    self._pytorch_model.train()\n",
            "  File \"/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1732, in train\n",
            "    module.train(mode)\n",
            "  File \"/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1732, in train\n",
            "    module.train(mode)\n",
            "  File \"/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/allennlp/modules/token_embedders/pretrained_transformer_embedder.py\", line 165, in train\n",
            "    if self.eval_mode and name == \"transformer_model\":\n",
            "  File \"/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1185, in __getattr__\n",
            "    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
            "AttributeError: 'PretrainedTransformerEmbedderMLM' object has no attribute 'eval_mode'\n"
          ]
        }
      ],
      "source": [
        "!allennlp train \"../training_config/declutr_small_v2.jsonnet\" \\\n",
        "    --serialization-dir \"output\" \\\n",
        "    --overrides \"$overrides\" \\\n",
        "    --include-package \"declutr\" \\\n",
        "    -f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsbr6OMv16GQ"
      },
      "source": [
        "### ü§ó Exporting a trained model to HuggingFace Transformers\n",
        "\n",
        "We have provided a simple script to export a trained model so that it can be loaded with [Hugging Face Transformers](https://github.com/huggingface/transformers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqmWVD0y16GQ"
      },
      "outputs": [],
      "source": [
        "!wget -nc https://github.com/JohnGiorgi/DeCLUTR/blob/master/scripts/save_pretrained_hf.py\n",
        "!python save_pretrained_hf.py --archive-file \"output\" --save-directory \"output_transformers\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0-NTFaH16GQ"
      },
      "source": [
        "The model, saved to `--save-directory`, can then be loaded using the Hugging Face Transformers library\n",
        "\n",
        "> See the [embedding notebook](https://colab.research.google.com/github/JohnGiorgi/DeCLUTR/blob/master/notebooks/embedding.ipynb) for more details on using trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAl1zIya16GQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "  \n",
        "tokenizer = AutoTokenizer.from_pretrained(\"output_transformers\")\n",
        "model = AutoModel.from_pretrained(\"output_transformers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzQ0G4rp16GQ"
      },
      "source": [
        "> If you would like to upload your model to the Hugging Face model repository, follow the instructions [here](https://huggingface.co/transformers/model_sharing.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD5dZo18EE-S"
      },
      "source": [
        "## ‚ôªÔ∏è Conclusion\n",
        "\n",
        "That's it! In this notebook, we covered how to collect data for training the model, and specifically how _long_ that text needs to be. We then briefly covered configuring and running a training session. Please see [our paper](https://arxiv.org/abs/2006.03659) and [repo](https://github.com/JohnGiorgi/DeCLUTR) for more details, and don't hesitate to open an issue if you have any trouble!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "training.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('39_declutr': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "29abfe4b2a2a13e76bcb98af1501c4f384e087a48356ceb41341cb18ef9ca38a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
