{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8jt6ML03DS5"
      },
      "source": [
        "# Training your own model\n",
        "\n",
        "This notebook will walk you through training your own model using [DeCLUTR](https://github.com/JohnGiorgi/DeCLUTR)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU3Iod2-g0-o"
      },
      "source": [
        "## üîß Install the prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sr4r5pN40Kli"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/JohnGiorgi/DeCLUTR.git\n",
        "\n",
        "# go to main dir i.e. DeCLUTR on local and run \"pip install --editable .\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = \"E:/working_incident_data/cleaned_data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<multiprocessing.pool.Pool at 0x1a914b8ed08>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# testing multiprocessing fork - \n",
        "\n",
        "from multiprocessing import get_context\n",
        "num_processes = 8\n",
        "pool = get_context(\"spawn\").Pool(num_processes)\n",
        "pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zog7ApwuUD7_"
      },
      "source": [
        "## üìñ Preparing a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwnLpUmN4Art"
      },
      "source": [
        "\n",
        "A dataset is simply a file containing one item of text (a document, a scientific paper, etc.) per line. For demonstration purposes, we have provided a script that will download the [WikiText-103](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/) dataset and format it for training with our method.\n",
        "\n",
        "The only \"gotcha\" is that each piece of text needs to be long enough so that we can sample spans from it. In general, you should collect documents of a minimum length according to the following:\n",
        "\n",
        "```python\n",
        "min_length = num_anchors * max_span_len * 2\n",
        "```\n",
        "\n",
        "In our paper, we set `num_anchors=2` and `max_span_len=512`, so we require documents of `min_length=2048`. We simply need to provide this value as an argument when running the script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q0fwnwq23aAZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚¨á Downloaded WikiText-103\n",
            "Preprocessing text\n",
            "Preprocessing text\n",
            "Preprocessing text\n",
            "Writing to disk\n",
            "üíæ 100 preprocessed documents saved to: E:\\wiki_text\\wikitext-103\\train.txt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "train_data_path = \"E:/wiki_text/wikitext-103/train.txt\"\n",
        "\n",
        "# run this to download and preprocess data\n",
        "\n",
        "min_length = 2048\n",
        "\n",
        "!python ../scripts/preprocess_wikitext_103.py $train_data_path --min-length $min_length --max-instances 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUEFeupP6qy-"
      },
      "source": [
        "Lets confirm that our dataset looks as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "K7ffGXCn7Cpq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     500 E:/wiki_text/wikitext-103/train.txt\n",
            "     500 total\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wc: '#': No such file or directory\n",
            "wc: This: No such file or directory\n",
            "wc: should: No such file or directory\n",
            "wc: be: No such file or directory\n",
            "wc: approximately: No such file or directory\n",
            "wc: 17.8K: No such file or directory\n",
            "wc: lines: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!wc -l $train_data_path  # This should be approximately 17.8K lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "10DprWZc9iV6"
      },
      "outputs": [],
      "source": [
        "# !head -n 1 $train_data_path  # This should be a single Wikipedia entry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Look at sampling technique\n",
        "\n",
        "This will help get an idea of what "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\conda_envs\\37_declutr\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from declutr.common.contrastive_utils import sample_anchor_positive_pairs\n",
        "from declutr.losses import NTXentLoss\n",
        "import torch\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"this is just an example sentence to test out some sampling and loss calculation from DeCLUTR. We want to see exactly how it works in order to implement it for our own use case\"\n",
        "len_text = len(text.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# just go with one anchor for now\n",
        "\n",
        "num_anchors = 1\n",
        "max_span_len = int((len_text/2)/num_anchors) \n",
        "min_span_len = 5\n",
        "num_positives = 5\n",
        "sampling_strat = \"adjacent\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "anchor_spans, positive_spans = sample_anchor_positive_pairs(\n",
        "    text = text,\n",
        "    num_anchors = num_anchors,\n",
        "    num_positives = num_positives,\n",
        "    max_span_len = max_span_len,\n",
        "    min_span_len = min_span_len,\n",
        "    sampling_strategy = sampling_strat\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['to see exactly how it works in order to implement it for our own use']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "anchor_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['loss calculation from DeCLUTR. We want',\n",
              " 'and loss calculation from DeCLUTR. We want',\n",
              " 'sampling and loss calculation from DeCLUTR. We want',\n",
              " 'sampling and loss calculation from DeCLUTR. We want',\n",
              " 'some sampling and loss calculation from DeCLUTR. We want']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "positive_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test loss function\n",
        "anchor_emb = torch.rand(64).unsqueeze(0)\n",
        "pos_emb = torch.rand(64).unsqueeze(0)\n",
        "neg_emb = torch.rand(64).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "anchor_pos_embs = torch.cat((anchor_emb, pos_emb))\n",
        "loss_func = NTXentLoss\n",
        "embs, labels = NTXentLoss.get_embeddings_and_label(anchor_emb, pos_emb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKYdambZ59nM"
      },
      "source": [
        "## üèÉ Training the model\n",
        "\n",
        "Once you have collected the dataset, you can easily initiate a training session with the `allennlp train` command. An experiment is configured using a [Jsonnet](https://jsonnet.org/) config file. Lets take a look at the config for the DeCLUTR-small model presented in [our paper](https://arxiv.org/abs/2006.03659):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open(\"../training_config/declutr_small.jsonnet\", \"r\") as f:\n",
        "#     print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f1HqWSscWOx"
      },
      "source": [
        "\n",
        "The only thing to configure is the path to the training set (`train_data_path`), which can be passed to `allennlp train` via the `--overrides` argument (but you can also provide it in your config file directly, if you prefer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YS9VuxESBcr3"
      },
      "outputs": [],
      "source": [
        "# overrides = (\n",
        "#     f\"{{'train_data_path': '{train_data_path}', \"\n",
        "#     # lower the batch size to be able to train on Colab GPUs\n",
        "#     \"'data_loader.batch_size': 2, \"\n",
        "#     # training examples / batch size. Not required, but gives us a more informative progress bar during training\n",
        "#     \"'data_loader.batches_per_epoch': None}\"\n",
        "# )\n",
        "\n",
        "\n",
        "overrides = (\n",
        "    f\"{{'train_data_path': '{train_data_path}', \"\n",
        "    # lower the batch size to be able to train on Colab GPUs\n",
        "    \"'data_loader.batch_size': 4,}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2v4tiiXgBC2M"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"{'train_data_path': 'E:/wiki_text/wikitext-103/train.txt', 'data_loader.batch_size': 4,}\""
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "overrides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Db_cNfZ76KRf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-09-21 13:50:51,055 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2022-09-21 13:50:51,056 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2022-09-21 13:50:51,056 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2022-09-21 13:50:51,204 - INFO - allennlp.common.checks - Pytorch version: 1.6.0\n",
            "2022-09-21 13:50:51,205 - INFO - allennlp.common.params - type = default\n",
            "2022-09-21 13:50:51,205 - INFO - allennlp.common.params - dataset_reader.type = declutr\n",
            "2022-09-21 13:50:51,205 - INFO - allennlp.common.params - dataset_reader.lazy = False\n",
            "2022-09-21 13:50:51,205 - INFO - allennlp.common.params - dataset_reader.cache_directory = None\n",
            "2022-09-21 13:50:51,206 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2022-09-21 13:50:51,206 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2022-09-21 13:50:51,206 - INFO - allennlp.common.params - dataset_reader.manual_multi_process_sharding = False\n",
            "2022-09-21 13:50:51,206 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n",
            "2022-09-21 13:50:51,207 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = distilroberta-base\n",
            "2022-09-21 13:50:51,207 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = True\n",
            "2022-09-21 13:50:51,207 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = 510\n",
            "2022-09-21 13:50:51,207 - INFO - allennlp.common.params - dataset_reader.tokenizer.stride = 0\n",
            "2022-09-21 13:50:51,207 - INFO - allennlp.common.params - dataset_reader.tokenizer.truncation_strategy = longest_first\n",
            "2022-09-21 13:50:51,207 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n",
            "2022-09-21 13:50:51,586 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-config.json from cache at C:\\Users\\niall/.cache\\torch\\transformers\\d52ced8fd31ba6aa311b6eeeae65178cca00ddd6333c087be4601dc46c20bd96.1221e000e415a518ec3c28f32c14c1dd3baa36dc0537db4848b7236b38f50313\n",
            "2022-09-21 13:50:51,587 - INFO - transformers.configuration_utils - Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "2022-09-21 13:50:52,286 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-vocab.json from cache at C:\\Users\\niall/.cache\\torch\\transformers\\5f11352d3c3e932888f3ba75bc24579eacb5d1596d39ce56166aeae8fd363df8.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "2022-09-21 13:50:52,286 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-merges.txt from cache at C:\\Users\\niall/.cache\\torch\\transformers\\01f63a14ad93494c050af2090c59930fb787bdfb347c4cad7ce9063e1a5fe140.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "2022-09-21 13:50:52,758 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-config.json from cache at C:\\Users\\niall/.cache\\torch\\transformers\\d52ced8fd31ba6aa311b6eeeae65178cca00ddd6333c087be4601dc46c20bd96.1221e000e415a518ec3c28f32c14c1dd3baa36dc0537db4848b7236b38f50313\n",
            "2022-09-21 13:50:52,758 - INFO - transformers.configuration_utils - Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "2022-09-21 13:50:53,460 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-vocab.json from cache at C:\\Users\\niall/.cache\\torch\\transformers\\5f11352d3c3e932888f3ba75bc24579eacb5d1596d39ce56166aeae8fd363df8.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "2022-09-21 13:50:53,460 - INFO - transformers.tokenization_utils_base - loading file https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-merges.txt from cache at C:\\Users\\niall/.cache\\torch\\transformers\\01f63a14ad93494c050af2090c59930fb787bdfb347c4cad7ce9063e1a5fe140.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "2022-09-21 13:50:53,553 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer\n",
            "2022-09-21 13:50:53,553 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2022-09-21 13:50:53,554 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = distilroberta-base\n",
            "2022-09-21 13:50:53,554 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2022-09-21 13:50:53,554 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = None\n",
            "2022-09-21 13:50:53,554 - INFO - allennlp.common.params - dataset_reader.num_anchors = 2\n",
            "2022-09-21 13:50:53,554 - INFO - allennlp.common.params - dataset_reader.num_positives = 2\n",
            "2022-09-21 13:50:53,555 - INFO - allennlp.common.params - dataset_reader.max_span_len = 512\n",
            "2022-09-21 13:50:53,555 - INFO - allennlp.common.params - dataset_reader.min_span_len = 32\n",
            "2022-09-21 13:50:53,555 - INFO - allennlp.common.params - dataset_reader.sampling_strategy = None\n",
            "2022-09-21 13:50:53,555 - INFO - allennlp.common.params - train_data_path = E:/wiki_text/wikitext-103/train.txt\n",
            "2022-09-21 13:50:53,555 - INFO - allennlp.common.params - datasets_for_vocab_creation = None\n",
            "2022-09-21 13:50:53,555 - INFO - allennlp.common.params - validation_dataset_reader = None\n",
            "2022-09-21 13:50:53,556 - INFO - allennlp.common.params - validation_data_path = None\n",
            "2022-09-21 13:50:53,556 - INFO - allennlp.common.params - validation_data_loader = None\n",
            "2022-09-21 13:50:53,556 - INFO - allennlp.common.params - test_data_path = None\n",
            "2022-09-21 13:50:53,556 - INFO - allennlp.common.params - evaluate_on_test = False\n",
            "2022-09-21 13:50:53,556 - INFO - allennlp.common.params - batch_weight_key = \n",
            "2022-09-21 13:50:53,556 - INFO - allennlp.training.util - Reading training data from E:/wiki_text/wikitext-103/train.txt\n",
            "2022-09-21 13:50:53,562 - INFO - declutr.dataset_reader - Reading instances from lines in file at: E:/wiki_text/wikitext-103/train.txt\n",
            "2022-09-21 13:50:57,131 - INFO - allennlp.common.params - vocabulary.type = empty\n",
            "2022-09-21 13:50:57,131 - INFO - allennlp.common.params - model.type = declutr\n",
            "2022-09-21 13:50:57,131 - INFO - allennlp.common.params - model.regularizer = None\n",
            "2022-09-21 13:50:57,132 - INFO - allennlp.common.params - model.text_field_embedder.type = mlm\n",
            "2022-09-21 13:50:57,132 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.type = pretrained_transformer_mlm\n",
            "2022-09-21 13:50:57,132 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.model_name = distilroberta-base\n",
            "2022-09-21 13:50:57,132 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.max_length = None\n",
            "2022-09-21 13:50:57,132 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.sub_module = None\n",
            "2022-09-21 13:50:57,132 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.train_parameters = True\n",
            "2022-09-21 13:50:57,132 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.last_layer_only = True\n",
            "2022-09-21 13:50:57,133 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_file = None\n",
            "2022-09-21 13:50:57,133 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.override_weights_strip_prefix = None\n",
            "2022-09-21 13:50:57,133 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.gradient_checkpointing = None\n",
            "2022-09-21 13:50:57,133 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.tokenizer_kwargs = None\n",
            "2022-09-21 13:50:57,133 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.transformer_kwargs = None\n",
            "2022-09-21 13:50:57,133 - INFO - allennlp.common.params - model.text_field_embedder.token_embedders.tokens.masked_language_modeling = True\n",
            "2022-09-21 13:50:57,502 - INFO - transformers.configuration_utils - loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-config.json from cache at C:\\Users\\niall/.cache\\torch\\transformers\\d52ced8fd31ba6aa311b6eeeae65178cca00ddd6333c087be4601dc46c20bd96.1221e000e415a518ec3c28f32c14c1dd3baa36dc0537db4848b7236b38f50313\n",
            "2022-09-21 13:50:57,502 - INFO - transformers.configuration_utils - Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 6,\n",
            "  \"output_hidden_states\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "2022-09-21 13:50:57,579 - INFO - transformers.modeling_utils - loading weights file https://cdn.huggingface.co/distilroberta-base-pytorch_model.bin from cache at C:\\Users\\niall/.cache\\torch\\transformers\\5aab0d7dfa1db7d97ead13a37479db888b133a51a05ae4ab62ff5c8d1fcabb65.52b6ec356fb91985b3940e086d1b2ebf8cd40f8df0ba1cabf4cac27769dee241\n",
            "2022-09-21 13:51:00,809 - INFO - transformers.modeling_utils - All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
            "\n",
            "2022-09-21 13:51:00,809 - WARNING - transformers.modeling_utils - Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2022-09-21 13:51:00,810 - INFO - allennlp.common.params - model.seq2vec_encoder = None\n",
            "2022-09-21 13:51:00,810 - INFO - allennlp.common.params - model.feedforward = None\n",
            "2022-09-21 13:51:00,810 - INFO - allennlp.common.params - model.miner = None\n",
            "2022-09-21 13:51:00,810 - INFO - allennlp.common.params - model.loss.type = nt_xent\n",
            "2022-09-21 13:51:00,810 - INFO - allennlp.common.params - model.loss.temperature = 0.05\n",
            "2022-09-21 13:51:00,811 - INFO - allennlp.common.params - model.scale_fix = False\n",
            "2022-09-21 13:51:00,811 - INFO - allennlp.common.params - model.initializer = <allennlp.nn.initializers.InitializerApplicator object at 0x00000251A9632848>\n",
            "2022-09-21 13:51:00,811 - INFO - allennlp.nn.initializers - Initializing parameters\n",
            "2022-09-21 13:51:00,811 - INFO - allennlp.nn.initializers - Done initializing parameters; the following parameters are using their default initialization from their code\n",
            "2022-09-21 13:51:00,811 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.bias\n",
            "2022-09-21 13:51:00,811 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.bias\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.weight\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.bias\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.weight\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.bias\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.weight\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.position_embeddings.weight\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.token_type_embeddings.weight\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.word_embeddings.weight\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.bias\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.weight\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.bias\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.weight\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.bias\n",
            "2022-09-21 13:51:00,812 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.weight\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.bias\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.weight\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.bias\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.weight\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.bias\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.weight\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.bias\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.weight\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.bias\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.weight\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.bias\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.weight\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.bias\n",
            "2022-09-21 13:51:00,813 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.weight\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.bias\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.weight\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.bias\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.weight\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.bias\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.weight\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.bias\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.weight\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.bias\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.weight\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.bias\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.weight\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.bias\n",
            "2022-09-21 13:51:00,814 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.weight\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.bias\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.weight\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.bias\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.weight\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.bias\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.weight\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.bias\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.weight\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.bias\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.weight\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.bias\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.weight\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.bias\n",
            "2022-09-21 13:51:00,815 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.weight\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.bias\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.weight\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.bias\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.weight\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.bias\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.weight\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.bias\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.weight\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.bias\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.weight\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.bias\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.weight\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.bias\n",
            "2022-09-21 13:51:00,816 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.weight\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.bias\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.weight\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.bias\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.weight\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.bias\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.weight\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.bias\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.weight\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.bias\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.weight\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.bias\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.weight\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.bias\n",
            "2022-09-21 13:51:00,817 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.weight\n",
            "2022-09-21 13:51:00,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.bias\n",
            "2022-09-21 13:51:00,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.weight\n",
            "2022-09-21 13:51:00,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.bias\n",
            "2022-09-21 13:51:00,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.weight\n",
            "2022-09-21 13:51:00,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.bias\n",
            "2022-09-21 13:51:00,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.weight\n",
            "2022-09-21 13:51:00,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.bias\n",
            "2022-09-21 13:51:00,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.weight\n",
            "2022-09-21 13:51:00,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.pooler.dense.bias\n",
            "2022-09-21 13:51:00,818 - INFO - allennlp.nn.initializers -    _text_field_embedder.token_embedder_tokens.transformer_model.roberta.pooler.dense.weight\n",
            "2022-09-21 13:51:00,819 - INFO - allennlp.common.params - data_loader.type = pytorch_dataloader\n",
            "2022-09-21 13:51:00,820 - INFO - allennlp.common.params - data_loader.batch_size = 4\n",
            "2022-09-21 13:51:00,820 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2022-09-21 13:51:00,820 - INFO - allennlp.common.params - data_loader.sampler = None\n",
            "2022-09-21 13:51:00,820 - INFO - allennlp.common.params - data_loader.batch_sampler = None\n",
            "2022-09-21 13:51:00,820 - INFO - allennlp.common.params - data_loader.num_workers = 0\n",
            "2022-09-21 13:51:00,820 - INFO - allennlp.common.params - data_loader.pin_memory = False\n",
            "2022-09-21 13:51:00,820 - INFO - allennlp.common.params - data_loader.drop_last = True\n",
            "2022-09-21 13:51:00,820 - INFO - allennlp.common.params - data_loader.timeout = 0\n",
            "2022-09-21 13:51:00,820 - INFO - allennlp.common.params - data_loader.worker_init_fn = None\n",
            "2022-09-21 13:51:00,820 - INFO - allennlp.common.params - data_loader.multiprocessing_context = None\n",
            "2022-09-21 13:51:00,820 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2022-09-21 13:51:00,821 - INFO - allennlp.common.params - trainer.type = gradient_descent\n",
            "2022-09-21 13:51:00,821 - INFO - allennlp.common.params - trainer.patience = None\n",
            "2022-09-21 13:51:00,821 - INFO - allennlp.common.params - trainer.validation_metric = -loss\n",
            "2022-09-21 13:51:00,821 - INFO - allennlp.common.params - trainer.num_epochs = 1\n",
            "2022-09-21 13:51:00,821 - INFO - allennlp.common.params - trainer.cuda_device = None\n",
            "2022-09-21 13:51:00,821 - INFO - allennlp.common.params - trainer.grad_norm = 1\n",
            "2022-09-21 13:51:00,822 - INFO - allennlp.common.params - trainer.grad_clipping = None\n",
            "2022-09-21 13:51:00,822 - INFO - allennlp.common.params - trainer.distributed = None\n",
            "2022-09-21 13:51:00,822 - INFO - allennlp.common.params - trainer.world_size = 1\n",
            "2022-09-21 13:51:00,822 - INFO - allennlp.common.params - trainer.num_gradient_accumulation_steps = 1\n",
            "2022-09-21 13:51:00,822 - INFO - allennlp.common.params - trainer.use_amp = True\n",
            "2022-09-21 13:51:00,822 - INFO - allennlp.common.params - trainer.no_grad = None\n",
            "2022-09-21 13:51:00,822 - INFO - allennlp.common.params - trainer.momentum_scheduler = None\n",
            "2022-09-21 13:51:00,822 - INFO - allennlp.common.params - trainer.tensorboard_writer = None\n",
            "2022-09-21 13:51:00,822 - INFO - allennlp.common.params - trainer.moving_average = None\n",
            "2022-09-21 13:51:00,823 - INFO - allennlp.common.params - trainer.batch_callbacks = None\n",
            "2022-09-21 13:51:00,823 - INFO - allennlp.common.params - trainer.epoch_callbacks = None\n",
            "2022-09-21 13:51:02,255 - INFO - allennlp.common.params - trainer.optimizer.type = huggingface_adamw\n",
            "2022-09-21 13:51:02,256 - INFO - allennlp.common.params - trainer.optimizer.lr = 5e-05\n",
            "2022-09-21 13:51:02,256 - INFO - allennlp.common.params - trainer.optimizer.betas = (0.9, 0.999)\n",
            "2022-09-21 13:51:02,256 - INFO - allennlp.common.params - trainer.optimizer.eps = 1e-06\n",
            "2022-09-21 13:51:02,256 - INFO - allennlp.common.params - trainer.optimizer.weight_decay = 0.1\n",
            "2022-09-21 13:51:02,256 - INFO - allennlp.common.params - trainer.optimizer.correct_bias = False\n",
            "2022-09-21 13:51:02,257 - INFO - allennlp.training.optimizers - Done constructing parameter groups.\n",
            "2022-09-21 13:51:02,257 - INFO - allennlp.training.optimizers - Group 0: ['_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.lm_head.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.pooler.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.bias', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.weight'], {'weight_decay': 0}\n",
            "2022-09-21 13:51:02,257 - INFO - allennlp.training.optimizers - Group 1: ['_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.position_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.word_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.pooler.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.token_type_embeddings.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.weight', '_text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.weight'], {}\n",
            "2022-09-21 13:51:02,258 - INFO - allennlp.training.optimizers - Number of trainable parameters: 82760793\n",
            "2022-09-21 13:51:02,258 - INFO - allennlp.common.util - The following parameters are Frozen (without gradient):\n",
            "2022-09-21 13:51:02,258 - INFO - allennlp.common.util - The following parameters are Tunable (with gradient):\n",
            "2022-09-21 13:51:02,258 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.word_embeddings.weight\n",
            "2022-09-21 13:51:02,259 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.position_embeddings.weight\n",
            "2022-09-21 13:51:02,259 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.token_type_embeddings.weight\n",
            "2022-09-21 13:51:02,259 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.weight\n",
            "2022-09-21 13:51:02,259 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.embeddings.LayerNorm.bias\n",
            "2022-09-21 13:51:02,259 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.weight\n",
            "2022-09-21 13:51:02,259 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.query.bias\n",
            "2022-09-21 13:51:02,259 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.weight\n",
            "2022-09-21 13:51:02,259 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.key.bias\n",
            "2022-09-21 13:51:02,259 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.weight\n",
            "2022-09-21 13:51:02,259 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.self.value.bias\n",
            "2022-09-21 13:51:02,259 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.weight\n",
            "2022-09-21 13:51:02,259 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.dense.bias\n",
            "2022-09-21 13:51:02,259 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:51:02,259 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:51:02,259 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.weight\n",
            "2022-09-21 13:51:02,260 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.intermediate.dense.bias\n",
            "2022-09-21 13:51:02,260 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.weight\n",
            "2022-09-21 13:51:02,260 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.dense.bias\n",
            "2022-09-21 13:51:02,260 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.weight\n",
            "2022-09-21 13:51:02,260 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.0.output.LayerNorm.bias\n",
            "2022-09-21 13:51:02,260 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.weight\n",
            "2022-09-21 13:51:02,260 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.query.bias\n",
            "2022-09-21 13:51:02,260 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.weight\n",
            "2022-09-21 13:51:02,260 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.key.bias\n",
            "2022-09-21 13:51:02,260 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.weight\n",
            "2022-09-21 13:51:02,260 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.self.value.bias\n",
            "2022-09-21 13:51:02,260 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.weight\n",
            "2022-09-21 13:51:02,260 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.dense.bias\n",
            "2022-09-21 13:51:02,260 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:51:02,260 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:51:02,261 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.weight\n",
            "2022-09-21 13:51:02,261 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.intermediate.dense.bias\n",
            "2022-09-21 13:51:02,261 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.weight\n",
            "2022-09-21 13:51:02,261 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.dense.bias\n",
            "2022-09-21 13:51:02,261 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.weight\n",
            "2022-09-21 13:51:02,261 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.1.output.LayerNorm.bias\n",
            "2022-09-21 13:51:02,261 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.weight\n",
            "2022-09-21 13:51:02,261 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.query.bias\n",
            "2022-09-21 13:51:02,261 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.weight\n",
            "2022-09-21 13:51:02,261 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.key.bias\n",
            "2022-09-21 13:51:02,261 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.weight\n",
            "2022-09-21 13:51:02,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.self.value.bias\n",
            "2022-09-21 13:51:02,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.weight\n",
            "2022-09-21 13:51:02,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.dense.bias\n",
            "2022-09-21 13:51:02,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:51:02,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:51:02,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.weight\n",
            "2022-09-21 13:51:02,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.intermediate.dense.bias\n",
            "2022-09-21 13:51:02,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.weight\n",
            "2022-09-21 13:51:02,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.dense.bias\n",
            "2022-09-21 13:51:02,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.weight\n",
            "2022-09-21 13:51:02,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.2.output.LayerNorm.bias\n",
            "2022-09-21 13:51:02,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.weight\n",
            "2022-09-21 13:51:02,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.query.bias\n",
            "2022-09-21 13:51:02,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.weight\n",
            "2022-09-21 13:51:02,262 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.key.bias\n",
            "2022-09-21 13:51:02,263 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.weight\n",
            "2022-09-21 13:51:02,263 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.self.value.bias\n",
            "2022-09-21 13:51:02,263 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.weight\n",
            "2022-09-21 13:51:02,263 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.dense.bias\n",
            "2022-09-21 13:51:02,263 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:51:02,263 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:51:02,263 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.weight\n",
            "2022-09-21 13:51:02,263 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.intermediate.dense.bias\n",
            "2022-09-21 13:51:02,263 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.weight\n",
            "2022-09-21 13:51:02,263 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.dense.bias\n",
            "2022-09-21 13:51:02,263 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.weight\n",
            "2022-09-21 13:51:02,263 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.3.output.LayerNorm.bias\n",
            "2022-09-21 13:51:02,263 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.weight\n",
            "2022-09-21 13:51:02,263 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.query.bias\n",
            "2022-09-21 13:51:02,264 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.weight\n",
            "2022-09-21 13:51:02,264 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.key.bias\n",
            "2022-09-21 13:51:02,264 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.weight\n",
            "2022-09-21 13:51:02,264 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.self.value.bias\n",
            "2022-09-21 13:51:02,264 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.weight\n",
            "2022-09-21 13:51:02,264 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.dense.bias\n",
            "2022-09-21 13:51:02,264 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:51:02,264 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:51:02,264 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.weight\n",
            "2022-09-21 13:51:02,264 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.intermediate.dense.bias\n",
            "2022-09-21 13:51:02,264 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.weight\n",
            "2022-09-21 13:51:02,264 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.dense.bias\n",
            "2022-09-21 13:51:02,264 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.weight\n",
            "2022-09-21 13:51:02,264 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.4.output.LayerNorm.bias\n",
            "2022-09-21 13:51:02,264 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.weight\n",
            "2022-09-21 13:51:02,265 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.query.bias\n",
            "2022-09-21 13:51:02,265 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.weight\n",
            "2022-09-21 13:51:02,265 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.key.bias\n",
            "2022-09-21 13:51:02,265 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.weight\n",
            "2022-09-21 13:51:02,265 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.self.value.bias\n",
            "2022-09-21 13:51:02,265 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.weight\n",
            "2022-09-21 13:51:02,265 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.dense.bias\n",
            "2022-09-21 13:51:02,265 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.weight\n",
            "2022-09-21 13:51:02,265 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.attention.output.LayerNorm.bias\n",
            "2022-09-21 13:51:02,265 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.weight\n",
            "2022-09-21 13:51:02,265 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.intermediate.dense.bias\n",
            "2022-09-21 13:51:02,265 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.weight\n",
            "2022-09-21 13:51:02,265 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.dense.bias\n",
            "2022-09-21 13:51:02,265 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.weight\n",
            "2022-09-21 13:51:02,265 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.encoder.layer.5.output.LayerNorm.bias\n",
            "2022-09-21 13:51:02,266 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.pooler.dense.weight\n",
            "2022-09-21 13:51:02,266 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.roberta.pooler.dense.bias\n",
            "2022-09-21 13:51:02,266 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.bias\n",
            "2022-09-21 13:51:02,266 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.weight\n",
            "2022-09-21 13:51:02,266 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.dense.bias\n",
            "2022-09-21 13:51:02,266 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.weight\n",
            "2022-09-21 13:51:02,266 - INFO - allennlp.common.util - _text_field_embedder.token_embedder_tokens.transformer_model.lm_head.layer_norm.bias\n",
            "2022-09-21 13:51:02,266 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.type = slanted_triangular\n",
            "2022-09-21 13:51:02,266 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.cut_frac = 0.1\n",
            "2022-09-21 13:51:02,267 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.ratio = 32\n",
            "2022-09-21 13:51:02,267 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.last_epoch = -1\n",
            "2022-09-21 13:51:02,267 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.gradual_unfreezing = False\n",
            "2022-09-21 13:51:02,267 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.discriminative_fine_tuning = False\n",
            "2022-09-21 13:51:02,267 - INFO - allennlp.common.params - trainer.learning_rate_scheduler.decay_factor = 0.38\n",
            "2022-09-21 13:51:02,267 - INFO - allennlp.common.params - trainer.checkpointer.type = default\n",
            "2022-09-21 13:51:02,267 - INFO - allennlp.common.params - trainer.checkpointer.keep_serialized_model_every_num_seconds = None\n",
            "2022-09-21 13:51:02,267 - INFO - allennlp.common.params - trainer.checkpointer.num_serialized_models_to_keep = -1\n",
            "2022-09-21 13:51:02,267 - INFO - allennlp.common.params - trainer.checkpointer.model_save_interval = None\n",
            "2022-09-21 13:51:02,271 - INFO - allennlp.training.trainer - Beginning training.\n",
            "2022-09-21 13:51:02,271 - INFO - allennlp.training.trainer - Epoch 0/0\n",
            "2022-09-21 13:51:02,271 - INFO - allennlp.training.trainer - Worker 0 memory usage MB: 0.0\n",
            "2022-09-21 13:51:02,310 - INFO - allennlp.training.trainer - GPU 0 memory usage MB: 796\n",
            "2022-09-21 13:51:02,311 - INFO - allennlp.training.trainer - Training\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 497])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 497])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 497])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([1008535986722519999, 4933943583666081203, 2607410277535464152,\n",
            "        6962253257909195170, 1827149348732502422, 8537296492787104864,\n",
            "        4800909042745599660, 2827511192811342410, 1008535986722519999,\n",
            "        4933943583666081203, 2607410277535464152, 6962253257909195170,\n",
            "        1827149348732502422, 8537296492787104864, 4800909042745599660,\n",
            "        2827511192811342410], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 486])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 486])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 486])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([4062454826293940239, 7782924986558656896,  870618211257141440,\n",
            "        8629997111080219827, 6766831486175047347, 4963236653265618308,\n",
            "        9212936989237016802, 3897339909342132777, 4062454826293940239,\n",
            "        7782924986558656896,  870618211257141440, 8629997111080219827,\n",
            "        6766831486175047347, 4963236653265618308, 9212936989237016802,\n",
            "        3897339909342132777], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 475])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 475])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 475])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([2141891889143041034, 7946523522633355393, 3883998484418700350,\n",
            "        6591069503533592933, 5502096171627157755, 1390117566987805477,\n",
            "         817816624894167220, 8372711951584746968, 2141891889143041034,\n",
            "        7946523522633355393, 3883998484418700350, 6591069503533592933,\n",
            "        5502096171627157755, 1390117566987805477,  817816624894167220,\n",
            "        8372711951584746968], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 410])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 410])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 410])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([2593206919282032099, 6821686347050774002, 7018275945504753176,\n",
            "        7080410497500051158, 3982279390250879793, 5508891023268943471,\n",
            "        7443006203072031337, 6397647508642882197, 2593206919282032099,\n",
            "        6821686347050774002, 7018275945504753176, 7080410497500051158,\n",
            "        3982279390250879793, 5508891023268943471, 7443006203072031337,\n",
            "        6397647508642882197], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 497])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 497])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 497])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([6630404730580834818, 7457089429054914903, 1559499419916070404,\n",
            "        2751097088403282221, 9193229091876227136,  538442097709131552,\n",
            "        8937018854109166044,  484625801196578264, 6630404730580834818,\n",
            "        7457089429054914903, 1559499419916070404, 2751097088403282221,\n",
            "        9193229091876227136,  538442097709131552, 8937018854109166044,\n",
            "         484625801196578264], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 453])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 453])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 453])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([2623524629157445385, 4806896573027933799, 2971469307990426740,\n",
            "        4851270592296060455, 1041038038391359029, 8071375358545656994,\n",
            "         455526174069733270, 5176623382573542034, 2623524629157445385,\n",
            "        4806896573027933799, 2971469307990426740, 4851270592296060455,\n",
            "        1041038038391359029, 8071375358545656994,  455526174069733270,\n",
            "        5176623382573542034], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 472])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 472])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 472])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([6978771107092668992,  582720934592913198, 2104581645106515016,\n",
            "        5539452491152499832,  862563698767165799,  281345813035300550,\n",
            "        5579089832465621522, 2665920757124210921, 6978771107092668992,\n",
            "         582720934592913198, 2104581645106515016, 5539452491152499832,\n",
            "         862563698767165799,  281345813035300550, 5579089832465621522,\n",
            "        2665920757124210921], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 501])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 501])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 501])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([1312170630386804264, 7451055096547730219, 4018994392250999558,\n",
            "          11493652773077458, 3260228724812714930, 9081423028620554474,\n",
            "        8788988600191305322, 6874771026321306941, 1312170630386804264,\n",
            "        7451055096547730219, 4018994392250999558,   11493652773077458,\n",
            "        3260228724812714930, 9081423028620554474, 8788988600191305322,\n",
            "        6874771026321306941], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 476])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 476])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 476])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([8602207885382949120, 2529048719106753630, 7915031577306325141,\n",
            "         113913641670422796, 4768293434914197024, 3253818520364595429,\n",
            "        7569053234077054586, 2003739049270586017, 8602207885382949120,\n",
            "        2529048719106753630, 7915031577306325141,  113913641670422796,\n",
            "        4768293434914197024, 3253818520364595429, 7569053234077054586,\n",
            "        2003739049270586017], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 437])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 437])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 437])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([6436221654842305812,  993843966865298620, 6722163774658223854,\n",
            "        4293906704607670295, 6596526182400211151, 8899770958139234654,\n",
            "        4270728638572189441, 1741851992501357685, 6436221654842305812,\n",
            "         993843966865298620, 6722163774658223854, 4293906704607670295,\n",
            "        6596526182400211151, 8899770958139234654, 4270728638572189441,\n",
            "        1741851992501357685], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 487])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 487])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 487])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([5747966992823003043, 3681874702620733048,   15193024850783713,\n",
            "         590034543520767372, 8936046174570820881, 6116821164801079230,\n",
            "        5771516596513944863, 2663443539512647674, 5747966992823003043,\n",
            "        3681874702620733048,   15193024850783713,  590034543520767372,\n",
            "        8936046174570820881, 6116821164801079230, 5771516596513944863,\n",
            "        2663443539512647674], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 461])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 461])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 461])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([3581387865452665453, 3682190315097773079, 6804693903614705200,\n",
            "        2279922197861543033, 4391919732198050059, 3571428883517402717,\n",
            "        6356307868493581162, 7737239958017987112, 3581387865452665453,\n",
            "        3682190315097773079, 6804693903614705200, 2279922197861543033,\n",
            "        4391919732198050059, 3571428883517402717, 6356307868493581162,\n",
            "        7737239958017987112], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 492])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 492])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 492])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([7255131213434556478, 1344503925609116549, 2361043131574660108,\n",
            "        8448817871796260738, 8485429796252569507, 7817591420994905321,\n",
            "        6771489352932971942, 1532127192997656767, 7255131213434556478,\n",
            "        1344503925609116549, 2361043131574660108, 8448817871796260738,\n",
            "        8485429796252569507, 7817591420994905321, 6771489352932971942,\n",
            "        1532127192997656767], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 431])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 431])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 431])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([4149118809078377528, 2240510898470926357, 4567222533018715500,\n",
            "        6885207071410479596, 4913567709155662588, 3827725363346195374,\n",
            "        7073299791107455022, 5032361229841425438, 4149118809078377528,\n",
            "        2240510898470926357, 4567222533018715500, 6885207071410479596,\n",
            "        4913567709155662588, 3827725363346195374, 7073299791107455022,\n",
            "        5032361229841425438], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 465])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 465])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 465])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([4064128365313297955, 8510048533205293919, 2870893009750320683,\n",
            "        2905417503930168789, 8864708572269652405,  533001813745921339,\n",
            "        4023858888591181571, 9213593694648188873, 4064128365313297955,\n",
            "        8510048533205293919, 2870893009750320683, 2905417503930168789,\n",
            "        8864708572269652405,  533001813745921339, 4023858888591181571,\n",
            "        9213593694648188873], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 460])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 460])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 460])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([3213367369556921456, 7386956368273621549, 5594737906650479376,\n",
            "        4416644724599084848, 7714743441363561075, 8042883843666953316,\n",
            "        6101865567225709232, 1113120260417084921, 3213367369556921456,\n",
            "        7386956368273621549, 5594737906650479376, 4416644724599084848,\n",
            "        7714743441363561075, 8042883843666953316, 6101865567225709232,\n",
            "        1113120260417084921], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 451])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 451])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 451])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([6825633186206772131,  576923353855007426, 2364653369498533074,\n",
            "        3465082583745816839, 3452019876417699967, 6727084714960820334,\n",
            "        3515524112027784353, 2183161215158623561, 6825633186206772131,\n",
            "         576923353855007426, 2364653369498533074, 3465082583745816839,\n",
            "        3452019876417699967, 6727084714960820334, 3515524112027784353,\n",
            "        2183161215158623561], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 479])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 479])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 479])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([4597249732919630616, 8121840881270717582, 8520934685477613609,\n",
            "        6416160497336335663, 1667393547299266911, 5560478264297072519,\n",
            "        5840696211217046332, 5398049358251313699, 4597249732919630616,\n",
            "        8121840881270717582, 8520934685477613609, 6416160497336335663,\n",
            "        1667393547299266911, 5560478264297072519, 5840696211217046332,\n",
            "        5398049358251313699], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 473])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 473])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 473])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([4914527218760914650, 4584986039377861907, 1732196796739916488,\n",
            "        7107553231370769653, 6458886809884647841, 3073813214604059223,\n",
            "         725022103884591053, 5265064812077607505, 4914527218760914650,\n",
            "        4584986039377861907, 1732196796739916488, 7107553231370769653,\n",
            "        6458886809884647841, 3073813214604059223,  725022103884591053,\n",
            "        5265064812077607505], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 490])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 490])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 490])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([2425728192405324928, 2766258949362543099, 5280023849460984455,\n",
            "        1865672336559499882, 2861152688021607667, 9093241049856069820,\n",
            "        4796890772879698139, 2959872553406972949, 2425728192405324928,\n",
            "        2766258949362543099, 5280023849460984455, 1865672336559499882,\n",
            "        2861152688021607667, 9093241049856069820, 4796890772879698139,\n",
            "        2959872553406972949], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 462])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 462])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 462])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([8407250421240583714, 3331966811793304954, 4265680594976365922,\n",
            "        3765267141427038476, 8641536528627863884, 8223918618136013651,\n",
            "        7978885257593376366, 8629511321678864826, 8407250421240583714,\n",
            "        3331966811793304954, 4265680594976365922, 3765267141427038476,\n",
            "        8641536528627863884, 8223918618136013651, 7978885257593376366,\n",
            "        8629511321678864826], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 400])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 400])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 400])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([1184117380885555880, 4578570908121710708,  232725622821541708,\n",
            "        2717068287715177412, 2809501211268143105, 8587173916191363037,\n",
            "        1608845863138033204, 2258902496446494126, 1184117380885555880,\n",
            "        4578570908121710708,  232725622821541708, 2717068287715177412,\n",
            "        2809501211268143105, 8587173916191363037, 1608845863138033204,\n",
            "        2258902496446494126], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 474])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 474])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 474])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([1181579537304426224,  756925142119294835, 5128381614974819336,\n",
            "        2255265946588705647, 6079118528315193586, 3396279506812426887,\n",
            "        8533636393847729449, 1450305737820992457, 1181579537304426224,\n",
            "         756925142119294835, 5128381614974819336, 2255265946588705647,\n",
            "        6079118528315193586, 3396279506812426887, 8533636393847729449,\n",
            "        1450305737820992457], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 479])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 479])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 479])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([7498617476745975077, 3209934470854003467, 7237162252934494626,\n",
            "        2308562020693753360, 8743801463150567214, 6352957698747544361,\n",
            "        3950594246181274134, 6788806128389041971, 7498617476745975077,\n",
            "        3209934470854003467, 7237162252934494626, 2308562020693753360,\n",
            "        8743801463150567214, 6352957698747544361, 3950594246181274134,\n",
            "        6788806128389041971], device='cuda:0') with shape: torch.Size([16])\n",
            " Name: token_ids and Shape of tensor before unpack:torch.Size([4, 2, 441])\n",
            " Name: mask and Shape of tensor before unpack:torch.Size([4, 2, 441])\n",
            " Name: type_ids and Shape of tensor before unpack:torch.Size([4, 2, 441])\n",
            "embeddings inside loss: torch.Size([16, 768])\n",
            "embeddings shape after getting embeds and labels: torch.Size([16, 768]) and labels:tensor([3451366272176481996, 1331853791955617354, 6704653355532904106,\n",
            "        3338449791156814507, 4249243779278904068, 7012912576437774303,\n",
            "        7144544941699944839, 7034922420269130743, 3451366272176481996,\n",
            "        1331853791955617354, 6704653355532904106, 3338449791156814507,\n",
            "        4249243779278904068, 7012912576437774303, 7144544941699944839,\n",
            "        7034922420269130743], device='cuda:0') with shape: torch.Size([16])\n",
            "2022-09-21 13:51:21,007 - INFO - allennlp.training.tensorboard_writer -                        Training |  Validation\n",
            "2022-09-21 13:51:21,008 - INFO - allennlp.training.tensorboard_writer - gpu_0_memory_MB    |   796.000  |       N/A\n",
            "2022-09-21 13:51:21,008 - INFO - allennlp.training.tensorboard_writer - loss               |     3.613  |       N/A\n",
            "2022-09-21 13:51:21,009 - INFO - allennlp.training.tensorboard_writer - worker_0_memory_MB |     0.000  |       N/A\n",
            "2022-09-21 13:51:33,489 - INFO - allennlp.training.checkpointer - Best validation performance so far. Copying weights to 'E:/saved_models/declutr/wiki/output/best.th'.\n",
            "2022-09-21 13:51:33,845 - INFO - allennlp.training.trainer - Epoch duration: 0:00:31.573769\n",
            "2022-09-21 13:51:33,846 - INFO - allennlp.training.checkpointer - loading best weights\n",
            "2022-09-21 13:51:34,064 - INFO - allennlp.common.util - Metrics: {\n",
            "  \"best_epoch\": 0,\n",
            "  \"peak_worker_0_memory_MB\": 0,\n",
            "  \"peak_gpu_0_memory_MB\": 796,\n",
            "  \"training_duration\": \"0:00:18.737389\",\n",
            "  \"training_start_epoch\": 0,\n",
            "  \"training_epochs\": 0,\n",
            "  \"epoch\": 0,\n",
            "  \"training_loss\": 3.612826271057129,\n",
            "  \"training_worker_0_memory_MB\": 0.0,\n",
            "  \"training_gpu_0_memory_MB\": 796\n",
            "}\n",
            "2022-09-21 13:51:34,065 - INFO - allennlp.models.archival - archiving weights and vocabulary to E:/saved_models/declutr/wiki/output\\model.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "reading instances: 0it [00:00, ?it/s]\n",
            "reading instances: 2it [00:00, 17.39it/s]\n",
            "reading instances: 5it [00:00, 22.97it/s]\n",
            "reading instances: 9it [00:00, 26.81it/s]\n",
            "reading instances: 12it [00:00, 23.27it/s]\n",
            "reading instances: 15it [00:00, 24.67it/s]\n",
            "reading instances: 18it [00:00, 25.20it/s]\n",
            "reading instances: 22it [00:00, 27.25it/s]\n",
            "reading instances: 26it [00:00, 28.77it/s]\n",
            "reading instances: 30it [00:01, 28.22it/s]\n",
            "reading instances: 33it [00:01, 26.71it/s]\n",
            "reading instances: 37it [00:01, 27.95it/s]\n",
            "reading instances: 40it [00:01, 28.32it/s]\n",
            "reading instances: 44it [00:01, 29.60it/s]\n",
            "reading instances: 47it [00:01, 28.95it/s]\n",
            "reading instances: 50it [00:01, 29.00it/s]\n",
            "reading instances: 53it [00:01, 28.80it/s]\n",
            "reading instances: 56it [00:02, 27.52it/s]\n",
            "reading instances: 59it [00:02, 24.87it/s]\n",
            "reading instances: 62it [00:02, 25.09it/s]\n",
            "reading instances: 66it [00:02, 27.94it/s]\n",
            "reading instances: 69it [00:02, 27.54it/s]\n",
            "reading instances: 72it [00:02, 27.90it/s]\n",
            "reading instances: 76it [00:02, 29.21it/s]\n",
            "reading instances: 80it [00:02, 30.55it/s]\n",
            "reading instances: 84it [00:03, 28.77it/s]\n",
            "reading instances: 88it [00:03, 29.79it/s]\n",
            "reading instances: 92it [00:03, 29.40it/s]\n",
            "reading instances: 95it [00:03, 29.47it/s]\n",
            "reading instances: 99it [00:03, 30.09it/s]\n",
            "reading instances: 100it [00:03, 27.97it/s]\n",
            "\n",
            "  0%|          | 0/25 [00:00<?, ?it/s]\n",
            "batch_loss: 5.1119, loss: 5.1119 ||:   4%|4         | 1/25 [00:01<00:27,  1.15s/it]\n",
            "batch_loss: 5.1756, loss: 5.1438 ||:   8%|8         | 2/25 [00:01<00:21,  1.06it/s]\n",
            "batch_loss: 5.1792, loss: 5.1556 ||:  12%|#2        | 3/25 [00:02<00:18,  1.20it/s]\n",
            "batch_loss: 4.7723, loss: 5.0598 ||:  16%|#6        | 4/25 [00:03<00:15,  1.35it/s]\n",
            "batch_loss: 4.5381, loss: 4.9554 ||:  20%|##        | 5/25 [00:04<00:15,  1.31it/s]\n",
            "batch_loss: 4.2037, loss: 4.8301 ||:  24%|##4       | 6/25 [00:04<00:14,  1.29it/s]\n",
            "batch_loss: 4.4895, loss: 4.7815 ||:  28%|##8       | 7/25 [00:05<00:13,  1.34it/s]\n",
            "batch_loss: 3.9610, loss: 4.6789 ||:  32%|###2      | 8/25 [00:06<00:13,  1.31it/s]\n",
            "batch_loss: 4.1152, loss: 4.6163 ||:  36%|###6      | 9/25 [00:07<00:11,  1.36it/s]\n",
            "batch_loss: 3.4838, loss: 4.5030 ||:  40%|####      | 10/25 [00:07<00:11,  1.36it/s]\n",
            "batch_loss: 3.1869, loss: 4.3834 ||:  44%|####4     | 11/25 [00:08<00:09,  1.41it/s]\n",
            "batch_loss: 3.2928, loss: 4.2925 ||:  48%|####8     | 12/25 [00:09<00:09,  1.37it/s]\n",
            "batch_loss: 3.1527, loss: 4.2048 ||:  52%|#####2    | 13/25 [00:10<00:09,  1.31it/s]\n",
            "batch_loss: 2.9622, loss: 4.1161 ||:  56%|#####6    | 14/25 [00:10<00:08,  1.35it/s]\n",
            "batch_loss: 3.1765, loss: 4.0534 ||:  60%|######    | 15/25 [00:11<00:07,  1.37it/s]\n",
            "batch_loss: 3.6013, loss: 4.0252 ||:  64%|######4   | 16/25 [00:12<00:06,  1.37it/s]\n",
            "batch_loss: 2.7188, loss: 3.9483 ||:  68%|######8   | 17/25 [00:12<00:05,  1.39it/s]\n",
            "batch_loss: 2.7160, loss: 3.8799 ||:  72%|#######2  | 18/25 [00:13<00:05,  1.34it/s]\n",
            "batch_loss: 3.0107, loss: 3.8341 ||:  76%|#######6  | 19/25 [00:14<00:04,  1.37it/s]\n",
            "batch_loss: 3.2179, loss: 3.8033 ||:  80%|########  | 20/25 [00:15<00:03,  1.38it/s]\n",
            "batch_loss: 2.8881, loss: 3.7597 ||:  84%|########4 | 21/25 [00:15<00:02,  1.35it/s]\n",
            "batch_loss: 2.7928, loss: 3.7158 ||:  88%|########8 | 22/25 [00:16<00:02,  1.44it/s]\n",
            "batch_loss: 2.7833, loss: 3.6752 ||:  92%|#########2| 23/25 [00:17<00:01,  1.40it/s]\n",
            "batch_loss: 2.8143, loss: 3.6394 ||:  96%|#########6| 24/25 [00:17<00:00,  1.38it/s]\n",
            "batch_loss: 2.9760, loss: 3.6128 ||: 100%|##########| 25/25 [00:18<00:00,  1.35it/s]\n",
            "batch_loss: 2.9760, loss: 3.6128 ||: 100%|##########| 25/25 [00:18<00:00,  1.34it/s]\n"
          ]
        }
      ],
      "source": [
        "!allennlp train \"../training_config/declutr_small_v2.jsonnet\" \\\n",
        "    --serialization-dir \"E:/saved_models/declutr/wiki/output\" \\\n",
        "    --overrides \"$overrides\" \\\n",
        "    --include-package \"declutr\" \\\n",
        "    -f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsbr6OMv16GQ"
      },
      "source": [
        "### ü§ó Exporting a trained model to HuggingFace Transformers\n",
        "\n",
        "We have provided a simple script to export a trained model so that it can be loaded with [Hugging Face Transformers](https://github.com/huggingface/transformers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KqmWVD0y16GQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ ü§ó Transformers compatible model saved to: E:\\saved_models\\declutr\\wiki\\output\\transformers_format. See https://huggingface.co/transformers/model_sharing.html for instructions on hosting the model with ü§ó Transformers.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "archive_file = \"E:/saved_models/declutr/wiki/output/\"\n",
        "save_directory = \"E:/saved_models/declutr/wiki/output/transformers_format/\"\n",
        "\n",
        "!python ../scripts/save_pretrained_hf.py --archive_file $archive_file --save_directory $save_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python ../scripts/save_pretrained_hf.py --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0-NTFaH16GQ"
      },
      "source": [
        "The model, saved to `--save-directory`, can then be loaded using the Hugging Face Transformers library\n",
        "\n",
        "> See the [embedding notebook](https://colab.research.google.com/github/JohnGiorgi/DeCLUTR/blob/master/notebooks/embedding.ipynb) for more details on using trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pAl1zIya16GQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
        "  \n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"{save_directory}\")\n",
        "model = AutoModel.from_pretrained(f\"{save_directory}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzQ0G4rp16GQ"
      },
      "source": [
        "> If you would like to upload your model to the Hugging Face model repository, follow the instructions [here](https://huggingface.co/transformers/model_sharing.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD5dZo18EE-S"
      },
      "source": [
        "## ‚ôªÔ∏è Conclusion\n",
        "\n",
        "That's it! In this notebook, we covered how to collect data for training the model, and specifically how _long_ that text needs to be. We then briefly covered configuring and running a training session. Please see [our paper](https://arxiv.org/abs/2006.03659) and [repo](https://github.com/JohnGiorgi/DeCLUTR) for more details, and don't hesitate to open an issue if you have any trouble!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "training.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('37_declutr')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "80c13ac005a4e8467463143a62ebf86e7f8ec07ba508cce481d79436c5ff6a9b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
