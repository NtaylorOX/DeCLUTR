{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8jt6ML03DS5"
      },
      "source": [
        "# Training your own model\n",
        "\n",
        "This notebook will walk you through training your own model using [DeCLUTR](https://github.com/JohnGiorgi/DeCLUTR)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU3Iod2-g0-o"
      },
      "source": [
        "## üîß Install the prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sr4r5pN40Kli"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/JohnGiorgi/DeCLUTR.git\n",
        "\n",
        "# go to main dir i.e. DeCLUTR on local and run \"pip install --editable .\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zog7ApwuUD7_"
      },
      "source": [
        "## üìñ Preparing a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwnLpUmN4Art"
      },
      "source": [
        "\n",
        "A dataset is simply a file containing one item of text (a document, a scientific paper, etc.) per line. For demonstration purposes, we have provided a script that will download the [WikiText-103](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/) dataset and format it for training with our method.\n",
        "\n",
        "The only \"gotcha\" is that each piece of text needs to be long enough so that we can sample spans from it. In general, you should collect documents of a minimum length according to the following:\n",
        "\n",
        "```python\n",
        "min_length = num_anchors * max_span_len * 2\n",
        "```\n",
        "\n",
        "In our paper, we set `num_anchors=2` and `max_span_len=512`, so we require documents of `min_length=2048`. We simply need to provide this value as an argument when running the script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q0fwnwq23aAZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "train_data_path = \"E:/wiki_text/wikitext-103/train.txt\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUEFeupP6qy-"
      },
      "source": [
        "Lets confirm that our dataset looks as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K7ffGXCn7Cpq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    17824 E:/wiki_text/wikitext-103/train.txt\n",
            "    17824 total\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wc: '#': No such file or directory\n",
            "wc: This: No such file or directory\n",
            "wc: should: No such file or directory\n",
            "wc: be: No such file or directory\n",
            "wc: approximately: No such file or directory\n",
            "wc: 17.8K: No such file or directory\n",
            "wc: lines: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!wc -l $train_data_path  # This should be approximately 17.8K lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "10DprWZc9iV6"
      },
      "outputs": [],
      "source": [
        "# !head -n 1 $train_data_path  # This should be a single Wikipedia entry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Look at sampling technique\n",
        "\n",
        "This will help get an idea of what "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\conda_envs\\37_declutr\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from declutr.common.contrastive_utils import sample_anchor_positive_pairs\n",
        "from declutr.losses import NTXentLoss\n",
        "import torch\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"this is just an example sentence to test out some sampling and loss calculation from DeCLUTR. We want to see exactly how it works in order to implement it for our own use case\"\n",
        "len_text = len(text.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# just go with one anchor for now\n",
        "\n",
        "num_anchors = 1\n",
        "max_span_len = int((len_text/2)/num_anchors) \n",
        "min_span_len = 5\n",
        "num_positives = 5\n",
        "sampling_strat = \"adjacent\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "anchor_spans, positive_spans = sample_anchor_positive_pairs(\n",
        "    text = text,\n",
        "    num_anchors = num_anchors,\n",
        "    num_positives = num_positives,\n",
        "    max_span_len = max_span_len,\n",
        "    min_span_len = min_span_len,\n",
        "    sampling_strategy = sampling_strat\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['to see exactly how it works in order to implement it for our own use']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "anchor_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['loss calculation from DeCLUTR. We want',\n",
              " 'and loss calculation from DeCLUTR. We want',\n",
              " 'sampling and loss calculation from DeCLUTR. We want',\n",
              " 'sampling and loss calculation from DeCLUTR. We want',\n",
              " 'some sampling and loss calculation from DeCLUTR. We want']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "positive_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKYdambZ59nM"
      },
      "source": [
        "## üèÉ Training the model\n",
        "\n",
        "Once you have collected the dataset, you can easily initiate a training session with the `allennlp train` command. An experiment is configured using a [Jsonnet](https://jsonnet.org/) config file. Lets take a look at the config for the DeCLUTR-small model presented in [our paper](https://arxiv.org/abs/2006.03659):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open(\"../training_config/declutr_small.jsonnet\", \"r\") as f:\n",
        "#     print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f1HqWSscWOx"
      },
      "source": [
        "\n",
        "The only thing to configure is the path to the training set (`train_data_path`), which can be passed to `allennlp train` via the `--overrides` argument (but you can also provide it in your config file directly, if you prefer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YS9VuxESBcr3"
      },
      "outputs": [],
      "source": [
        "# overrides = (\n",
        "#     f\"{{'train_data_path': '{train_data_path}', \"\n",
        "#     # lower the batch size to be able to train on Colab GPUs\n",
        "#     \"'data_loader.batch_size': 2, \"\n",
        "#     # training examples / batch size. Not required, but gives us a more informative progress bar during training\n",
        "#     \"'data_loader.batches_per_epoch': None}\"\n",
        "# )\n",
        "\n",
        "\n",
        "overrides = (\n",
        "    f\"{{'train_data_path': '{train_data_path}', \"\n",
        "    # lower the batch size to be able to train on Colab GPUs\n",
        "    \"'data_loader.batch_size': 2,}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2v4tiiXgBC2M"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"{'train_data_path': 'E:/wiki_text/wikitext-103/train.txt', 'data_loader.batch_size': 2,}\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "overrides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Db_cNfZ76KRf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-09-20 15:22:24,602 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2022-09-20 15:22:24,602 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2022-09-20 15:22:24,603 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2022-09-20 15:22:24,607 - INFO - allennlp.common.checks - Pytorch version: 1.12.1\n",
            "2022-09-20 15:22:24,608 - INFO - allennlp.common.params - type = default\n",
            "2022-09-20 15:22:24,608 - INFO - allennlp.common.params - dataset_reader.type = declutr\n",
            "2022-09-20 15:22:24,609 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2022-09-20 15:22:24,609 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2022-09-20 15:22:24,609 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
            "2022-09-20 15:22:24,609 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n",
            "2022-09-20 15:22:24,610 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = distilroberta-base\n",
            "2022-09-20 15:22:24,610 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = True\n",
            "2022-09-20 15:22:24,610 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = 510\n",
            "2022-09-20 15:22:24,610 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n",
            "2022-09-20 15:22:24,610 - INFO - allennlp.common.params - dataset_reader.tokenizer.verification_tokens = None\n",
            "2022-09-20 15:22:31,730 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.type = pretrained_transformer\n",
            "2022-09-20 15:22:31,730 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.token_min_padding_length = 0\n",
            "2022-09-20 15:22:31,730 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.model_name = distilroberta-base\n",
            "2022-09-20 15:22:31,731 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.namespace = tags\n",
            "2022-09-20 15:22:31,731 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.max_length = None\n",
            "2022-09-20 15:22:31,731 - INFO - allennlp.common.params - dataset_reader.token_indexers.tokens.tokenizer_kwargs = None\n",
            "2022-09-20 15:22:31,732 - INFO - allennlp.common.params - dataset_reader.num_anchors = 2\n",
            "2022-09-20 15:22:31,732 - INFO - allennlp.common.params - dataset_reader.num_positives = 2\n",
            "2022-09-20 15:22:31,732 - INFO - allennlp.common.params - dataset_reader.max_span_len = 512\n",
            "2022-09-20 15:22:31,732 - INFO - allennlp.common.params - dataset_reader.min_span_len = 32\n",
            "2022-09-20 15:22:31,732 - INFO - allennlp.common.params - dataset_reader.sampling_strategy = None\n",
            "2022-09-20 15:22:31,732 - INFO - allennlp.common.params - train_data_path = E:/wiki_text/wikitext-103/train.txt\n",
            "2022-09-20 15:22:31,733 - INFO - allennlp.common.params - datasets_for_vocab_creation = None\n",
            "2022-09-20 15:22:31,733 - INFO - allennlp.common.params - validation_dataset_reader = None\n",
            "2022-09-20 15:22:31,733 - INFO - allennlp.common.params - validation_data_path = None\n",
            "2022-09-20 15:22:31,733 - INFO - allennlp.common.params - validation_data_loader = None\n",
            "2022-09-20 15:22:31,733 - INFO - allennlp.common.params - test_data_path = None\n",
            "2022-09-20 15:22:31,734 - INFO - allennlp.common.params - evaluate_on_test = False\n",
            "2022-09-20 15:22:31,734 - INFO - allennlp.common.params - batch_weight_key = \n",
            "2022-09-20 15:22:31,734 - INFO - allennlp.common.params - data_loader.type = multiprocess\n",
            "2022-09-20 15:22:31,734 - INFO - allennlp.common.params - data_loader.batch_size = 2\n",
            "2022-09-20 15:22:31,734 - INFO - allennlp.common.params - data_loader.drop_last = True\n",
            "2022-09-20 15:22:31,734 - INFO - allennlp.common.params - data_loader.shuffle = False\n",
            "2022-09-20 15:22:31,735 - INFO - allennlp.common.params - data_loader.batch_sampler = None\n",
            "2022-09-20 15:22:31,735 - INFO - allennlp.common.params - data_loader.batches_per_epoch = None\n",
            "2022-09-20 15:22:31,735 - INFO - allennlp.common.params - data_loader.num_workers = 1\n",
            "2022-09-20 15:22:31,735 - INFO - allennlp.common.params - data_loader.max_instances_in_memory = None\n",
            "2022-09-20 15:22:31,735 - INFO - allennlp.common.params - data_loader.start_method = fork\n",
            "2022-09-20 15:22:31,735 - INFO - allennlp.common.params - data_loader.cuda_device = None\n",
            "2022-09-20 15:22:31,735 - INFO - allennlp.common.params - data_loader.quiet = False\n",
            "2022-09-20 15:22:31,735 - INFO - allennlp.common.params - data_loader.collate_fn = <allennlp.data.data_loaders.data_collator.DefaultDataCollator object at 0x000001B7BFFDDA60>\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-09-20 15:22:24,601 - INFO - allennlp.common.params - evaluation = None\n",
            "2022-09-20 15:22:24,602 - INFO - allennlp.common.params - include_in_archive = None\n",
            "2022-09-20 15:22:31,736 - CRITICAL - root - Uncaught exception\n",
            "Traceback (most recent call last):\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"E:\\conda_envs\\declutr\\Scripts\\allennlp.exe\\__main__.py\", line 7, in <module>\n",
            "    sys.exit(run())\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\site-packages\\allennlp\\__main__.py\", line 39, in run\n",
            "    main(prog=\"allennlp\")\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\site-packages\\allennlp\\commands\\__init__.py\", line 120, in main\n",
            "    args.func(args)\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\site-packages\\allennlp\\commands\\train.py\", line 111, in train_model_from_args\n",
            "    train_model_from_file(\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\site-packages\\allennlp\\commands\\train.py\", line 177, in train_model_from_file\n",
            "    return train_model(\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\site-packages\\allennlp\\commands\\train.py\", line 258, in train_model\n",
            "    model = _train_worker(\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\site-packages\\allennlp\\commands\\train.py\", line 494, in _train_worker\n",
            "    train_loop = TrainModel.from_params(\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 604, in from_params\n",
            "    return retyped_subclass.from_params(\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 638, in from_params\n",
            "    return constructor_to_call(**kwargs)  # type: ignore\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\site-packages\\allennlp\\commands\\train.py\", line 716, in from_partial_objects\n",
            "    \"train\": data_loader.construct(reader=dataset_reader, data_path=train_data_path)\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\site-packages\\allennlp\\common\\lazy.py\", line 82, in construct\n",
            "    return self.constructor(**contructor_kwargs)\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\site-packages\\allennlp\\common\\lazy.py\", line 66, in constructor_to_use\n",
            "    return self._constructor.from_params(  # type: ignore[union-attr]\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 604, in from_params\n",
            "    return retyped_subclass.from_params(\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\site-packages\\allennlp\\common\\from_params.py\", line 638, in from_params\n",
            "    return constructor_to_call(**kwargs)  # type: ignore\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\site-packages\\allennlp\\data\\data_loaders\\multiprocess_data_loader.py\", line 296, in __init__\n",
            "    deque(self.iter_instances(), maxlen=0)\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\site-packages\\allennlp\\data\\data_loaders\\multiprocess_data_loader.py\", line 370, in iter_instances\n",
            "    ctx = mp.get_context(self.start_method)\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\multiprocessing\\context.py\", line 239, in get_context\n",
            "    return super().get_context(method)\n",
            "  File \"E:\\conda_envs\\declutr\\lib\\multiprocessing\\context.py\", line 193, in get_context\n",
            "    raise ValueError('cannot find context for %r' % method) from None\n",
            "ValueError: cannot find context for 'fork'\n"
          ]
        }
      ],
      "source": [
        "!allennlp train \"../training_config/declutr_small_v2.jsonnet\" \\\n",
        "    --serialization-dir \"output\" \\\n",
        "    --overrides \"$overrides\" \\\n",
        "    --include-package \"declutr\" \\\n",
        "    -f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsbr6OMv16GQ"
      },
      "source": [
        "### ü§ó Exporting a trained model to HuggingFace Transformers\n",
        "\n",
        "We have provided a simple script to export a trained model so that it can be loaded with [Hugging Face Transformers](https://github.com/huggingface/transformers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqmWVD0y16GQ"
      },
      "outputs": [],
      "source": [
        "!wget -nc https://github.com/JohnGiorgi/DeCLUTR/blob/master/scripts/save_pretrained_hf.py\n",
        "!python save_pretrained_hf.py --archive-file \"output\" --save-directory \"output_transformers\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0-NTFaH16GQ"
      },
      "source": [
        "The model, saved to `--save-directory`, can then be loaded using the Hugging Face Transformers library\n",
        "\n",
        "> See the [embedding notebook](https://colab.research.google.com/github/JohnGiorgi/DeCLUTR/blob/master/notebooks/embedding.ipynb) for more details on using trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAl1zIya16GQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "  \n",
        "tokenizer = AutoTokenizer.from_pretrained(\"output_transformers\")\n",
        "model = AutoModel.from_pretrained(\"output_transformers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzQ0G4rp16GQ"
      },
      "source": [
        "> If you would like to upload your model to the Hugging Face model repository, follow the instructions [here](https://huggingface.co/transformers/model_sharing.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD5dZo18EE-S"
      },
      "source": [
        "## ‚ôªÔ∏è Conclusion\n",
        "\n",
        "That's it! In this notebook, we covered how to collect data for training the model, and specifically how _long_ that text needs to be. We then briefly covered configuring and running a training session. Please see [our paper](https://arxiv.org/abs/2006.03659) and [repo](https://github.com/JohnGiorgi/DeCLUTR) for more details, and don't hesitate to open an issue if you have any trouble!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "training.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('declutr')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "4434633c66200ec48b92bc826f9d54149d3d82c3c560723a0ff4b34a6ca6fdbc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
