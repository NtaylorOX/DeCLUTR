{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8jt6ML03DS5"
      },
      "source": [
        "# Training your own model\n",
        "\n",
        "This notebook will walk you through training your own model using [DeCLUTR](https://github.com/JohnGiorgi/DeCLUTR)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU3Iod2-g0-o"
      },
      "source": [
        "## üîß Install the prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sr4r5pN40Kli"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/JohnGiorgi/DeCLUTR.git\n",
        "\n",
        "# go to main dir i.e. DeCLUTR on local and run \"pip install --editable .\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_dir = \"E:/working_incident_data/cleaned_data/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<multiprocessing.pool.Pool at 0x1a914b8ed08>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# testing multiprocessing fork - \n",
        "\n",
        "from multiprocessing import get_context\n",
        "num_processes = 8\n",
        "pool = get_context(\"spawn\").Pool(num_processes)\n",
        "pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zog7ApwuUD7_"
      },
      "source": [
        "## üìñ Preparing a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwnLpUmN4Art"
      },
      "source": [
        "\n",
        "A dataset is simply a file containing one item of text (a document, a scientific paper, etc.) per line. For demonstration purposes, we have provided a script that will download the [WikiText-103](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/) dataset and format it for training with our method.\n",
        "\n",
        "The only \"gotcha\" is that each piece of text needs to be long enough so that we can sample spans from it. In general, you should collect documents of a minimum length according to the following:\n",
        "\n",
        "```python\n",
        "min_length = num_anchors * max_span_len * 2\n",
        "```\n",
        "\n",
        "In our paper, we set `num_anchors=2` and `max_span_len=512`, so we require documents of `min_length=2048`. We simply need to provide this value as an argument when running the script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q0fwnwq23aAZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "train_data_path = \"E:/working_incident_data/cleaned_data/declutr_lm/min_128_1_anchors/train.txt\"\n",
        "\n",
        "\n",
        "# this data was created using ../scripts/preprocess_incident_reports.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUEFeupP6qy-"
      },
      "source": [
        "Lets confirm that our dataset looks as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K7ffGXCn7Cpq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   1000 E:/working_incident_data/cleaned_data/declutr_lm/min_128_1_anchors/train.txt"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wc: '#': No such file or directory\n",
            "wc: see: No such file or directory\n",
            "wc: number: No such file or directory\n",
            "wc: of: No such file or directory\n",
            "wc: lines: No such file or directory\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "   1000 total\n"
          ]
        }
      ],
      "source": [
        "!wc -l $train_data_path  # see number of lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "10DprWZc9iV6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> E:/working_incident_data/cleaned_data/declutr_lm/min_128_1_anchors/train.txt <==\n",
            "attended to patient this morning who was short of breath , i was told by csw af that the patient sat himself on the edge of the bed and the ngt was caught and pulling on the bed . i attended to the patient and stopped the ng feed as it may have moved and be unsafe . i measured the ngt external length which was 22.5cm and went to compare the length on the ngt care plan , however , there was no care plan in situ and the ng feed was commenced without any documentation of the ngt checks required before administering the feed . i was unable to check if the external measurement was correct as there was no documentation on ppm or care plan in situ . therefore there were no checks carried out that were documented when the feed was commenced by the rn at 6am . .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "head: cannot open '#' for reading: No such file or directory\n",
            "head: cannot open 'This' for reading: No such file or directory\n",
            "head: cannot open 'should' for reading: No such file or directory\n",
            "head: cannot open 'be' for reading: No such file or directory\n",
            "head: cannot open 'a' for reading: No such file or directory\n",
            "head: cannot open 'single' for reading: No such file or directory\n",
            "head: cannot open 'doc' for reading: No such file or directory\n",
            "head: cannot open 'per' for reading: No such file or directory\n",
            "head: cannot open 'line' for reading: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!head -n 1 $train_data_path  # This should be a single doc per line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Look at sampling technique\n",
        "\n",
        "This will help get an idea of what "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "e:\\conda_envs\\37_declutr\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from declutr.common.contrastive_utils import sample_anchor_positive_pairs\n",
        "from declutr.losses import NTXentLoss\n",
        "import torch\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"this is just an example sentence to test out some sampling and loss calculation from DeCLUTR. We want to see exactly how it works in order to implement it for our own use case\"\n",
        "len_text = len(text.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# just go with one anchor for now\n",
        "\n",
        "num_anchors = 1\n",
        "max_span_len = int((len_text/2)/num_anchors) \n",
        "min_span_len = 5\n",
        "num_positives = 5\n",
        "sampling_strat = \"adjacent\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "anchor_spans, positive_spans = sample_anchor_positive_pairs(\n",
        "    text = text,\n",
        "    num_anchors = num_anchors,\n",
        "    num_positives = num_positives,\n",
        "    max_span_len = max_span_len,\n",
        "    min_span_len = min_span_len,\n",
        "    sampling_strategy = sampling_strat\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['to see exactly how it works in order to implement it for our own use']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "anchor_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['loss calculation from DeCLUTR. We want',\n",
              " 'and loss calculation from DeCLUTR. We want',\n",
              " 'sampling and loss calculation from DeCLUTR. We want',\n",
              " 'sampling and loss calculation from DeCLUTR. We want',\n",
              " 'some sampling and loss calculation from DeCLUTR. We want']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "positive_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test loss function\n",
        "anchor_emb = torch.rand(64).unsqueeze(0)\n",
        "pos_emb = torch.rand(64).unsqueeze(0)\n",
        "neg_emb = torch.rand(64).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "anchor_pos_embs = torch.cat((anchor_emb, pos_emb))\n",
        "loss_func = NTXentLoss\n",
        "embs, labels = NTXentLoss.get_embeddings_and_label(anchor_emb, pos_emb)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKYdambZ59nM"
      },
      "source": [
        "## üèÉ Training the model\n",
        "\n",
        "Once you have collected the dataset, you can easily initiate a training session with the `allennlp train` command. An experiment is configured using a [Jsonnet](https://jsonnet.org/) config file. Lets take a look at the config for the DeCLUTR-small model presented in [our paper](https://arxiv.org/abs/2006.03659):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open(\"../training_config/declutr_small.jsonnet\", \"r\") as f:\n",
        "#     print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f1HqWSscWOx"
      },
      "source": [
        "\n",
        "The only thing to configure is the path to the training set (`train_data_path`), which can be passed to `allennlp train` via the `--overrides` argument (but you can also provide it in your config file directly, if you prefer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YS9VuxESBcr3"
      },
      "outputs": [],
      "source": [
        "# overrides = (\n",
        "#     f\"{{'train_data_path': '{train_data_path}', \"\n",
        "#     # lower the batch size to be able to train on Colab GPUs\n",
        "#     \"'data_loader.batch_size': 2, \"\n",
        "#     # training examples / batch size. Not required, but gives us a more informative progress bar during training\n",
        "#     \"'data_loader.batches_per_epoch': None}\"\n",
        "# )\n",
        "\n",
        "\n",
        "overrides = (\n",
        "    f\"{{'train_data_path': '{train_data_path}', \"\n",
        "    # lower the batch size to be able to train on Colab GPUs\n",
        "    \"'data_loader.batch_size': 4,\"    \n",
        "    \"'data_loader.num_workers': 0,}\" # at the moment on windows any multi-processing throws error as allennlp was written for linux\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2v4tiiXgBC2M"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"{'train_data_path': 'E:/working_incident_data/cleaned_data/declutr_lm/min_128_1_anchors/train.txt', 'data_loader.batch_size': 4,'data_loader.num_workers': 0,}\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "overrides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Db_cNfZ76KRf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!allennlp train \"../training_config/declutr_small_incident.jsonnet\" \\\n",
        "    --serialization-dir \"E:/saved_models/declutr/incident/output\" \\\n",
        "    --overrides \"$overrides\" \\\n",
        "    --include-package \"declutr\" \\\n",
        "    -f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsbr6OMv16GQ"
      },
      "source": [
        "### ü§ó Exporting a trained model to HuggingFace Transformers\n",
        "\n",
        "We have provided a simple script to export a trained model so that it can be loaded with [Hugging Face Transformers](https://github.com/huggingface/transformers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KqmWVD0y16GQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíæ ü§ó Transformers compatible model saved to: E:\\saved_models\\declutr\\wiki\\output\\transformers_format. See https://huggingface.co/transformers/model_sharing.html for instructions on hosting the model with ü§ó Transformers.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "archive_file = \"E:/saved_models/declutr/wiki/output/\"\n",
        "save_directory = \"E:/saved_models/declutr/wiki/output/transformers_format/\"\n",
        "\n",
        "!python ../scripts/save_pretrained_hf.py --archive_file $archive_file --save_directory $save_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !python ../scripts/save_pretrained_hf.py --help"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0-NTFaH16GQ"
      },
      "source": [
        "The model, saved to `--save-directory`, can then be loaded using the Hugging Face Transformers library\n",
        "\n",
        "> See the [embedding notebook](https://colab.research.google.com/github/JohnGiorgi/DeCLUTR/blob/master/notebooks/embedding.ipynb) for more details on using trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pAl1zIya16GQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel\n",
        "  \n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"{save_directory}\")\n",
        "model = AutoModel.from_pretrained(f\"{save_directory}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RobertaModel(\n",
              "  (embeddings): RobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (1): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (2): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (3): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (4): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (5): BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSelfAttention(\n",
              "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): BertPooler(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzQ0G4rp16GQ"
      },
      "source": [
        "> If you would like to upload your model to the Hugging Face model repository, follow the instructions [here](https://huggingface.co/transformers/model_sharing.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD5dZo18EE-S"
      },
      "source": [
        "## ‚ôªÔ∏è Conclusion\n",
        "\n",
        "That's it! In this notebook, we covered how to collect data for training the model, and specifically how _long_ that text needs to be. We then briefly covered configuring and running a training session. Please see [our paper](https://arxiv.org/abs/2006.03659) and [repo](https://github.com/JohnGiorgi/DeCLUTR) for more details, and don't hesitate to open an issue if you have any trouble!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "training.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.13 ('37_declutr')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "80c13ac005a4e8467463143a62ebf86e7f8ec07ba508cce481d79436c5ff6a9b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
