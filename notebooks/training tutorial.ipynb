{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8jt6ML03DS5"
      },
      "source": [
        "# Training your own model\n",
        "\n",
        "This notebook will walk you through training your own model using [DeCLUTR](https://github.com/JohnGiorgi/DeCLUTR)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU3Iod2-g0-o"
      },
      "source": [
        "## üîß Install the prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sr4r5pN40Kli"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/JohnGiorgi/DeCLUTR.git\n",
        "\n",
        "# go to main dir i.e. DeCLUTR on local and run \"pip install --editable .\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zog7ApwuUD7_"
      },
      "source": [
        "## üìñ Preparing a dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwnLpUmN4Art"
      },
      "source": [
        "\n",
        "A dataset is simply a file containing one item of text (a document, a scientific paper, etc.) per line. For demonstration purposes, we have provided a script that will download the [WikiText-103](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/) dataset and format it for training with our method.\n",
        "\n",
        "The only \"gotcha\" is that each piece of text needs to be long enough so that we can sample spans from it. In general, you should collect documents of a minimum length according to the following:\n",
        "\n",
        "```python\n",
        "min_length = num_anchors * max_span_len * 2\n",
        "```\n",
        "\n",
        "In our paper, we set `num_anchors=2` and `max_span_len=512`, so we require documents of `min_length=2048`. We simply need to provide this value as an argument when running the script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q0fwnwq23aAZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "train_data_path = \"/mnt/sdg/niallt/wiki_text/wikitext-103/train.txt\"\n",
        "\n",
        "min_length = 2048\n",
        "# run this first timeP\n",
        "\n",
        "\n",
        "# !python ../scripts/preprocess_wikitext_103.py $train_data_path --min-length $min_length --max-instances 500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUEFeupP6qy-"
      },
      "source": [
        "Lets confirm that our dataset looks as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K7ffGXCn7Cpq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17824 /mnt/sdg/niallt/wiki_text/wikitext-103/train.txt\n"
          ]
        }
      ],
      "source": [
        "!wc -l $train_data_path  # This should be approximately 17.8K lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "10DprWZc9iV6"
      },
      "outputs": [],
      "source": [
        "# !head -n 1 $train_data_path  # This should be a single Wikipedia entry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Look at sampling technique\n",
        "\n",
        "This will help get an idea of what "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/niallt/venvs/39_declutr/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from declutr.common.contrastive_utils import sample_anchor_positive_pairs\n",
        "from declutr.losses import NTXentLoss\n",
        "import torch\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = \"this is just an example sentence to test out some sampling and loss calculation from DeCLUTR. We want to see exactly how it works in order to implement it for our own use case\"\n",
        "len_text = len(text.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# just go with one anchor for now\n",
        "\n",
        "num_anchors = 1\n",
        "max_span_len = int((len_text/2)/num_anchors) \n",
        "min_span_len = 5\n",
        "num_positives = 5\n",
        "sampling_strat = \"adjacent\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "anchor_spans, positive_spans = sample_anchor_positive_pairs(\n",
        "    text = text,\n",
        "    num_anchors = num_anchors,\n",
        "    num_positives = num_positives,\n",
        "    max_span_len = max_span_len,\n",
        "    min_span_len = min_span_len,\n",
        "    sampling_strategy = sampling_strat\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['is just an example sentence to test out some sampling and loss']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "anchor_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['loss calculation from DeCLUTR. We want',\n",
              " 'and loss calculation from DeCLUTR. We want',\n",
              " 'sampling and loss calculation from DeCLUTR. We want',\n",
              " 'sampling and loss calculation from DeCLUTR. We want',\n",
              " 'some sampling and loss calculation from DeCLUTR. We want']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "positive_spans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKYdambZ59nM"
      },
      "source": [
        "## üèÉ Training the model\n",
        "\n",
        "Once you have collected the dataset, you can easily initiate a training session with the `allennlp train` command. An experiment is configured using a [Jsonnet](https://jsonnet.org/) config file. Lets take a look at the config for the DeCLUTR-small model presented in [our paper](https://arxiv.org/abs/2006.03659):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open(\"../training_config/declutr_small.jsonnet\", \"r\") as f:\n",
        "#     print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f1HqWSscWOx"
      },
      "source": [
        "\n",
        "The only thing to configure is the path to the training set (`train_data_path`), which can be passed to `allennlp train` via the `--overrides` argument (but you can also provide it in your config file directly, if you prefer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YS9VuxESBcr3"
      },
      "outputs": [],
      "source": [
        "# overrides = (\n",
        "#     f\"{{'train_data_path': '{train_data_path}', \"\n",
        "#     # lower the batch size to be able to train on Colab GPUs\n",
        "#     \"'data_loader.batch_size': 2, \"\n",
        "#     # training examples / batch size. Not required, but gives us a more informative progress bar during training\n",
        "#     \"'data_loader.batches_per_epoch': None}\"\n",
        "# )\n",
        "\n",
        "\n",
        "overrides = (\n",
        "    f\"{{'train_data_path': '{train_data_path}', \"\n",
        "    # lower the batch size to be able to train on Colab GPUs\n",
        "    \"'data_loader.batch_size': 2,}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2v4tiiXgBC2M"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"{'train_data_path': '/mnt/sdg/niallt/wiki_text/wikitext-103/train.txt', 'data_loader.batch_size': 2,}\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "overrides"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --serialization-dir \"/mnt/sdg/niallt/declutr/wiki/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Db_cNfZ76KRf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-11-21 11:05:14,611 - INFO - allennlp.common.params - evaluation = None\n",
            "2022-11-21 11:05:14,611 - INFO - allennlp.common.params - include_in_archive = None\n",
            "2022-11-21 11:05:14,611 - INFO - allennlp.common.params - random_seed = 13370\n",
            "2022-11-21 11:05:14,611 - INFO - allennlp.common.params - numpy_seed = 1337\n",
            "2022-11-21 11:05:14,611 - INFO - allennlp.common.params - pytorch_seed = 133\n",
            "2022-11-21 11:05:14,612 - INFO - allennlp.common.checks - Pytorch version: 1.11.0+cu102\n",
            "2022-11-21 11:05:14,612 - INFO - allennlp.common.params - type = default\n",
            "2022-11-21 11:05:14,613 - INFO - allennlp.common.params - dataset_reader.type = declutr\n",
            "2022-11-21 11:05:14,613 - INFO - allennlp.common.params - dataset_reader.max_instances = None\n",
            "2022-11-21 11:05:14,613 - INFO - allennlp.common.params - dataset_reader.manual_distributed_sharding = False\n",
            "2022-11-21 11:05:14,613 - INFO - allennlp.common.params - dataset_reader.manual_multiprocess_sharding = False\n",
            "2022-11-21 11:05:14,613 - INFO - allennlp.common.params - dataset_reader.tokenizer.type = pretrained_transformer\n",
            "2022-11-21 11:05:14,613 - INFO - allennlp.common.params - dataset_reader.tokenizer.model_name = distilroberta-base\n",
            "2022-11-21 11:05:14,613 - INFO - allennlp.common.params - dataset_reader.tokenizer.add_special_tokens = True\n",
            "2022-11-21 11:05:14,613 - INFO - allennlp.common.params - dataset_reader.tokenizer.max_length = 510\n",
            "2022-11-21 11:05:14,614 - INFO - allennlp.common.params - dataset_reader.tokenizer.tokenizer_kwargs = None\n",
            "2022-11-21 11:05:14,614 - INFO - allennlp.common.params - dataset_reader.tokenizer.verification_tokens = None\n"
          ]
        }
      ],
      "source": [
        "!allennlp train \"../training_config/declutr_small_v2.jsonnet\" \\\n",
        "    --serialization-dir \"output\" \\\n",
        "    --overrides \"$overrides\" \\\n",
        "    --include-package \"declutr\" \\\n",
        "    -f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsbr6OMv16GQ"
      },
      "source": [
        "### ü§ó Exporting a trained model to HuggingFace Transformers\n",
        "\n",
        "We have provided a simple script to export a trained model so that it can be loaded with [Hugging Face Transformers](https://github.com/huggingface/transformers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqmWVD0y16GQ"
      },
      "outputs": [],
      "source": [
        "!wget -nc https://github.com/JohnGiorgi/DeCLUTR/blob/master/scripts/save_pretrained_hf.py\n",
        "!python save_pretrained_hf.py --archive-file \"output\" --save-directory \"output_transformers\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0-NTFaH16GQ"
      },
      "source": [
        "The model, saved to `--save-directory`, can then be loaded using the Hugging Face Transformers library\n",
        "\n",
        "> See the [embedding notebook](https://colab.research.google.com/github/JohnGiorgi/DeCLUTR/blob/master/notebooks/embedding.ipynb) for more details on using trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAl1zIya16GQ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "  \n",
        "tokenizer = AutoTokenizer.from_pretrained(\"output_transformers\")\n",
        "model = AutoModel.from_pretrained(\"output_transformers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzQ0G4rp16GQ"
      },
      "source": [
        "> If you would like to upload your model to the Hugging Face model repository, follow the instructions [here](https://huggingface.co/transformers/model_sharing.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD5dZo18EE-S"
      },
      "source": [
        "## ‚ôªÔ∏è Conclusion\n",
        "\n",
        "That's it! In this notebook, we covered how to collect data for training the model, and specifically how _long_ that text needs to be. We then briefly covered configuring and running a training session. Please see [our paper](https://arxiv.org/abs/2006.03659) and [repo](https://github.com/JohnGiorgi/DeCLUTR) for more details, and don't hesitate to open an issue if you have any trouble!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "training.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('39_declutr': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "29abfe4b2a2a13e76bcb98af1501c4f384e087a48356ceb41341cb18ef9ca38a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
